# Explaining Image Classification with Deep and Gradient Explainer {#image-deep-explainer}


::: {.callout-tip appearance="simple"}

By the end of this chapter, you will be able to:

- Apply SHAP on a pixel level for image models.
- Understand the differences between pixel-based SHAP and larger patch-based SHAP.
- Explain the Deep Explainer and the Gradient Explainer.

:::

In the previous chapter, we explored the Partition Explainer, which treated larger image patches as features for SHAP.
In this chapter, we will explain image classifier classifications using a different approach, akin to tabular data.
This involves two key aspects:

- Assigning one SHAP value for each input pixel.
- Simulating feature absence (pixel absence) by sampling pixel from a background dataset.


Since we are working with a neural network, two model-specific tools are available:

- The Gradient Explainer, as neural networks often rely on gradients.
- The Deep Explainer, which utilizes neural network layers to backpropagate SHAP values.

Both methods are discussed in greater detail in the [Estimation Appendix](#estimation).
For this example, we will use the MNIST dataset.
The MNIST dataset contains 70,000 handwritten digits (0-9), each represented as a 28x28 pixel grayscale image.
The goal of the MNIST task is to create a machine learning algorithm that can accurately classify these images into their corresponding digit categories.
This well-established benchmark problem has been used to evaluate the performance of various algorithms, including neural networks, decision trees, and support vector machines.
Researchers in machine learning, computer vision, and pattern recognition have extensively used the MNIST dataset.

Why did I choose the MNIST dataset instead of ImageNet, as in the previous example?
Because using pixel-wise explanations with Gradient or Deep Explainer requires sampling absent features using a background dataset.

Imagine using the ImageNet dataset, where we have an image of a burger and replace the "absent" pixels with pixels from a dog image -- it would result in a strange outcome.
However, for the MNIST dataset, this approach is more reasonable, as digits are more similar to each other, and replacing some pixels of a "2" with those of a "3" won't generate bizarre images.
I acknowledge that this is a somewhat vague argument, but generally, explanations for images can be more challenging.

## Training the neural network

Let's start by training a neural network from scratch using TensorFlow.
The code is partially based on [this shap notebook](https://shap.readthedocs.io/en/latest/example_notebooks/image_examples/image_classification/Multi-input%20Gradient%20Explainer%20MNIST%20Example.html).

```{python}
#| output: false
import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import (
    Dense, Dropout, Flatten, Conv2D, MaxPooling2D
)
from tensorflow.keras.utils import to_categorical

# Load the MNIST dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Preprocess the data
x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)
x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)
x_train = x_train.astype('float32') / 255
x_test = x_test.astype('float32') / 255
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

# Define model architecture
model = Sequential()
model.add(Conv2D(
  32,
  kernel_size=(3, 3),
  activation='relu',
  input_shape=(28, 28, 1)
))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(10, activation='softmax'))

# Compile model
model.compile(
  loss='categorical_crossentropy',
  optimizer='adam',
  metrics=['accuracy']
)

# Train model
model.fit(
  x_train,
  y_train,
  batch_size=128,
  epochs=5,
  validation_data=(x_test, y_test)
)
score = model.evaluate(x_test, y_test, verbose=0)
```

Next, we evaluate the model's performance:

```{python}
# Evaluate model on test set
print('Test loss:', score[0])
print('Test accuracy:', score[1])
```

## Computing SHAP values with the Gradient Explainer

After training and evaluating our model, we'll explain its classification using SHAP:

```{python}
import shap
import time

# x_test is large, take a sample
x_sample = shap.sample(x_train, 500)

# Pass list of inputs to explainer since we have two inputs
explainer = shap.GradientExplainer(model, data=x_sample)

# Explain model's predictions on first three test set samples
start_time = time.time()
shap_values = explainer.shap_values(x_test[:3])
gradient_time = time.time() - start_time
```
The output, `shap_values`, is a list with a length equal to the number of classes (10 in this case):

```{python}
print(len(shap_values))
```

For each model output, we obtain the SHAP values:

```{python}
print(shap_values[0].shape)
```

The first dimension represents the number of images for which we computed the SHAP values.
The remaining dimensions contain the SHAP values in the form of an image, because the input data was an image.

Now, let's plot the SHAP values:

```{python}
#| label: fig-digits
#| fig-cap: SHAP values for the input pixels of different input images (one per row). First column shows input image, then each column shows the SHAP values for the classes from 1 to 9. 
shap.image_plot(shap_values, x_test[:3])
```

In the plot, red pixels contributed positively to the respective class, while blue pixels contributed negatively. Grey pixels have a near zero SHAP value.
The first row shows a 7 and, for example, in the 8th column we see positive contributions of the pixels that make up a 7.
The second row shows the image of a  2 and we can see that especially the start and end of the "2" contributed positively to the class "2" (3rd column).
The start of the "2" and the slope in the middle contributed negatively to a prediction of "1".

## SHAP with the Deep Explainer

We'll use the same process as above, but this time we'll use the DeepExplainer:

```{python}
#| output: false
explainer = shap.DeepExplainer(model, data = x_sample)
start_time = time.time()
shap_values = explainer.shap_values(x_test[:3])
deep_time = time.time() - start_time
```

Then, we plot the results again:

```{python}
shap.image_plot(shap_values, x_test[:3])
```

The SHAP values are very similar to the Gradient Explainer.
This makes sense, since in both cases the result should be the same SHAP values and the difference is only due to the fact that both are approximated.

## Time comparison

We measured the time for both gradient and deep explainer.

Let's examine the results:

```{python}
print('Gradient explainer: ', round(gradient_time, 2))
print('Deep explainer: ', round(deep_time, 2))
```

In theory, to obtain a reliable time comparison, you should repeat the calls several hundred times and avoid using other programs simultaneously.
However, this comparison was conducted only once on my MacBook with an M1 chip.
Keeping this in mind, we see a bit of difference.
Since only the CPU is used, the efficiency of calling the model could improve if a GPU were involved.
But I'd rather use the Gradient Explainer.
