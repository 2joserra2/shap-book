# Explaining MNIST Model with DeepExplainer and GradientExplainer

In the previous chapter, we explored the Partition explainer, where larger image patches were treated as features for SHAP.

In this chapter, we will explain image classifier classifications using a different approach, more akin to tabular data.
This entails two key aspects:

- assigning one Shapley value for each input pixel
- simulating feature absence (pixel absence) by replacing it with a pixel from a background dataset

Although the network receives individual pixels as input, we don't need to use the same granularity for explanations.

As we are working with a neural network, two model-specific tools are available:

- the gradient explainer, since neural networks often rely on gradients
- the deep explainer, which utilizes neural network layers to backpropagate Shapley values

Both methods are discussed in greater detail in the [estimation chapter](#estimation).
For this example, we will use the MNIST dataset.
The MNIST dataset contains 70,000 handwritten digits (0-9), each represented as a 28x28 pixel grayscale image.
The objective of the MNIST task is to create a machine learning algorithm capable of accurately classifying these images into their corresponding digit categories.
This well-established benchmark problem has been employed to evaluate the performance of various algorithms, such as neural networks, decision trees, and support vector machines.
The MNIST task is a supervised learning problem where the algorithm is trained on a subset of the dataset and tested on a separate set to gauge its accuracy.
Researchers in machine learning, computer vision, and pattern recognition have extensively used the MNIST dataset.

Why did I choose the MNIST dataset instead of ImageNet, as in the previous example?
Because using pixel-wise explanations with Gradient or Deep explainer requires sampling absent features using a background dataset.

Essentially, there are two options to replace a value with images:

- replace them with background data
- substitute them with a reference, such as blurring or replacing them with grey pixels (or another "neutral" color)
Imagine using the ImageNet dataset, where we have an image of a burger and replace the "absent" pixels with pixels from a dog image - it would result in a strange outcome.
However, for the MNIST dataset, this approach is more reasonable, as digits are more similar to each other, and replacing some pixels of a "2" with those of a "3" won't generate bizarre images.
I acknowledge that this is a somewhat vague argument, but generally, explanations for images can be more challenging.

Now, let's begin by training a neural network from scratch using TensorFlow:

```{python}
#| output: false
import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D
from tensorflow.keras.utils import to_categorical

# Load the MNIST dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Preprocess the data
x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)
x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)
x_train = x_train.astype('float32') / 255
x_test = x_test.astype('float32') / 255
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

# Define model architecture
model = Sequential()
model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(10, activation='softmax'))

# Compile model
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train model
model.fit(x_train, y_train, batch_size=128, epochs=5, validation_data=(x_test, y_test))
score = model.evaluate(x_test, y_test, verbose=0)

# Evaluate model on test set
print('Test loss:', score[0])
print('Test accuracy:', score[1])
```

CONTINUE HERE

TODO: INTERPRET loss

## Gradient Explainer

Next, explain with SHAP:

```{python}
import shap
import time

# x_test is large, take a sample
x_sample = shap.sample(x_test, 500)

# Pass list of inputs to explainer since we have two inputs
explainer = shap.GradientExplainer(model, data=x_sample)

# Explain model's predictions on first three test set samples
start_time = time.time()
shap_values = explainer.shap_values(x_test[:3])
gradient_time = time.time() - start_time
```
Let's briefly discuss the output.
The `shap_values` is a list with a length equal to the number of classes.
There are 10 classes, from "0" to "9", resulting in a list of length 10:

```{python}
print(len(shap_values))
```

For each model output, we obtain the Shapley values:

```{python}
print(shap_values[0].shape)
```

The first dimension represents the number of images for which we computed the Shapley values.
The second, third, and fourth dimensions contain the Shapley values in the form of an image since the input data was an image.

Let's plot the Shap values:

```{python}
shap.image_plot(shap_values, x_test[:3])
```

## Deep Explainer

We follow the same process, but use the DeepExplainer this time:

```{python}
#| output: false
explainer = shap.DeepExplainer(model, data = x_sample)
start_time = time.time()
shap_values = explainer.shap_values(x_test[:3])
deep_time = time.time() - start_time
```

Then, plot the results again:

```{python}
shap.image_plot(shap_values, x_test[:3])
```

TODO: Interpret results

TODO: Compare results, qualitatively the results with gradient explainer

Comparing with

## Time comparison between deep explainer

We measured the time for both gradient and deep explainer.

Let's examine the results:

```{python}
print("Gradient explainer: ", round(gradient_time, 2))
print("Deep explainer: ", round(deep_time, 2))
```

In theory, to obtain a reliable time comparison, you should repeat the calls several hundred times, avoid using other programs simultaneously, and so on.
However, I conducted this comparison only once on my MacBook with an M1 chip.
Nonetheless, the difference is evident.
Additionally, since only the CPU is used, calling the model is not as efficient as it could be, as there is no GPU involved.
