# SHAP for Linear Models {#linear}

::: {.callout-tip appearance="simple"}

By the end of this chapter, you will be able to:

- Calculate SHAP for linear models.
- Install and use `shap`.
- Interpret and visualize SHAP values.
- Analyze the model with summary and dependence plots.

:::

Let's start with a straightforward example.
This example is "simple" because we are using a linear regression model, which is naturally interpretable, or so they say.
This allows us to compare SHAP values with the coefficients of the model.

## The wine data

We will utilize the wine dataset from the UCI repository.
The objective is to predict wine quality based on its physicochemical properties.
The target variable "quality" is an average rating from three blind testers, ranging from 0 to 10.

Let's first examine the features in the data by downloading it.

```{python}
import pandas as pd
# Set the file URL and filename
url = 'https://archive.ics.uci.edu/ml/' \
      'machine-learning-databases/' \
      'wine-quality/winequality-white.csv'
file_name = 'wine.csv'

# Check if the file exists in the current directory
try:
    wine = pd.read_csv(file_name)
except FileNotFoundError:
    print(f'Downloading {file_name} from {url}...')
    wine = pd.read_csv(url,  sep=";")
    wine.to_csv(file_name, index=False)
    print('Download complete!')
```

Let's analyze the distributions of the features.

```{python}
#| output: asis
from tabulate import tabulate
summary = wine.describe().transpose().round(2)
summary = summary.drop("count", axis=1)
# Create a markdown table
markdown_table = tabulate(
  summary, headers='keys', tablefmt='pipe'
)
print(markdown_table)
```

As observed, the highest quality is 9 (out of 10), and the lowest is 3.
The other features have varying scales, but this is not an issue for SHAP values, as they explain the prediction on the outcome's scale.

## Fitting a linear regression model

With the wine dataset in our hands, we aim to predict the quality of a wine based on its physicochemical features.
A linear model for one data instance is represented as:

$$f(x^{(i)})=\beta_0+\beta_{1}x^{(i)}_{1}+\ldots+\beta_{p}x^{(i)}_{p}$$

where $x^{(i)}$ is the instance for which we want to compute the contributions.
Each $x^{(i)}_j$ is a feature value, with $j = 1,\ldots,p$.
The $\beta_j$ is the weight in the linear regression model corresponding to feature j.

Before fitting the linear model, let's divide the data into training and test sets.

```{python}
from sklearn.model_selection import train_test_split

# Extract the target variable (wine quality) from the data
y = wine['quality']
X = wine.drop('quality', axis=1)

X_train, X_test, y_train, y_test = train_test_split(
  X, y, test_size=0.2, random_state=42
)
```

We'll now train the linear regression model using the scikit-learn package.

::: {.callout-tip}

`shap` can be used with all sklearn models.

:::


```{python}
#| output: false
from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(X_train, y_train)
```
How does the model perform?
To evaluate, we calculate the mean absolute error (MAE) on the test data.

```{python}
from sklearn.metrics import mean_absolute_error
y_pred = model.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)
print(f"MAE: {mae:.2f}")
```
```{python}
#| echo: false
from IPython.display import display, Markdown
display(Markdown("""This indicates that, on average, the prediction deviates by {mae} from the actual value.""".format(mae=round(mae, 2))))
```

Next, we aim to understand how the model generates a prediction.
How is the predicted quality of a given wine related to its input features?

## Interpreting the coefficients

First, let's look at the coefficients and later compare them with the corresponding SHAP values.

```{python}
#| output: asis
import numpy as np
coefs = pd.DataFrame({
  'feature': X.columns.values,
  'coefficient': np.round(model.coef_, 3)
})
print(coefs.to_markdown(index=False))
```

Interpretation:

- For instance, increasing the fixed acidity of a wine by 1 unit raises the predicted quality by 0.046.
- Increasing the density by 1 unit reduces the predicted quality by -124.264.
- Volatile acidity, citric acid, chlorides, total sulfur dioxide, and density negatively affect the predicted quality.

## Model coefficients provide a global perspective

The coefficient for a feature is constant across all data points, rendering the coefficient a "global" model interpretation.

For a specific data point (a local explanation), we can also consider the feature value.
Consider a coefficient $\beta_j=0.5$.
The feature value could be 0, 0.008, or -4, yielding different results when multiplied with the feature value (0, 0.004, -2).
A more meaningful way to present a feature effect would be the product of the coefficient and the feature value: $\beta_j \cdot x^{(i)}_j$.

However, this formula is also "incomplete" as it doesn't provide an understanding of whether the contribution is large (in absolute terms) or not.
Suppose the contribution is $\beta_j \cdot x^{(i)}_j = 2$, with $\beta_j=0.5$ and $x^{(i)}_j=4$.
Is that a large contribution?
It depends on the range of $x_j$.
If 4 is the smallest possible value of $x^{(i)}_j$, then 2 is actually the smallest possible contribution, but if $x^{(i)}_j$ follows a Normal distribution centered at 0, then a contribution of 4 is more than expected.
An easy solution is to center the effect $\beta_j x^{(i)}_j$ around the expected effect $E(\beta_j X_j)$.
As we'll see in the coming section, this is the same as how the SHAP values are defined.

## Theory: SHAP for linear models

For linear regression models without interaction terms, the computation of SHAP values is straightforward since the model only has linear relations between the feature and target and no interactions.
The SHAP value $\phi^{(i)}_j$ of the j-th feature on the prediction $f(x^{(i)}_j)$ for a linear regression model is defined as:

$$\phi^{(i)}_j=\beta_{j}x^{(i)}_j-\mathbb{E}(\beta_{j}X_{j})=\beta_{j}(x^{(i)}_j - \mathbb{E}(X_{j}))$$
Where $\mathbb{E}(\beta_jX_{j})$ denotes the expected effect estimate for feature j, the contribution refers to the difference between the feature effect and the average effect.
Here's why SHAP has such a simplified form for linear models:
When calculating the marginal contribution $v(S \cup \{j\}) - v(S)$, all components of the linear function cancel out, leaving only those associated with feature $j$.
In $v(S \cup \{j\})$, only $\beta_j x^{(i)}_j$ remains, while in $v(S)$, only $\beta_j \mathbb{E}(X_j)$ persists.
All marginal contributions remain constant due to the absence of feature interactions, hence, the coalition to which we add feature $j$ is irrelevant.

We simply replace the expectation with the mean for the estimation:

$$\phi^{(i)}_j = \beta_{j}\left(x^{(i)}_j - \frac{1}{n}\sum_{k=1}^n(x^{(k)}_{j})\right)$$

Fantastic!
Now we are aware of each feature's contribution to the prediction.
Let's verify the efficiency axiom:

\begin{align*}
\sum_{j=1}^{p}\phi^{(i)}_j=&\sum_{j=1}^p(\beta_{j}x^{(i)}_j-\mathbb{E}(\beta_{j}X_{j}))\\
=&\beta_0+\sum_{j=1}^p\beta_{j}x^{(i)}_j-(\beta_0+\sum_{j=1}^{p}\mathbb{E}(\beta_{j}X_{j}))\\
=&f(x)-\mathbb{E}(f(X))
\end{align*}

This is the predicted value for data point x subtracted from the average predicted value.  Feature contributions can be negative.
Now, let's apply these to the wine quality prediction.

## Installing `shap`

Although the computation of SHAP for linear models is straightforward enough to implement on our own, we'll take the easier route and install the `shap` library, which also provides extensive plotting functions and other utilities.

::: {.callout-note}

## `shap` library

The `shap` library was developed by Scott Lundberg, the author of the SHAP paper [@lundberg2017unified] and many other SHAP-related papers.
The [initial commit](https://github.com/slundberg/shap/tree/7673c7d0e147c1f9d3942b32ca2c0ba93fd37875) was made on November 22nd, 2016.
At the time of writing, the library has over 2000 commits.
`shap` is open-source and hosted on Github, allowing public access and tracking of its progress.
The repository has received over 19k stars and almost 3k forks.
In terms of features, it's the most comprehensive library available for SHAP values.
I believe that the `shap` library is the most widely-used implementation of SHAP values in machine learning.

You can find the `shap` repository at: https://github.com/slundberg/shap

:::

Like most Python packages, you can install `shap` using `pip`.

```{sh}
pip install shap
```

All examples in this book utilize `shap` version 0.42.0.
To install this exact version, execute the following command:

```{sh}
pip install shap==0.42.0
```


### If you use virtualenv {-}

If you're using virtualenv or venv, activate the environment first.
Assuming the environment is named venv:

```{sh}
source venv/bin/activate
pip install shap
```

### If you use conda {-}

If you're using conda, use the following commands to install `shap`:

```{sh}
conda install -c conda-forge shap
```

For the version used in this book:

```{sh}
conda install -c conda-forge shap=0.42.0
```


## Computing SHAP values
To better comprehend the effects of features, we can calculate SHAP values for a single data instance.
For this, we construct a `LinearExplainer` object.

```{python}
import shap
explainer = shap.LinearExplainer(model, X_train)
```

::: {.callout-note}

While the model here is a `LinearRegression` model from the `sklearn` library, `shap` works with any model from `sklearn` as well as with other libraries such as `xgboost` and `lightgbm`.
`shap` also works with custom prediction functions, so it's quite flexible!

:::


Alternatively, we could utilize the `Explainer` as follows:

```{python}
shap.Explainer(model, X_train)
```

This is also a `Linear` explainer object.
The advantage of the `algorithm='auto'` option, which is the default setting when creating an Explainer, is that `shap` identifies the model as a linear regression model and selects the efficient linear explainer.

There is another way, which involves directly choosing the appropriate `algorithm` in the explainer:

```{python}
shap.Explainer(model, X_train, algorithm='linear')
```

To ultimately calculate SHAP values, we call the explainer with the data to be explained.

```{python}
shap_values = explainer(X_test)
```

::: {.callout-note}

When constructing a prediction model, you divide training and testing data to prevent overfitting and to achieve a fair evaluation.
Although the risk of overfitting doesn't apply in the same way to SHAP, it's considered best practice to use training data for the Explainer (i.e., for the background data) and compute explanations for new data.
This separation prevents a data point's feature values from being "replaced" by its own values.
It also means we calculate explanations for fresh data that the model hasn't previously encountered.
However, I must confess that I haven't seen much research on dataset choices, so take this information with a pinch of salt.

:::

Next, let's examine the SHAP values.

While initially looking a bit messy, it's insightful to inspect the `Explanation` object.
The Explanation object includes `.values`, `.base_values`, and `.data` fields.
The `.values` represent the SHAP values as $n \times p$ array, `.base_values` is the average prediction (consistent for each data point), and `.data` contains the feature values.
Each element in these arrays corresponds to one data point in `X_test`, which we used to calculate the SHAP values.

```{python}
print(shap_values.values)
```

However, having only the raw SHAP values isn't particularly useful.
The true power of the `shap` library lies in its various visualization capabilities.

## Interpreting SHAP values

We have yet to see the SHAP values.
So, let's visualize the SHAP values for the first data instance:

```{python}
shap.plots.waterfall(shap_values[0])

```

::: {.callout-note}

## The Waterfall Plot

- The plot is dubbed "waterfall" because each step resembles flowing water. Water can flow in either direction, just as SHAP values can be positive or negative. Positive SHAP values point to the right.
- The y-axis exhibits the individual features, along with the values for the selected data instance.
- The feature values are ordered by the magnitudes of their SHAP values.
- The x-axis is on the scale of SHAP values
- Each bar signifies the SHAP value for that specific feature value.
- The x-axis also shows the estimated expected prediction $\mathbb{E}(f(X))$ and the actual prediction of the instance $f(x^{(i)})$.
- The bars start at the bottom from the expected prediction and add up to the actual prediction.

:::

```{python}
#| echo: false
from IPython.display import display, Markdown

i = 0
y = model.predict(X_test)[i]
bv = shap_values.base_values[0]
diff = y - bv

feature1 = "residual sugar"
ind = X_test.columns.get_loc(feature1)
fv1 = X_test.iloc[i, ind]
sv1 = shap_values.values[i,ind]

feature2 = "free sulfur dioxide"
ind = X_test.columns.get_loc(feature2)
fv2 = X_test.iloc[i, ind]
sv2 = shap_values.values[i,ind]

feature3 = "alcohol"
ind = X_test.columns.get_loc(feature3)
fv3 = X_test.iloc[i, ind]
sv3 = shap_values.values[i,ind]

display(Markdown("""
Interpretation: The predicted value of {y} for instance {i} differs from the average prediction of {base_value} by {diff}.

- {feature1}={fv1} contributed {sv1}
- {feature2}={fv2} contributed {sv2}
- {feature3}={fv3} contributed {sv3}
- ...

The sum of all SHAP values equals the difference between the prediction ({y}) and expected value ({base_value}).
""".format(base_value=round(bv, 2), y=np.round(y, 2), i=i, diff=np.round(diff, 2),
           feature1=feature1, fv1=round(fv1, 2), sv1=np.round(sv1, 2),
           feature2=feature2, fv2=round(fv2, 2), sv2=np.round(sv2, 2),
           feature3=feature3, fv3=round(fv3, 2), sv3=np.round(sv3, 2))))
```

:::{.callout-tip}

## Interpretation Template *(replace [] with your data)*

Prediction [$f(x)$] for instance [$i$] differs from the average prediction [$\mathbb{E}(f(X))$] by [$f(xi) − \mathbb{E}(f(X))j$] to which [feature name = feature value] contributed [$\phi^{(i)}_j$].

:::

Here's an interesting analogy to view SHAP values:
Imagine the prediction as an option floating in a 1-dimensional universe.
It's starting point is the average prediction of the background data -- this is our "center of gravity".
Each feature of the data point can be seen as a force acting on this object and can either push it up or down.
The SHAP value describes how strong each forces is and in which direction it pushes.
Eventually, these forces reach equilibrium, which represents the predicted value.

Alright, back from this little universe to our example. Several observations can be made:

- The most influential feature was 'residual sugar' (=10.8), with a SHAP value of 0.32, indicating it had an increasing impact on the quality on average.
- Overall, the prediction surpassed the average, suggesting a high-quality wine.
- Most of this wine's feature values were assigned a positive SHAP value.
- The feature 'pH' with a value of 3.09 had the largest negative SHAP value.

Let's examine another data point:

```{python}
shap.waterfall_plot(shap_values[1])
```

This wine has a similar predicted rating to the previous one, but the contributions to this prediction differ.
It has two substantial positive contributions from the 'density' and 'alcohol' values, but also two strong negative factors: 'volatile acidity' and 'residual sugar'. 

The waterfall plot lacks context for interpretation.
For instance, while we know 'residual sugar' increased the prediction for the first wine, we cannot deduce from the waterfall plot alone whether low or high levels of 'residual sugar' are associated with small or large SHAP values.

## Global model understanding

We computed SHAP values to explain individual predictions.
However, we can also compute SHAP values for more data points, ideally for the entire (test) dataset (and training data as background data).
By visualizing the SHAP values across all features and multiple data points, we can uncover patterns of how the model makes predictions.
This gives us a global model interpretation.

::: {.callout-note}

## Global versus local interpretation

SHAP values allow for a local interpretation -- how features contribute to a prediction.
Global interpretations focus on *average model behavior*, which includes how features affect the prediction, how important each feature was for the prediction, and how features interact.
:::

We previously computed the SHAP values for the test data, which are now stored in the `shap_values` variable.
We can create a summary plot from this variable for further insights into the model.

```{python}
shap.plots.beeswarm(shap_values)
```

::: {.callout-note}

## Summary plot

- Also known as beeswarm plot.
- The x-axis represents the SHAP values, while the y-axis shows the features, and the color indicates the feature's value.
- Each row corresponds to a feature.
- The feature order is determined by importance, defined as the average of absolute SHAP values: $I_j = \frac{1}{n}\sum_{i=1}^n \phi^{(i)}_j$
- Each dot represents the SHAP value of a feature for a data point, resulting in a total of $p \cdot n$ dots.

:::

The summary plot automatically ranks features based on importance.
Density, residual sugar, and alcohol are the most important features for predicting wine quality, according to SHAP values.

The coloring also reveals that the relationships are monotonic for all features:
A feature's increase (or decrease) consistently influences the prediction in one direction.
Since the model is a linear regression model, the modeled relationship between each input feature and the target must be linear.
That means a 1 unit increase in a feature always increases the prediction by $\beta_j$, the corresponding coefficient from the linear model.
However, that's just true for the linear model.
The "true" relationship in the data might be non-linear.

The coloring for each feature in the summary plot reveals the direction of the effect a feature has:
For example higher density wines are associated with lower SHAP values, while wines with more residual sugar have a higher corresponding SHAP value.
Again, this is information that directly corresponds to the coefficients from the linear regression model.
In later chapters we will see examples with more insightful summary plots.

::: {.callout-tip}

## How to interpret the summary plot

- Observe the ranking of the features. The higher the feature, the greater its SHAP importance.
- For each feature of interest:
  - Examine the distribution of the SHAP values. This provides insight into the various ways the feature values can influence the prediction. For instance, a wide spread indicates a broad range of influence.
  - Understand the color trend for a feature: This offers an initial insight into the direction of a feature effect and whether the relationship is monotonic or exhibits a more complex pattern.
  - Look for color clusters that may indicate interesting data clusters. Not relevant for linear models, but for non-linear ones.

:::



## Comparison between coefficients and SHAP values

Now, we'll explore a new type of plot - the SHAP dependence plot.
The dependence plot should confirm that the SHAP values also exhibit a linear relationship with the target for features known to have a linear relation.

```{python}
shap.plots.scatter(shap_values[:, 'alcohol'])
```

::: {.callout-note}

## The dependence plot

- Also referred to as scatter plot.
- Mathematically, the plot contains these points: $\{(x^{(i)}_j,\phi^{(i)}_j)\}_{i=1}^n$.
- The x-axis represents the feature value, and the y-axis represents the SHAP value.
- Highlighting feature interactions on the dependence plot can enhance its effectiveness.
- The dependence plot is similar to the summary plot for a single feature, but instead of using color to represent the feature value, these values are distributed across the x-axis.
- The grey histogram indicates the distribution of the feature values. Ranges with little data should be interpreted more cautiously. 

:::

This plot demonstrates the global dependence modeled by the linear regression between alcohol and the corresponding SHAP values for alcohol.
The dependence plot will be much more insightful for non-linear model, but it's a great way to confirm that the SHAP values reflect the linear relationship in the case of a linear regression model.
As the alcohol content increases, the corresponding SHAP value also increases linearly.
This increase corresponds to the slope in the linear regression model:

```{python}
feature = 'alcohol'
ind = X_test.columns.get_loc(feature)
coefs.coefficient[ind]
```

A visual inspection of the dependence plot confirms the same slope, as the plot ranges from (8, -0.6) to (14, 0.8), resulting in a slope of $(0.8 - (-0.6))/(14 - 8) \approx 0.23$
