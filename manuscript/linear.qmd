# SHAP for Linear Models {#linear}

Let's begin with a simple example.
"Simple", because we'll start with a linear regression model which is already interpretable, allowing us to compare SHAP values with our intuitive understanding of the model.

## What about coefficients?

But why do we need SHAP values for linear regression anyway?
Couldn't we just look at the coefficients?[^global-and-local]

We could, but that would only tell half of the story.
The coefficient for a feature is the same for all data points, which means you can see the coefficient as a "global" model interpretation.

But for a specific data point (= local explanation), we can also make use of the information of the feature value.
Imagine we have coefficient $\beta_j=0.5$.
It matters whether the feature value is, for example, 0, 0.008, or -4, because multiplication will produce different results (0, 0.004, -2).
A more meaningful way to present a feature effect would be the coefficient multiplied with the feature value: $\beta_j \cdot x_{ij}$.

However, this formula is also "incomplete" as it leaves us without any sense of whether the contribution is large or small.
Let's say that the contribution is $\beta_j \cdot x_{ij} = 2$, again with $\beta_j=0.5$ and $x_{ij}=4$.
Is that a lot?
That depends on the range of $x_j$.
If 4 is the lowest possible value of $x_{j}$, then 2 is actually the smallest possible contribution, but if $x_j$ follows a Normal distribution centered at 0, then a contribution of 4 is more than expected.
An easy fix is to center the effect $\beta_j x_{ij}$ around the expected effect $E(\beta_j X_j)$.
It's not coincidence that the result is also the SHAP value $\phi_j$ for a feature in a linear model:

$\phi_{ji}(\hat{f}) = \beta_j x_{ij} - E(\beta_jX_j) = \beta_{j} (x_{ij} - E(X_j))$.

Let's see how this works in the wine data.

## The wine data

We will be using the wine dataset available from the `scikit-learn` library.
The goal is to predict wine quality based on its physico-chemical attributes.
The target variable "quality" is the average rating from three blind testers and ranges from 0 to 10.

Now, let's take a quick look at what the features look like in the data.
First, we download the data.


```{python}
import pandas as pd
# Set the file URL and filename
url = 'https://archive.ics.uci.edu/ml/' \
      'machine-learning-databases/' \
      'wine-quality/winequality-white.csv'
file_name = 'wine.csv'

# Check if the file exists in the current directory
try:
    wine = pd.read_csv(file_name)
except FileNotFoundError:
    print(f'Downloading {file_name} from {url}...')
    wine = pd.read_csv(url, header=None, sep=";")
    wine.to_csv(file_name, index=False)
    print('Download complete!')
```

To examine the features, let's take a look at their distributions.

```{python}
#| output: asis
from tabulate import tabulate
summary = wine.describe().transpose().round(2)
summary = summary.drop("count", axis=1)
# Create a nice markdown table
markdown_table = tabulate(
  summary, headers='keys', tablefmt='pipe'
)
print(markdown_table)
```

Here, we observe that the highest quality is 9 (out of a possible 10) and the lowest is 3.
The other features have different scales, but as we will discover, this is not an issue for SHAP values, since they explain the prediction **on the scale of the outcome**.

## Fitting a linear regression model

Now that we have our wine dataset, we want to predict the quality of a wine based on its chemical features.
Since we've decided to use a linear regression model, let's proceed with that.
Before that, though, let's split the data into training and test data.

```{python}
from sklearn.model_selection import train_test_split
# Extract the target variable (wine quality) from the data
y = wine['quality']
# Remove the target variable from the data
X = wine.drop('quality', axis=1)
X_train, X_test, y_train, y_test = train_test_split(
  X, y, test_size=0.2, random_state=42
)
```

Next, we'll train the linear regression model using the scikit-learn package.

```{python}
#| output: false
from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(X_train, y_train)
```
How well does the model perform?
To evaluate its performance, we calculate the mean absolute error (MAE) on the test data.

```{python}
from sklearn.metrics import mean_absolute_error

y_pred = model.predict(X_test)
# Calculate the mean absolute error 
mae = mean_absolute_error(y_test, y_pred)

# Print the mean absolute error
print(f"MAE: {mae:.2f}")
```

```{python}
#| echo: false
from IPython.display import display, Markdown
display(Markdown("""This indicates that, on average, the prediction deviates by {mae} from the actual value.""".format(mae=round(mae, 2))))
```
This appears sufficient for now.
Next, we aim to comprehend how the model generates a prediction.

For a given wine, how is the predicted quality connected to the input features?

## Interpreting the coefficients

First, let's examine the coefficients and then compare them to the corresponding SHAP values.

```{python}
#| output: asis
import numpy as np
coefs = pd.DataFrame({
  "feature": X.columns.values,
  "coefficient": np.round(model.coef_, 3)
})
print(coefs.to_markdown(index=False))
```
Interpretation:

- For example, increasing the fixed acidity of a wine by 1 unit increases the predicted quality by 0.046.
- Volatile acidity, citric acid, chlorides, total sulfur dioxide, and density have a negative effect.

However, this information alone does not help us understand the effect of these factors on a single prediction, as it also depends on the specific value of the feature for that data instance.

## Computing SHAP values

To better understand the feature effects, we can compute SHAP values for one of the data instances.
To do this, we create a `LinearExplainer` object.

```{python}
import shap
explainer = shap.LinearExplainer(model, X_train)
```
Alternatively, one could use the `Explainer` like this:

```{python}
shap.Explainer(model, X_train)
```

This is also a `Linear` explainer object.
That's the magic of the `algorithm='auto'` option, which is the default when creating an Explainer.
In this case, `shap` identifies the model as a linear regression model and selects the efficient linear explainer.

There is another way, which involves directly choosing the appropriate `algorithm` in the explainer:

```{python}
shap.Explainer(model, X_train, algorithm='linear')
```

And to finally compute SHAP values we call the `.shap_values` method from the explainer.

```{python}
shap_values = explainer.shap_values(X_test)
```

## Interpreting SHAP values

We still haven't seen the SHAP values.
So, let's take the first data instance and visualize its SHAP values:

```{python}
exp = shap.Explanation(
  values=shap_values[0],
  base_values=explainer.expected_value,
  data=X_test.values[0],
  feature_names=wine.columns
)

shap.waterfall_plot(exp)
```

Observations:

- The most crucial feature was residual sugar (=10.8), with a SHAP value of 0.27, signifying it had an increasing effect on the quality.
- Overall, the prediction was higher than the average, indicating good quality wine.
- Most feature values of this wine were assigned a positive SHAP value.
- pH of 3.09 is the feature value with the largest negative SHAP value.
This plot provides a nice overview, but it somewhat omits crucial information that one often desires.
For instance, in the case of residual sugar, we know it increased the prediction, but we cannot determine from the waterfall plot alone what happens to the prediction if the value of residual sugar (10.8) increased or decreased.
We could infer this from the coefficient, but this will change in the next chapter when we discuss additive models.

## Global model understanding

::: {.callout-note}

## Global versus local interpretation

SHAP values allows a local interpretation -- how features affected a prediction.
Global interpretations are about how the model behaves *on average*.
This includes how features affect the prediction, how important each feature was for the prediction and how features interact.

:::


Additionally, we lack a global understanding of the most important features.
To address this, we can use the summary plot:

```{python}
shap.summary_plot(shap_values, X_test, feature_names=wine.columns)
```
This plot automatically ranks the features by importance.
Density, residual sugar, and alcohol are the most important features for predicting wine quality, according to SHAP values.
The importance ordering is determined by the average absolute SHAP values across the test data (in this case).

The coloring also shows us that the relationships are monotonic for all features:
Increasing (or decreasing) a feature always increases the prediction, and only in one direction.
Since we know that the model is a linear regression model, we also know that this relationship must be linear.

## Comparing coefficients and SHAP values

Now, let's introduce a new kind of plot - the SHAP dependence plot.
With this plot, we can hopefully confirm that the SHAP values also show a linear relationship with the target for features we know have a linear relation:

```{python}
shap.dependence_plot(
  'alcohol', shap_values, X_test,
  feature_names=wine.columns,
  interaction_index=None
)
```
This plot illustrates the global dependence modeled by the linear regression between alcohol and the predicted wine quality.
It's evident that as alcohol content increases, so does the predicted quality of the wine.

Here are a few more explanations about the dependence plot:

- I set the interaction index argument to avoid attempting to detect interactions with other features, as it's a linear model and detecting interactions wouldn't make sense.
- If we don't provide the feature names, we need to use an index in the first position of the features we're interested in.

We can observe that the SHAP value increases linearly with each increment in the feature.
This increase corresponds to the slope in:

```{python}
feature = 'alcohol'
ind = X_test.columns.get_loc(feature)
coefs.coefficient[ind]
```

Eye-balling the dependence plot confirms the same slope, since the plot goes from (8, -0.6) to (14, 0.8), which is a slope of $(0.8 - (-0.6))/(14 - 8) \approx 0.23$

In a way, linear regression is always an easy example.
The assumption that everything is linear makes interpretation so easy.
So in the next chapter, we will juice it up a little and allow non-linear functions.

