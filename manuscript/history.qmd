# A Short History of SHAP and Shapley Values {#history}

::: {.callout-tip appearance="simple"}

By the end of this chapter, you will be able to:

- Understand the key historical milestones of SHAP.
- Explain the relationship between SHAP and Shapley values.

:::

This chapter offers a succinct overview of the history of SHAP and Shapley values, focusing on their chronological development.
The history is divided into three parts, each highlighted by a milestone:

- 1953: The introduction of Shapley values in game theory.
- 2010: The initial steps toward applying Shapley values in machine learning.
- 2017: The advent of SHAP, a turning point in machine learning.


## Lloyd Shapley's pursuit of fairness

Shapley values have greater importance than might initially be apparent from the book.
These values are named after their creator, Lloyd Shapley, who first introduced them in 1951.
In the 1950s, game theory saw an active period, during which many core concepts were formulated, including repeated games, the prisoner's dilemma, fictitious play, and, of course, Shapley values.
Lloyd Shapley, a mathematician, was renowned in game theory, with fellow theorist Robert Aumann calling him the ["greatest game theorist of all time"](https://www.wsj.com/articles/lloyd-shapley-won-the-nobel-prize-for-economics-1923-2016-1458342678).
After World War II, Shapley completed his PhD at Princeton University with a thesis titled "Additive and Non-Additive Set Functions."
In 1953, his paper "A Value for n-Person Games" [@shapley1953value] introduced Shapley values.
In 2012, Lloyd Shapley and Alvin Roth were awarded the Nobel Prize in Economics[^fake-nobel] for their work in "market design" and "matching theory."

Shapley values serve as a solution in cooperative game theory, which deals with games where players cooperate to achieve a payout.
They address the issue of a group of players participating in a collaborative game, where they work together to reach a certain payout.
The payout of the game needs to be evenly distributed among the players, who may have contributed differently.
Shapley values provide a mathematical method of fairly dividing the payout among the players.

Shapley values have since become a cornerstone of coalitional game theory, with applications in various fields such as political science, economics, and computer science.
They are frequently used to determine fair and efficient strategies for resource distribution within a group, including dividing profits among shareholders, allocating costs among collaborators, and assigning credit to contributors in a research project.
However, Shapley values were not yet employed in machine learning, which was still in its early stages at the time.

## Early days in machine learning

Fast forward to 2010.
Shapley hadn't yet received his Nobel Prize in Economics, but the theory of Shapley values had been established for nearly 60 years.
In contrast, machine learning had made tremendous strides during this period.
In 2012, the ImageNet competition [@deng2009imagenet], led by Fei-Fei Li, was won for the first time by a team using a deep neural network (AlexNet) with a significant lead over the runner-up.
Machine learning continued to advance and attract more research in many other ways.

While Shapley values had previously been defined for linear models, 2010 marks the beginning of model-agnostic estimation of Shapley values.
In 2010, Erik Štrumbelj and Igor Kononenko published a paper titled "An efficient explanation of individual classifications using game theory" [@strumbelj2010efficient], proposing the use of Shapley values to explain machine learning model predictions.
In 2014, they further developed their methodology for computing Shapley values [@strumbelj2014explaining].

However, this approach did not immediately gain popularity.
Some possible reasons why Shapley values were not widely adopted at the time include:

- Explainable AI/Interpretable machine learning was not as widely recognized.
- The papers by Štrumbelj and Kononenko did not include code.
- The estimation method was still relatively slow and not suitable for image or text classification.

Next, we will look at the events that led to the rise of Shapley values in machine learning.

## The SHAP Cambrian explosion
 
In 2016, @ribeiro2016should published a paper introducing Local Interpretable Model-Agnostic Explanations (LIME), a method that uses local linear regression models to explain predictions.
This paper served as a catalyst for the field of explainable AI and interpretable machine learning.
A more cautious claim might be that the paper's publication coincided with a growing interest in interpreting machine learning models.
The prevailing sentiment at the time was a concern over the complexity of advanced machine learning algorithms, such as deep neural networks, and the lack of understanding of how these models generate their predictions.

Shortly after the LIME paper, in 2017, Scott Lundberg and Su-In Lee published a paper titled "A Unified Approach to Interpreting Model Predictions" [@lundberg2017unified].
This paper introduced SHapley Additive exPlanations (SHAP), another method to explain machine learning predictions.
The paper was published at NIPS, now known as NeurIPS[^neurips].
NeurIPS is a major machine learning conference, and if your research is published there as a machine learning researcher, it's more likely to draw attention.
But what exactly did the SHAP paper introduce, given that Shapley values for machine learning were already defined in 2010/2014?

Lundberg and Lee presented a new way to estimate SHAP values using a weighted linear regression with a kernel function to weight the data points[^new-estimator].
The paper also demonstrated how their proposed estimation method could integrate other explanation techniques, such as DeepLIFT [@shrikumar2017learning], LIME [@ribeiro2016should], and Layer-Wise Relevance Propagation [@bach2015pixel].


[^unification]: The notion of unification is somewhat misleading.
While a framework that connects different methods is helpful, SHAP doesn't truly unify these methods as it requires modifications to other methods to produce SHAP values.
This is a separate discussion.

[^new-estimator]: The `shap` package no longer uses Kernel SHAP, rendering the paper somewhat historical.

Here's why I believe SHAP gained popularity:

- It was published in a reputable venue (NIPS/NeurIPS).
- It was a pioneering work in a rapidly growing field.
- Ongoing research by the original authors and others contributed to its development.
- An open-source Python implementation (`shap` package) was available.

The availability of open-source code played a significant role, as it enabled people to integrate SHAP values into their projects.


::: {.callout-note}

## Naming conventions

The naming can be slightly confusing for several reasons:

- Both the method and the resulting numbers can be referred to as Shapley values (and SHAP values).
- @lundberg2017unified renamed Shapley values for machine learning as SHAP, an acronym for SHapley Additive exPlanations.

This book will adhere to these conventions:

- Shapley values: the original method from game theory.
- SHAP: the application of Shapley values for interpreting machine learning predictions.
- SHAP values: the resulting values from using SHAP for the features.
- `shap`: the library that implements SHAP.

"SHAP" is similar to a brand name used to describe a product category, like Post-it, Jacuzzi, Frisbee, or Band-Aid.
I chose to use it since it's well-established in the community and it distinguishes between the general game-theoretic method of Shapley values and the specific machine learning application of SHAP.

:::
Since its inception, SHAP's popularity has steadily increased.
A significant milestone was reached in 2020 when @lundberg2020local proposed an efficient computation method specifically for SHAP, targeting tree-based models.
This advancement was crucial because tree-boosting excels in many applications, enabling rapid estimation of SHAP values for state-of-the-art models.
Another remarkable achievement by Lundberg involved extending SHAP beyond individual predictions.
He stacked SHAP values, similar to assembling Legos, to create global model interpretations.
This method was made possible by the fast computation designed for tree-based models.
Thanks to numerous contributors, Lundberg continued to enhance the `shap` package, transforming it into a comprehensive library with a wide range of estimators and functionalities.
Besides Lundberg's work, other researchers have also contributed to SHAP, proposing [extensions](#extensions).
Moreover, SHAP has been implemented in other contexts, indicating that the `shap` package is not the only source of this method.

Given this historical context, we will begin with the theory of Shapley values and gradually progress to SHAP.


[^fake-nobel]: It's not the real Nobel prize, but the "Nobel Memorial Prize in Economic Sciences." Officially, it's termed the "Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel." This prize is a kind of surrogate Nobel award created by economists since they were not included in the original five Nobel Prizes.

[^neurips]: The name NIPS faced criticism due to its association with "nipples" and its derogatory usage against Japanese individuals, leading to its change to NeurIPS.
