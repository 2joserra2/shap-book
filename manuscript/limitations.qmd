## Limitations of Shapley Values


There also has been critique of Shapley values.
And they have some limitations.
This chapter helps you understand these limitations.
If there are easy fixes, they are hinted at.

Some seem very discouraging.


Papers:

- [@sundararajan2020many]
- https://arxiv.org/pdf/2002.11097.pdf 
- http://proceedings.mlr.press/v119/sundararajan20b/sundararajan20b.pdf [@sundararajan2020many]
- https://www.tandfonline.com/doi/abs/10.1198/tast.2009.08199?role=button&needAccess=true&journalCode=utas20


### Unrealistic Data through Marginal Sampling

Problem: Shapley values use mostly marginal sampling

Produces unrealistic data
Not good for interpretation.

Many remedies, but most not perfect
An imperfection to live with

Solutions:

- reduce correlations
- remove correlated features
- Use Partition explainer
- User Tree or Linear with the conditional options
- Compare conditional and marginal
- ...


TODO: simulate two correlated variables x1 and x2 

First we simulate two variables X1 and X2.
Imagine them as features in a machine learning model.

```{python}
import numpy as np

p = 0.8
mean = [0, 0] # mean vector
cov = [[1, p], [p, 1]] # covariance matrix
n = 100 # number of samples

x1, x2 = np.random.multivariate_normal(mean, cov, n).T # generate the samples
```


```{python}
# select one point from the distribution
index = np.random.randint(0, n)
point = (x1[index], x2[index])
```


```{python}
import matplotlib.pyplot as plt

# set number of samples for conditional distribution
m = 10

# create marginal and conditional distribution
x2_cond = np.random.normal(loc=p*x1[index], scale=np.sqrt(1-p**2), size=m)
x2_marg = np.random.choice(x2, size=m)

# create scatter plot with fixed x1 and variable x2
plt.subplot(121)
plt.scatter(x1, x2, color='black', alpha=0.1)
plt.scatter(np.repeat(x1[index], m), x2_cond, color='green')
plt.scatter(point[0], point[1], color='red')
plt.subplot(122)
plt.scatter(x1, x2, color='black', alpha=0.1)
plt.scatter(np.repeat(x1[index], m), x2_marg, color='blue')
plt.scatter(point[0], point[1], color='red')
plt.xlabel('x1')
plt.ylabel('x2')

plt.show()
```



### Problems Arising From the Additivity constraint

- see 3.2 from paper https://arxiv.org/pdf/2002.11097.pdf
- TODO: Read section from paper
- TODO: rename section to something more concrete once issue is clear
- function to analzye: $f(x) = \prod_{j=1}^p x_j$
- purely multiplicative function
- first of all, what would you expect?
- without further information, it's hard to tell
- more information: all variables are independent and have expectation of zero
- then all variables will get equal shapley value
- even if they have different scales CITE https://arxiv.org/pdf/2002.11097.pdf
- the paper states "Shapley values are touted for their “model-agnostic” quality, but under the lens of a particular interpretation, this is not the case"
- however, I disagree
- first, this is a misinterpretation of model-agnostic, as it's a more technical term
- and it's just something that you have to know
- and second of all:
- if you have two features x1 and x2
- both centered around 0
- but x1 has higher variance, and spans from -100 to +100
- x2 spans only from -1 to 1
- but the way they work in the model is purely multiplicative
- maybe it's okay to say that they contribute equally
- because what would be the interpretation of a higher Shapley importance for X1
- $\Rightarrow$ that it's more important to "know" X1 than X2
- but that's totally not the case
- becaue X2 can completely change the prediciton, even if it's on a much smaller scale
- it can set it to zero
- X2 can flip the signs
- ...
- for me it makes sense 


### No consensus what an attribution should look like

- TODO: find a reference
- TODO: bring arguments of BAyesian vs. freq unsettled
-

### Shapley values aren't contrastive

- A common argument is that human understand explanations better when they are contrastive
- like counterfactuals
- TODO: cite Miller
- but on the other hand, people should be trained to understand Shapley values
- so nothing you just serve the user 


### Shapley values don't enable actions (by the user)

- let's say you applied for a loan
- get rejected
- based on ML model
- you get the Shapley values
- then without further information, you wouldn't know how to improve your situation
- ofc completely unrealistic scenario, bc. the bank would never be that transparent
- but a good illustration
- indeed, shapley values are not useful for that
- if no further context is given 
- here I would suggest, as solution, to use something like counterfactual explanations
- you can also add more context with SHAP summary plots
- again, I wouldn't serve Shap values to untrained users

### Advantages

The difference between the prediction and the average prediction is **fairly distributed** among the feature values of the instance -- the Efficiency property of Shapley values.
This property distinguishes the Shapley value from other methods such as [LIME](#lime).
LIME does not guarantee that the prediction is fairly distributed among the features.
The Shapley value might be the only method to deliver a full explanation.
In situations where the law requires explainability -- like EU's "right to explanations" -- the Shapley value might be the only legally compliant method, because it is based on a solid theory and distributes the effects fairly.
I am not a lawyer, so this reflects only my intuition about the requirements.

The Shapley value allows **contrastive explanations**.
Instead of comparing a prediction to the average prediction of the entire dataset, you could compare it to a subset or even to a single data point.
This contrastiveness is also something that local models like LIME do not have.

The Shapley value is the only explanation method with a **solid theory**.
The axioms -- efficiency, symmetry, dummy, additivity -- give the explanation a reasonable foundation.
Methods like LIME assume linear behavior of the machine learning model locally, but there is no theory as to why this should work.

It is mind-blowing to **explain a prediction as a game** played by the feature values.


### Disadvantages

The Shapley value requires **a lot of computing time**.
In 99.9% of real-world problems, only the approximate solution is feasible.
An exact computation of the Shapley value is computationally expensive because there are 2^k^ possible coalitions of the feature values and the "absence" of a feature has to be simulated by drawing random instances, which increases the variance for the estimate of the Shapley values estimation.
The exponential number of the coalitions is dealt with by sampling coalitions and limiting the number of iterations M.
Decreasing M reduces computation time, but increases the variance of the Shapley value.
There is no good rule of thumb for the number of iterations M.
M should be large enough to accurately estimate the Shapley values, but small enough to complete the computation in a reasonable time.
It should be possible to choose M based on Chernoff bounds, but I have not seen any paper on doing this for Shapley values for machine learning predictions.

The Shapley value **can be misinterpreted**.
The Shapley value of a feature value is not the difference of the predicted value after removing the feature from the model training.
The interpretation of the Shapley value is:
Given the current set of feature values, the contribution of a feature value to the difference between the actual prediction and the mean prediction is the estimated Shapley value.

The Shapley value is the wrong explanation method if you seek sparse explanations (explanations that contain few features).
Explanations created with the Shapley value method **always use all the features**.
Humans prefer selective explanations, such as those produced by LIME.
LIME might be the better choice for explanations lay-persons have to deal with.
Another solution is [SHAP](https://github.com/slundberg/shap) introduced by Lundberg and Lee (2016)[^lundberg2017], which is based on the Shapley value, but can also provide explanations with few features.

The Shapley value returns a simple value per feature, but **no prediction model** like LIME.
This means it cannot be used to make statements about changes in prediction for changes in the input, such as:
"If I were to earn €300 more a year, my credit score would increase by 5 points."

Another disadvantage is that **you need access to the data** if you want to calculate the Shapley value for a new data instance.
It is not sufficient to access the prediction function because you need the data to replace parts of the instance of interest with values from randomly drawn instances of the data.
This can only be avoided if you can create data instances that look like real data instances but are not actual instances from the training data.

Like many other permutation-based interpretation methods, the Shapley value method suffers from **inclusion of unrealistic data instances** when features are correlated.
To simulate that a feature value is missing from a coalition, we marginalize the feature.
This is achieved by sampling values from the feature's marginal distribution.
This is fine as long as the features are independent.
When features are dependent, then we might sample feature values that do not make sense for this instance.
But we would use those to compute the feature's Shapley value.
One solution might be to permute correlated features together and get one mutual Shapley value for them.
Another adaptation is conditional sampling: Features are sampled conditional on the features that are already in the team.
While conditional sampling fixes the issue of unrealistic data points, a new issue is introduced:
The resulting values are no longer the Shapley values to our game, since they violate the symmetry axiom, as found out by Sundararajan et al. (2019)[^cond1] and further discussed by Janzing et al. (2020)[^cond2].


### Advantages

Since SHAP computes Shapley values, all the advantages of Shapley values apply:
SHAP has a **solid theoretical foundation** in game theory.
The prediction is **fairly distributed** among the feature values.
We get **contrastive explanations** that compare the prediction with the average prediction.

SHAP **connects LIME and Shapley values**.
This is very useful to better understand both methods.
It also helps to unify the field of interpretable machine learning.

SHAP has a **fast implementation for tree-based models**.
I believe this was key to the popularity of SHAP, because the biggest barrier for adoption of Shapley values is the slow computation.

The fast computation makes it possible to compute the many Shapley values needed for the **global model interpretations**.
The global interpretation methods include feature importance, feature dependence, interactions, clustering and summary plots.
With SHAP, global interpretations are consistent with the local explanations, since the Shapley values are the "atomic unit" of the global interpretations.
If you use LIME for local explanations and partial dependence plots plus permutation feature importance for global explanations, you lack a common foundation.

### Disadvantages

**KernelSHAP is slow**.
This makes KernelSHAP impractical to use when you want to compute Shapley values for many instances.
Also all global SHAP methods such as SHAP feature importance require computing Shapley values for a lot of instances.

**KernelSHAP ignores feature dependence**.
Most other permutation based interpretation methods have this problem.
By replacing feature values with values from random instances, it is usually easier to randomly sample from the marginal distribution.
However, if features are dependent, e.g. correlated, this leads to putting too much weight on unlikely data points.
TreeSHAP solves this problem by explicitly modeling the conditional expected prediction.

**TreeSHAP can produce unintuitive feature attributions**.
While TreeSHAP solves the problem of extrapolating to unlikely data points, it does so by changing the value function and therefore slightly changes the game.
TreeSHAP changes the value function by relying on the conditional expected prediction.
With the change in the value function, features that have no influence on the prediction can get a TreeSHAP value different from zero.

The disadvantages of Shapley values also apply to SHAP:
Shapley values **can be misinterpreted** and access to data is needed to compute them for new data (except for TreeSHAP).

It is **possible to create intentionally misleading interpretations** with SHAP, which can hide biases [^fool].
If you are the data scientist creating the explanations, this is not an actual problem (it would even be an advantage if you are the evil data scientist who wants to create misleading explanations).
For the receivers of a SHAP explanation, it is a disadvantage: they cannot be sure about the truthfulness of the explanation.




