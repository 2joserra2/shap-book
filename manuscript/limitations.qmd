## Limitations of Shapley Values

By initial training, I'm a statistician (Bachelors and Masters).
As such, I am obliged to be anti-hype and very critical of methods.
And this is the chapter where I will go full statistician on Shapley values.

Shapley values, while popular, are not without criticism.
We have touched upon many limitations, like the [correlation problem](#correlation) the difficulty of [interpreting interactions](#interactions).
But let's go a bit deeper.

## Computation time can be a lot 

Especially Kernel and Sampling estimator can be very slow.
And depending on your use case it might be overkill to compute Shapley values.
Maybe you just want the global effect of a feature.
In this case it would be much faster and cheaper to compute [partial dependence plots (PDPs).
](https://christophm.github.io/interpretable-ml-book/pdp.html).

However, model-specific versions such as Tree Estimator are quite fast, so if you can use that it's fine.

## Interactions can be confusing

The critique that interactions can be confusing came from @kumar2020problems, they phrased it more that the additivity axiom can be non-intuitive.
They analyzed the function $f(x) = \prod_{j=1}^p x_j$, which was the "model", but it was actually a pre-defined function so that the Shapley values can be studied.
The function is purely multiplicative.
All features were independent and had an expectation of zero.
Given this information, what would you expect for the Shapley values importance for each feature?
I would expect all the Shapley importances to be the same.

But they also get the same importance when two features have different scales, say $X_1$ goes from -1 to +1, and $X_2$ goes from -1000 to +1000.
Clearly, $X_2$ has more influence on the prediction than $X_1$, just because of the large scale.
But the Shapley importance for both features are the same!
I say it make sense, since knowing $X_1$ is as important as $X_2$.
Even though $X_2$ has a much wider range, without knowing $X_1$ the prediction can go into any direction because of the multiplicative nature of the problem.
But it shows that the interactions (which multiplicative relationships are) are not always intuitive to interpret.
In the [interaction chapter](#interaction) we also saw how interactions are split between features and often show a mixture of local and global effects.
Another quirk to keep in mind.

## No consensus what an attribution should look like

Another problem, which goes a bit beyond shap:
We don't have a coherent idea of what an attribution method should actually look like. 
It's unclear how interactions should be attributed or what importance really means.

In interpretable machine learning, it's often the other way around:
We have some method that is somewhat mathematically coherent which produces some kind of "explanation" for a model, and just try to interpret what it means.
Ideally we could articulate and define what property of data and model we are interested in and 

With Shapley values you could argue that the axioms perfectly define the Shapley values.
After all, the Shapley values are the only solution that fulfills the axioms.
Now the question is: Are these axioms useful for interpretation? Did we define the right game?

As chapters about [linear models](#linear) and [additive models](#additive) have demonstrated, at least for these more constrained models, the outcome is in line with our expectations and what similar metrics (like the coefficients or the PDP) would say.

### Shapley values aren't human-friendly explanations

Human-friendly explanations are short, contrastive, and focus on "abnormal" causes [@miller2019explanation].
And Shapley values are not short -- they always attribute to all feature values.
They are also not contrastive.
Well, they are contrastive to some background data, but that's difficult to grasp, and due to their nature of averaging over many coalitions the contrastiveness is washed out anyways.
So the most inner building block of Shapley values, the marginal contribution (efffect of adding a feature value to a coalition) is contrastive, at least if you just sample one background data point.
Do Shapley values at least focus on the "abnormal"?
Well, the method doesn't really have a sense of abnormal, because that would also mean to have a sense of how likely a feature combination is.

So, Shapley values don't necessarily provide human-friendly explanations.
@kumar2020problems showed that Shapley values don't necessarily match with what humans would expect.
That's why I wouldn't serve them to end users like fries, it's more like a complex type of food where you have to explain the order of which forks to use first, how to use the napkin the right way and so on.

### Shapley values don't enable actions (by the user)

This limitation closely relates to lack of contrastiveness.
Let's say you use Shapley values to explain a model that predicts corn yield based on multiple inputs like weather and fertilizer use.
Then you produce an explanation for a field that says that the prediction was quite low and the use of fertilizer was slightly positive.
The question: How can you improve the yield of the field?
Increase fertilizer use?
Shapley values don't give an answer here, because they only say for a data point how the current value affected the prediction, compared to the background data.
But it doesn't tell you how to change it.

For that you should rather use [counterfactual explanations](christophm.github.io/interpretable-ml-book/counterfactual.html).
Also you should make sure that the model itself reflects actionable advice, in the sense that you data sample is representative and you modeled causal relationships.


## The Shapley value can be misinterpreted

The Shapley value of a feature value is not the difference of the predicted value after removing the feature from the model training.
The interpretation of the Shapley value is:
Given the current set of feature values, the contribution of a feature value to the difference between the actual prediction and the mean prediction is the estimated Shapley value.

The Shapley value is the wrong explanation method if you seek sparse explanations (explanations that contain few features).
Explanations created with the Shapley value method **always use all the features**.

## The Shapley value method is not a surrogate model

The Shapley value returns a simple value per feature, but **no prediction model** like LIME.
This means it cannot be used to make statements about changes in prediction for changes in the input, such as:
"If I were to earn â‚¬300 more a year, my credit score would increase by 5 points."


## Only works if you have data

Another limitation is that **you need access to the data** if you want to calculate the Shapley value for a new data instance.
It is not sufficient to access the prediction function because you need the data to replace parts of the instance of interest with values from randomly drawn instances of the data.
This can only be avoided if you can create data instances that look like real data instances but are not actual instances from the training data.


## Fooling Shap

It is **possible to create intentionally misleading interpretations** with SHAP, which can hide biases.[@slack2020fooling]
If you are the data scientist creating the explanations, this is not an actual problem (it would even be an advantage if you are the evil data scientist who wants to create misleading explanations).
For the receivers of a SHAP explanation, it is a disadvantage: they cannot be sure about the truthfulness of the explanation.


### Unrealistic data when features are correlated

See [correlation chapter](#correlation)


