# Limitations of SHAP {#limitations}

::: {.callout-tip appearance="simple"}

By the end of this chapter, you will be able to:

- Recognize the limitations of SHAP values.
- Resist the allure of hype.

:::

As a statistician holding both Bachelor's and Master's degrees, I feel obligated to critically examine methods and resist hype. 
In this chapter, we will scrutinize SHAP.
While SHAP is popular, it is not without its shortcomings. 
We have already discussed issues such as the [correlation problem](#correlation) and the complexity of [interpreting interactions](#interaction). 
Let's dive further into these limitations.

## Computation time can be excessive

Kernel and Sampling estimators can be particularly slow. 
Depending on your use case, computing SHAP values might be unnecessary. 
If you only need to determine the global effect of a feature, it would be more efficient and cost-effective to compute [partial dependence plots (PDPs)](https://christophm.github.io/interpretable-ml-book/pdp.html). 
However, model-specific versions like the Tree Estimator are relatively fast, making them a viable option if feasible.

## Interactions can be perplexing

The argument that interactions can be confusing comes from @kumar2020problems, who claimed that the additivity axiom can be counterintuitive. 
They studied the function $f(x) = \prod_{j=1}^p x_j$, which served as the "model" but was actually a predefined function for studying SHAP values. 
This function is purely multiplicative. 
All features were independent and had an expected value of zero. 
Given this information, what would you anticipate for the SHAP importance of each feature? 
I would expect all SHAP importances to be equal. 
However the two features have different scales, such as $X_1$ ranging from -1 to +1 and $X_2$ ranging from -1000 to +1000. 
While $X_2$ appears to have a greater influence on the prediction due to its larger scale, the SHAP importance for both features is the same. 
This is because knowing $X_1$ is as important as knowing $X_2$. 
$X_2$ has a much larger range, but it doesn't impact the SHAP value because $X_1$ can set the prediction to zero or flip the sign. 
This shows that interpreting interactions is not always intuitive. 
In the [Interaction Chapter](#interaction), we also observed how interactions are divided between features and often exhibit a combination of local and global effects. 
Keep this peculiarity in mind.

## No consensus on what an attribution should look like

Another issue, which extends beyond SHAP, is the lack of a cohesive understanding of what an attribution method should entail. 
There is uncertainty about how interactions should be attributed or what "importance" truly represents.

In interpretable machine learning, the process is often reversed: 
We use a somewhat mathematically coherent method that generates an "explanation" for a model and then attempt to decipher its meaning. 
Ideally, we would specify what we want in terms of an explanation and then select or design the method based on our needs. 
However, this is rarely the case in the real world.

With SHAP values, one could argue that the axioms perfectly define them. 
After all, SHAP values are the only solution that satisfies the axioms. 
The question now is: Are these axioms useful for interpretation? Did we define the right game?

As demonstrated in the [Linear Chapter](#linear) and the [Additive Chapter](#additive), at least for these more restricted models, the results align with our expectations and what similar metrics (such as coefficients) would indicate.

Bear in mind that SHAP values are but one example of an attribution method.
And even attribution method is just a subset of methods in interpretable machine learning.

## SHAP values don't always provide human-friendly explanations.

Human-friendly explanations should be concise, contrastive, and focus on "abnormal" causes [@miller2019explanation].
However, SHAP values are not concise because they attribute significance to all feature values.
They are not inherently contrastive either.
While they provide contrast to some background data, this can be difficult to comprehend due to their averaging nature over numerous coalitions, which tends to dilute the contrastiveness.
The fundamental building block of SHAP values, the marginal contribution (effect of adding a feature value to a coalition), is contrastive when considering a single background data point.
As for focusing on the "abnormal," SHAP values don't inherently possess this quality, which would require a sense of how likely a particular feature combination is.

SHAP values don't always provide human-friendly explanations.
@kumar2020problems demonstrated that SHAP values might not align with human expectations.
Hence, it's not advisable to present them to end-users as straightforward information; they are complex concepts requiring explanations for proper usage and understanding.

## SHAP values don't enable user actions

This limitation is closely related to the lack of contrastiveness.
For instance, consider using SHAP values to explain a model that predicts corn yield based on multiple inputs, such as weather and fertilizer use.
An explanation for a field with a low prediction might state that fertilizer use had a slightly positive impact.
However, it doesn't answer the question of how to improve the field's yield.
Should fertilizer use be increased?
SHAP values don't provide a solution, as they only explain how the current value affected the prediction compared to the background data, without suggesting how to modify it.
However, they do indicate which features need to be preserved to avoid regression to the mean.

For actionable recommendations, consider using [counterfactual explanations](christophm.github.io/interpretable-ml-book/counterfactual.html).
Additionally, ensure the model itself offers actionable advice by using representative data samples and modeling causal relationships.

## SHAP values can be misinterpreted

The SHAP value of a feature is not the difference in predicted value after removing the feature from model training.
Instead, the SHAP value interpretation is as follows: given the current set of feature values, the contribution of a feature value to the difference between the actual prediction and the mean prediction is the estimated SHAP value.
SHAP is not the ideal explanation method for sparse explanations (explanations with few features), as it always uses all the features.

## SHAP values aren't a surrogate model

Unlike LIME, the SHAP value provides a single value per feature but no prediction model.
Thus, it cannot be used to make statements about changes in predictions for changes in input, such as, "If I were to earn â‚¬300 more a year, my credit score would increase by 5 points."

## Data access is necessary

Another limitation is that data access is required to calculate the SHAP value for a new data instance.
Access to the prediction function alone is insufficient, as you need the data to replace parts of the instance of interest with values from randomly drawn instances.
This can only be avoided if you can create data instances that resemble real data instances but are not actual instances from the training data.

## You can fool SHAP

It's possible to create intentionally misleading interpretations with SHAP, which can conceal biases [@slack2020fooling], at least if you use the marginal sampling version of SHAP.
If you are the data scientist creating the explanations, this is not an issue (it could even be advantageous if you are an unscrupulous data scientist who wants to create misleading explanations).
However, for the recipients of a SHAP explanation, it is a disadvantage as they cannot be certain of the explanation's truthfulness.

## Unrealistic data when features are correlated

See [Correlation Chapter](#correlation).
