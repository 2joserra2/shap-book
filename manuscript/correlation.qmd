# The Correlation Problem {#correlation}

::: {.callout-tip appearance="simple"}

By the end of this chapter, you will be able to:

- Describe the challenge of correlated features for SHAP.
- Select solutions to reduce the correlation problem.

:::

SHAP values encounter a subtle yet relevant issue when dealing with correlated features. Simulating the absence of features by replacing them with sampled values from the background data can generate unrealistic data points. These implausible data points are then used to produce explanations, resulting in several problems:

- Including unlikely or unrealistic data points in the explanation may not be desirable.
- Physically impossible data points (e.g., a 2-meter tall person weighing 10 kg) may present a conceptual issue.
- The model may not have been trained on data in these areas and might yield unexpected results.

In this chapter, we will first dive into the problem in more detail using a small simulation, followed by a discussion of possible solutions.

## Correlated features cause extrapolation

To illustrate this issue, I simulated two highly correlated features, $X_1$ and $X_2$. Assuming a machine learning model is later trained on this data, we won't actually train a model but will instead explore how sampling background data can be problematic.

```{python}
import numpy as np

np.random.seed(42)

p = 0.9
mean = [0, 0] # mean vector
cov = [[1, p], [p, 1]] # covariance matrix
n = 100 # number of samples

x1, x2 = np.random.multivariate_normal(mean, cov, n).T
```

Next, let's create a data point for which we will simulate the sampling from the background data.

```{python}
point = (-1.7, -1.7)
```

I'm interested in the SHAP value for feature $X_1$. I will demonstrate how SHAP values sample from the marginal distribution and compare that to what sampling from the conditional distribution would look like.

```{python}
#| label: fig-sampling
#| fig-cap: "Marginal and conditional sampling"
import matplotlib.pyplot as plt
# set number of samples
m = 15

# create marginal and conditional distribution
x2_cond = np.random.normal(
  loc=p*point[0], scale=np.sqrt(1-p**2), size=m
)
x2_marg = np.random.choice(x2, size=m)

# create scatter plot with fixed x1 and variable x2
plt.subplot(121)
plt.scatter(x1, x2, color='black', alpha=0.1)
plt.scatter(np.repeat(point[0], m), x2_marg, color='blue')
plt.scatter(point[0], point[1], color='red')
plt.ylabel('x2')
plt.xlabel('x1')
plt.subplot(122)
plt.scatter(x1, x2, color='black', alpha=0.1)
plt.scatter(np.repeat(point[0], m), x2_cond, color='green')
plt.scatter(point[0], point[1], color='red')
plt.xlabel('x1')
plt.show()
```

The plot on the left shows sampling from the marginal distribution: we disregard the correlation between $X_1$ and $X_2$ and sample $X_2$ independently from $X_1$.
Marginal sampling in this correlated case creates new data points outside of the distribution.
This is what occurs with SHAP values as we have used them throughout the book.
On the right, we see conditional sampling.
Conditional sampling means that we respect the distribution of $X_2$, given that we already know the value for $X_1$ and sample from $P(X_2 | X_1)$ instead of $P(X_2)$.
Conditional sampling preserves the distribution, whereas marginal sampling may distort it when features are correlated.

## A philosophical problem

Let's dive deeper into the correlation problem before discussing solutions.
The problem of correlation and extrapolation is not just a technical challenge but also a philosophical one: do we want the interpretation to reflect the modeled relations or the data?
Consider this: what does it mean when features are correlated and what are the consequences for interpretation?
From an information theory perspective, correlation implies that two features share information.
If information is shared, we cannot simply sample one feature without considering the correlated feature.

For instance, suppose we have two strongly correlated features: rainfall yesterday and cloudiness yesterday.
If it rained yesterday, it must have been cloudy - these features are correlated.
How can we then isolate the effect of rain while ignoring the cloudiness?
Even if we disentangle the two, it would make for a weird interpretation.

## Solution: Reduce correlation in the model

Although it may appear overly simplistic, avoiding the correlation problem can be resolved by eliminating correlated features.
This solution requires flexibility in training the model with a different set of features.
If you have the liberty to alter the features, we can leverage a range of techniques to reduce the correlation:

- Use feature selection methods, especially those that eliminate correlated features.
- Eliminate features with minimal variance.
- Implement dimensionality reduction techniques. However, this may lead to a loss of interpretability; for example, principal component analysis generates features that are difficult to interpret.
- Apply feature engineering to decrease correlation. For instance, if you have the features "apartment rent" and "number of rooms", they will be correlated. You can decorrelate them by converting the rent into "rent per square meter". 
- Combine features. Perhaps having the amount of rain in the morning and afternoon as separate features is unnecessary. Would daily rainfall be sufficient? Test it out.

Reducing correlated features and the overall number of features can significantly enhance model fitting.
You can assess how each of these steps impacts predictive performance, aiding in your decision-making for possible trade-offs.

## Solution: Combined explanation of correlated features with Partition explainer

Rather than using the `Independent` explainer, opt for the `Partition` explainer.
The Partition explainer generates a feature hierarchy presented as a tree.
While any metric could be used to produce the hierarchy, it should be a correlation-based metric that hierarchically clusters the features, like Pearson correlation.

Within this hierarchy, SHAP values are calculated recursively.
This procedure is its own game, known as Owen's game, and we get Owen values back.
Owen values are closely linked to Shapley values, as we will see in the following example.
Let's say for the apartment example we have the following binary tree of correlations:

```{mermaid}

graph TB
    A[X]
    S[size, cat, park]
    F[floor]
    C[park]
    P[size,cat]
    Q[cat]
    R[size]
    A --> S
    A --> F
    S --> C
    S --> P
    P --> Q
    P --> R

```

- Cat and size are most strongly correlated.
- Both are slightly correlated with the park feature.
- The floor feature is not correlated with any of the features.

To compute Owen values, also known as hierarchical Shapley values, we start at the top with two feature groups: {size,cat,park} and {floor}.
We treat both groups as players and compute their SHAP values $\phi_{\text{size,cat,park}}$ and $\phi_{floor}$.
In the next recursion, we split up the $\phi_{\text{size,cat,park}}$ among the groups {park} and {size, cat}, again as if we were computing SHAP, but with groups as players.
We then move down to {cat} and {size}.
The Partition explainer doesn't fully eliminate the correlation problem.
As we move deeper down the correlation tree, we start to also break up strongly correlated features to recursively compute Owen's value.
However, it's reduced, since we only try to explain the group's SHAP value and permute fewer features.

Owen's game alters the complexity from $\mathcal{O}(2^p)$ to $\mathcal{O}(p^2)$ (when the clustering tree is balanced), where $p$ is the number of input features for the model.

## Solution: Conditional sampling

As suggested in @fig-sampling, conditional sampling can also be utilised.

Here's a brief example:

- We have four features: $X_1$ through $X_4$.
- Calculate the SHAP value for $X_1$.
- During the sampling process, we add $X_1$ to the coalition $\{X_2\}$.
- Compute the predictions for the coalitions $\{X_2\}$ and $\{X_1, X_2\}$.
- The missing features are $\{X_1, X_3, X_4\}$ and $\{X_3, X_4\}$, and we sample these from the background data.
- Sampling can be carried out in two ways:
  - From $P(X_1, X_3, X_4)$ and $P(X_3, X_4)$ (marginal sampling), or
  - From $P(X_1, X_3, X_4 | X_2)$ and $P(X_3, X_4| X_1, X_2)$ (conditional sampling)

Conditional sampling helps avoid extrapolation problems.
@aas2021explaining suggested incorporating this method into KernelSHAP.
However, implementing conditional sampling is not straightforward, given that supervised machine learning mainly learns $P(Y|X_1, \ldots, X_p)$, and we now need to learn complex distributions for numerous variables.
Generally, we make simplifying assumptions for the conditional distributions to facilitate sampling, as suggested by @aas2021explaining:

- Use multivariate Gaussians.
- Utilize Gaussian copulas.
- Apply kernel estimators (if the dimensions are not too many).

The paper suggests more techniques, but these are the most significant.

::: {.callout-warning}

Switching the sampling method from marginal to conditional changes the value function to a conditional value function:

$$v_{f,x^{(i)}}(S) = \int f(x^{(i)}_S \cup X_C)d\mathbb{P}_{X_{C}|X_S=x^{(i)}_S} - \mathbb{E}(f(X))$$

This shift changes the game.
For example, the resulting SHAP values might seem to not adhere to the Dummy axiom, as unused features with, for example, $\beta_j=0$ in a linear model can suddenly have non-zero SHAP values.
However, conditional SHAP does not actually violate the Dummy axiom, as they still qualify as (conditional) SHAP values based on the new payout using conditional distributions.
Without correlation, marginal and conditional SHAP are identical, since then $P(X_1) = P(X_1|X_2)$.

:::

If you want to employ conditional sampling in `shap`, use the Linear explainer with the `feature_pertubation='correlation_dependent'` option.
The Tree explainer provides a similar option called `feature_pertubation= 'tree_path_dependent'`, which uses the splits in the underlying tree-based model to ensure SHAP is not extrapolating.
However, I advise against using the tree-path option, as it does not accurately model the conditional distribution and does not approximate the conditional SHAP values well [@aas2021explaining].

Whether a conditional SHAP interpretation is useful depends on your interpretation objectives.
If you want to audit the model, marginal sampling may be more suitable.
If your goal is to better understand the data, conditional sampling might be the better choice.
This trade-off is often described as being true to the model (marginal sampling) or true to the data (conditional sampling) [@chen2020true].
@sundararajan2020many further discusses this concept.
