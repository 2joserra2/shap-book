# The Correlation Problem {#correlation}

Shapley values have a subtle but significant issue when dealing with correlated features:
simulating feature absence by replacing them with sampled values from the background data can produce unrealistic data points.
These unrealistic data points are then used to generate explanations, which is problematic for several reasons:

- Involving unrealistic or unlikely data points in the explanation may not be desirable.
- There could be a conceptual issue with physically impossible data points (e.g., a 2-meter tall person weighing 10 kg).
- The model may not have been trained on data in these areas and might produce unexpected results.

In this chapter, we will first delve into the problem in more detail using a small simulation, followed by discussing possible solutions.

## Correlated features cause extrapolation

To illustrate this issue, I simulated two strongly correlated features, $X_1$ and $X_2$.
Suppose a machine learning model is later trained on this data.
We will not train a model, but rather examine how sampling background data can be problematic.

```{python}
import numpy as np

p = 0.9
mean = [0, 0] # mean vector
cov = [[1, p], [p, 1]] # covariance matrix
n = 100 # number of samples

x1, x2 = np.random.multivariate_normal(mean, cov, n).T
```

Next, let's create a data point for which we will simulate drawing from the background data.

```{python}
point = (-1.7, -1.7)
```

I'm interested in the Shapley value for feature $X_1$.
I will demonstrate both what Shapley values do and how background data sampling should appear when respecting the conditional distribution of the data.
```{python}
#| label: fig-sampling
#| fig-cap: "Marginal and conditional sampling"
import matplotlib.pyplot as plt
# set number of samples for conditional distribution
m = 15

# create marginal and conditional distribution
x2_cond = np.random.normal(
  loc=p*point[0], scale=np.sqrt(1-p**2), size=m
)
x2_marg = np.random.choice(x2, size=m)

# create scatter plot with fixed x1 and variable x2
plt.subplot(121)
plt.scatter(x1, x2, color='black', alpha=0.1)
plt.scatter(np.repeat(point[0], m), x2_cond, color='green')
plt.scatter(point[0], point[1], color='red')
plt.subplot(122)
plt.scatter(x1, x2, color='black', alpha=0.1)
plt.scatter(np.repeat(point[0], m), x2_marg, color='blue')
plt.scatter(point[0], point[1], color='red')
plt.xlabel('x1')
plt.ylabel('x2')

plt.show()
```
The plot above illustrates that when we ignore the correlation between $X_1$ and $X_2$ and sample $X_2$ independently from $X_1$, we create new data points outside of the distribution.
This is what occurs with Shapley values.
On the left, we can see what sampling from $P(X_2 | X_1)$ instead of $P(X_2)$ would look like.
This conditional sampling will also be part of one of the solutions.

## A philosophical problem
Before delving into solution details, let's explore the issue further.
The problem of correlation and extrapolation is not just a technical challenge but also a philosophical one.
We must decide what we truly want when interpreting a model.

Consider this: What does it **mean** when features are correlated, and what are the consequences for interpretation?
From an information theoretic perspective, correlation means that two features share information.
If they share information, it makes sense that we cannot simply sample one of the features without considering the correlated feature.

For example, let's say we have two strongly correlated features: rainfall yesterday and cloudiness yesterday.
If it rained yesterday, we know it must have been cloudy - this is correlation.
How can we isolate the effect of rain while ignoring the cloudiness?

## What about images and text?

SHAP for image and text features works differently: no background data is involved.
However, simulating absent feature values still generates new images and texts that might deviate from the distribution.
In case of images, the removed areas are blurred or inpainted.
For text, it's replaced by a replacement token like "..." or just a whitespace " ".
It's difficult to gauge the severity of the problem, but inpainting can be seen as a form of conditional sampling, and blurring is even more unique as it doesn't use all the feature's information.

The question becomes: What is a meaningful "reference data point"?
For images, the default is a blurred or inpainted image.
For text, it's an empty string.

## Solution: Reduce correlation in the model

While it may seem overly simplistic, avoiding the correlation problem can be achieved by eliminating correlated features.
This solution requires flexibility in training the model with a different set of features.
If you have the option to change the features, we can leverage a wider range of techniques to reduce the number of features used:

- Employ feature selection methods, particularly those that eliminate correlated features.
- Remove features with minimal variance.
- Utilize dimensionality reduction techniques.
- Apply feature engineering to decrease correlation: For instance, if you have a person's height and weight, use height and BMI instead.
- Combine features: Perhaps you have the amount of rain in the morning and afternoon as separate features. Maybe daily rainfall is sufficient? Test it out.

Reducing correlated features and the overall number of features can significantly benefit model fitting.
For each of these steps, you can assess how much they reduce or potentially enhance model performance, aiding in your decision-making for possible trade-offs.

## Solution: Combined explanation of correlated features

Instead of using the `Independent` explainer, opt for the `Partition` explainer.
By default, the `Partition` explainer groups features based on correlation, but you can also define your own feature groupings.

The Partition explainer generates a feature hierarchy based on a provided distance matrix or custom groupings.
Within this hierarchy, Shapley values are calculated recursively.
This alters the complexity from $\mathcal{O}(2^p)$ to $\mathcal{O}(p^2)$ (when the clustering tree is balanced), where $p$ is the number of input features for the model.

This specific type of game is called the Owen game.
The benefit is that for groups of correlated features, we obtain a single Shapley value for interpretation.

## Solution: Conditional sampling

As hinted in @fig-sampling, it's also possible to use conditional sampling.

A brief example:

- We have four features $X_1$ to $X_4$.
- Compute the Shapley value for $X_1$.
- During sampling, one of the coalitions we add $X_1$ to is $\{X_2\}$.
- Compute the predictions for the teams $\{X_2\}$ and $\{X_1, X_2\}$.
- The absent features are $\{X_1, X_3, X_4\}$ and $\{X_3, X_4\}$, which are sampled from the background data.
- Sampling can be either from:
  - $P(X_1, X_3, X_4)$ and $P(X_3, X_4)$ (marginal sampling), or
  - $P(X_1, X_3, X_4 | X_2)$ and $P(X_3, X_4| X_1, X_2)$ (conditional sampling)

Conditional sampling avoids the extrapolation problem.
@aas2021explaining suggested incorporating conditional sampling in KernelSHAP.
However, conditional sampling is easier said than done, considering that supervised machine learning focuses on learning $P(Y|X_1, \ldots, X_p)$, and we now need to learn more complex distributions for multiple variables.
Typically, simplifying assumptions for the conditional distributions are used to make sampling easier, as done by @aas2021explaining:

- Use multivariate Gaussians
- Employ Gaussian copulas
- Apply kernel estimators (if there are not too many dimensions)

The paper proposed more techniques, but these are the key ones

::: {.callout-warning}
Switching the sampling from marginal to conditional alters the game payout.
For instance, the resulting Shapley values may appear to no longer adhere to the Dummy axiom, as unused features like $\beta_j=0$ in a linear model can suddenly have non-zero Shapley values.
However, this doesn't actually violate the Dummy axiom, since based on the new payout using conditional distributions, they still qualify as Shapley values.
Just be mindful of this shift in interpretation.

:::

As mentioned above, the Shapley game changes and features not utilized by the model can have non-zero Shapley values.

Whether this is desirable depends on your interpretation goals.
If your objective is to audit the model, marginal sampling might be more appropriate.
If you want to better understand the data, conditional sampling could be the better option.
This trade-off is also characterized as being true to the model (marginal sampling) or true to the data (conditional sampling) [@chen2020true].
This concept is further discussed by @sundararajan2020many.

::: {.callout-note}

When features are uncorrelated, marginal and conditional distributions are identical.

:::
