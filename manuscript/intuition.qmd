# Intuition Behind Shapley Values

Shapley values have their origins in game theory.
However, before diving into the theory behind them and their estimations, let's first discuss the intuition of how Shapley values operate based on game theory.

## Origins in game theory

What does a prediction have to do with games? 
The connection between these may not be immediately apparent.
It certainly wasn't for me.
When we delve into the origins of Shapley values, they were initially devised to address the issue of fair attribution in cooperative games.
Imagine a group of players who collaborate, and once the game concludes, they receive payouts based on their performance.
However, the payout is designated for the entire team, requiring them to divide it amongst themselves.
Players may have contributed differently to the game.
For instance, Larry, who barely contributed, shouldn't necessarily receive a share.
On the other hand, Darlene, the brilliant strategist, significantly enhanced the team's performance, surpassing their expectations.
She deserves a larger portion.
But how much larger?
To solve this attribution problem, we must first establish a set of axioms defining what fair attribution entails.
We'll discuss that in more detail later.

For now, let's focus on connecting these concepts to predictions.

## The prediction attribution game

A prediction can be explained by assuming that each feature value of the instance is a "player" in a game where the prediction is the payout.
Shapley values, a method from coalitional game theory, tell us how to fairly distribute the "payout" among the features.

Let's consider the following scenario:

You have trained a machine learning model to predict apartment prices.
For a specific apartment, it predicts €300,000 and you need to explain this prediction.
The apartment has an area of 50 m^2^ (538 square feet), is located on the 2nd floor, has a park nearby, and cats are banned:

![The predicted price for a 50 $m^2$ 2nd floor apartment with a nearby park and cat ban is €300,000. Our goal is to explain how each of these feature values contributed to the prediction.](images/shapley-instance.jpg)
The average prediction for all apartments in the test data is €310,000.
How much has each feature value contributed to the prediction compared to the average prediction?

To translate the game scenario to machine learning predictions:

- The "game" refers to the prediction task for a single instance of the dataset.
- The "payout" represents the actual prediction for this instance minus the average prediction for all instances.
- The "players" are the feature values of the instance that collaborate to receive the gain (i.e., predict a certain value).

In the apartment example, the feature values `park-nearby`, `cat-banned`, `area-50`, and `floor-2nd` worked together to achieve the prediction of €300,000.
In the data, the row would look like this:

<!-- TODO: Check markdown tables again if correct values -->

| Park| Cat | Area | Floor | Predicted Price   |
|-------------|------------|------|-------|---------|
| Nearby | Banned | 50   | 2nd   | €300,000|

Our goal is to explain the difference between the actual prediction (€300,000) and the average prediction (€310,000), which amounts to a difference of -€10,000.

The answer could be as follows:
`park-nearby` contributed €30,000; `area-50` contributed €10,000; `floor-2nd` contributed €0; and `cat-banned` contributed -€50,000.
The contributions add up to -€10,000, which is the final prediction minus the average predicted apartment price.

## How to calculate the Shapley value for one feature


The Shapley value represents the average marginal contribution of a feature value across all possible coalitions.
A coalition is, in this case, any subset of feature values, including the empty set and the set containing all feature values of the instance.
In the following figure, we evaluate the contribution of the `cat-banned` feature value when added to a coalition of `park-nearby` and `area-50`.
We simulate a coalition consisting of only `park-nearby`, `cat-banned`, and `area-50` by randomly drawing another apartment from the data and using its value for the floor feature.
The other apartment might look like this:

| Park | Cat | Area | Floor | Predicted Price   |
|-------------|------------|------|-------|---------|
| Nearby | Allowed    | 100   | 1st  | €504,000|

Then we replace the value `floor-2nd` of the original apartment by the randomly drawn `floor-1st`.
We then predict the price of the apartment with this combination (€310,000).


| Park        | Cat        | Area | Floor | Predicted Price   |
|-------------|------------|------|-------|---------|
| Nearby      | Banned     | 50   | 1st   | €310,000|

Next, we remove `cat-banned` from the coalition by replacing it with a random value of the cat allowed/banned feature from the randomly drawn apartment.

| Park        | Cat        | Area | Floor | Predicted Price   |
|-------------|------------|------|-------|---------|
| Nearby      | Allowed    | 50   | 1st   | €320,000|

In this example, it is `cat-allowed`, but it could have been `cat-banned` again.
We predict the apartment price for the coalition of `park-nearby` and `area-50` (€320,000).
The contribution of `cat-banned` is €310,000 - €320,000 = -€10,000.
This estimate depends on the values of the randomly drawn apartment that served as a "donor" for the cat and floor feature values.
We can obtain better estimates by repeating this sampling step and averaging the contributions.
![One sample repetition to estimate the contribution of `cat-banned` to the prediction when added to the coalition of `park-nearby` and `area-50`.](images/shapley-instance-intervention.jpg)

We repeat this computation for all possible coalitions.
The Shapley value is the average of all the marginal contributions to all possible coalitions.
The computation time increases exponentially with the number of features, because there are $2^p$ possible coalitions, where $p$ is the number of features.
One solution to keep the computation time manageable is to compute contributions for only a few samples of the possible coalitions.

The following figure shows all coalitions of feature values that are needed to determine the Shapley value for `cat-banned`.
The first row shows the coalition without any feature values.
The second, third, and fourth rows show different coalitions with increasing coalition size, separated by "|".
In total, the following coalitions are possible:

- No feature values
- `park-nearby`
- `area-50`
- `floor-2nd`
- `park-nearby`+`area-50`
- `park-nearby`+`floor-2nd`
- `area-50`+`floor-2nd`
- `park-nearby`+`area-50`+`floor-2nd`.

For each of these coalitions, we compute the predicted apartment price with and without the feature value `cat-banned` and take the difference to obtain the marginal contribution.
The Shapley value is the (weighted) average of these marginal contributions.
To get a prediction from the machine learning model, we replace the feature values of features not in a coalition with random feature values from the apartment dataset.

![All 8 coalitions needed for computing the exact Shapley value of the `cat-banned` feature value.](images/shapley-coalitions.jpg)

By estimating the Shapley values for all feature values, we obtain the complete distribution of the prediction (minus the average) among the feature values.
That's not the only way to estimate Shapley values, nor is it explained in detail here.
In fact, there are various ways to estimate Shapley values as you can see in the [Estimation Chapter](#estimation).
This is because, especially for specific models like linear regression and tree-based models, there are more efficient ways to estimate the Shapley values.

## What about images and texts?

The intuition above primarily addressed tabular data.
But what if the input is an image or text?
One option is to treat them similarly to tabular input:
Each input dimension represents a feature – pixels for images and tokens for texts.

We will explore this further in the respective chapters on [image](#image) and [text](#text) data.
But here's some intuition:
For images, it's possible to define super-pixels, which are groups of connected pixels.
These super-pixels are treated as a "feature" for Shapley values.
When a particular super-pixel is part of the team, it's visible, and when it's not, it's removed.
There are different options for removing super-pixels, such as blurring or graying them out.
For text, it's a bit simpler; we can remove words or replace them with other tokens when necessary.
