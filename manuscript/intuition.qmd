# Intuition

A prediction can be explained by assuming that each feature value of the instance is a "player" in a game where the prediction is the payout.
Shapley values -- a method from coalitional game theory -- tells us how to fairly distribute the "payout" among the features.

### General Idea

Assume the following scenario:

You have trained a machine learning model to predict apartment prices.
For a certain apartment it predicts €300,000 and you need to explain this prediction.
The apartment has an area of 50 m^2^, is located on the 2nd floor, has a park nearby and cats are banned:

```{r shapley-instance, fig.cap = "The predicted price for a 50 $m^2$ 2nd floor apartment with a nearby park and cat ban is €300,000. Our goal is to explain how each of these feature values contributed to the prediction."}
knitr::include_graphics("images/shapley-instance.png")
```

The average prediction for all apartments is €310,000.
How much has each feature value contributed to the prediction compared to the average prediction?


The answer is simple for linear regression models.
The effect of each feature is the weight of the feature times the feature value.
This only works because of the linearity of the model.
For more complex models, we need a different solution.
For example, [LIME](#lime) suggests local models to estimate effects.
Another solution comes from cooperative game theory:
The Shapley value, coined by Shapley (1953)[^shapley1953], is a method for assigning payouts to players depending on their contribution to the total payout.
Players cooperate in a coalition and receive a certain profit from this cooperation.

Players?
Game?
Payout?
What is the connection to machine learning predictions and interpretability?
The "game" is the prediction task for a single instance of the dataset.
The "gain" is the actual prediction for this instance minus the average prediction for all instances.
The "players" are the feature values of the instance that collaborate to receive the gain (= predict a certain value).
In our apartment example, the feature values `park-nearby`, `cat-banned`, `area-50` and `floor-2nd` worked together to achieve the prediction of €300,000.
Our goal is to explain the difference between the actual prediction (€300,000) and the average prediction (€310,000): a difference of -€10,000.

The answer could be:
The `park-nearby` contributed €30,000; `area-50` contributed €10,000; `floor-2nd` contributed €0; `cat-banned` contributed -€50,000.
The contributions add up to -€10,000, the final prediction minus the average predicted apartment price.

**How do we calculate the Shapley value for one feature?**

The Shapley value is the average marginal contribution of a feature value across all possible coalitions.
All clear now?

In the following figure we evaluate the contribution of the `cat-banned` feature value when it is added to a coalition of `park-nearby` and `area-50`.
We simulate that only `park-nearby`, `cat-banned` and `area-50` are in a coalition by randomly drawing another apartment from the data and using its value for the floor feature.
The value `floor-2nd` was replaced by the randomly drawn `floor-1st`.
Then we predict the price of the apartment with this combination (€310,000).
In a second step, we remove `cat-banned` from the coalition by replacing it with a random value of the cat allowed/banned feature from the randomly drawn apartment.
In the example it was `cat-allowed`, but it could have been `cat-banned` again.
We predict the apartment price for the coalition of `park-nearby` and `area-50` (€320,000).
The contribution of `cat-banned` was €310,000 - €320,000 = -€10,000.
This estimate depends on the values of the randomly drawn apartment that served as a "donor" for the cat and floor feature values.
We will get better estimates if we repeat this sampling step and average the contributions.


```{r shapley-instance-intervened, fig.cap = "One sample repetition to estimate the contribution of `cat-banned` to the prediction when added to the coalition of `park-nearby` and `area-50`."}
knitr::include_graphics("images/shapley-instance-intervention.png")
```

We repeat this computation for all possible coalitions.
The Shapley value is the average of all the marginal contributions to all possible coalitions.
The computation time increases exponentially with the number of features.
One solution to keep the computation time manageable is to compute contributions for only a few samples of the possible coalitions.

The following figure shows all coalitions of feature values that are needed to determine the Shapley value for `cat-banned`.
The first row shows the coalition without any feature values.
The second, third and fourth rows show different coalitions with increasing coalition size, separated by "|".
All in all, the following coalitions are possible:

- `No feature values`
- `park-nearby`
- `area-50`
- `floor-2nd`
- `park-nearby`+`area-50`
- `park-nearby`+`floor-2nd`
- `area-50`+`floor-2nd`
- `park-nearby`+`area-50`+`floor-2nd`.

For each of these coalitions we compute the predicted apartment price with and without the feature value `cat-banned` and take the difference to get the marginal contribution.
The Shapley value is the (weighted) average of marginal contributions.
We replace the feature values of features that are not in a coalition with random feature values from the apartment dataset to get a prediction from the machine learning model.


```{r shapley-coalitions, fig.cap = "All 8 coalitions needed for computing the exact Shapley value of the `cat-banned` feature value."}
knitr::include_graphics("images/shapley-coalitions.png")
```

If we estimate the Shapley values for all feature values, we get the complete distribution of the prediction (minus the average) among the feature values.


The intuition behind how shap works.

TODO: Copy from IML book and rewrite a bit.



## For TAbular


## For Image


## For Text




