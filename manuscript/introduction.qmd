# Introduction {#introduction}

<!-- Motivation for XAI -->
Machine learning models are ubiquitous and powerful tools, but their lack of interpretability poses a significant challenge.
It's often unclear why a certain prediction was made, what the most important features were, and how the features influence the predictions in general.
<!-- is it important? -->
Many people argue that as long as a machine learning model performs well, interpretability is unnecessary.
However, there are many practical reasons why you need interpretability, ranging from debugging to building trust in your model.

## Interpret to debug

Interpretability is valuable for model debugging, as illustrated by a study predicting pneumonia [@caruana2015intelligible].
The authors trained a rule-based model, which learned that if a patient has asthma, they have a lower risk of pneumonia.
I mean, seriously?
I'm no doctor, but that seems off.
Asthma patients typically have a higher risk of lung-related diseases.
It appears the model got it all wrong.
However, it turns out that asthma patients in this dataset were less likely to get pneumonia.
The indirect reason was that these patients received "aggressive care," such as early antibiotic treatment.
Consequently, they were less likely to develop pneumonia.
A typical case of "correlation does not imply causation" as you can see in @fig-asthma.

![Asthma increases the likelihood of pneumonia. However, in the study, asthma also increased the (preemptive) use of antibiotics which generally protects against pneumonia and led to an overall lower pneumonia risk for asthma patients.](images/asthma-dag.jpg){#fig-asthma}

This pathological dependence on the asthma feature was only discovered because the model was interpretable.
Imagine repeating this scenario with a neural network.
No rule would jump out, saying "asthma $\Rightarrow$ lower risk."
Instead, the network would learn this rule but keep it hidden, potentially causing harm if deployed in the real world.

In theory, you could have spotted the problem by examining the data closely and applying domain knowledge.
In practice, however, it's easier to identify such issues if you can interpret what the model has learned.
Black box machine learning models create a distance between the data and the modeler, and interpretability methods help bridge that gap.

## Users may create their own interpretations anyway

This is a story of how the lack of interpretability made the users of a model come up with their own (wrong) explanations.
The story starts with sepsis, a life-threatening medical condition in which the body responds in extreme ways to an infection.
It's one of the most common causes of death in hospitals.
Diagnosing sepsis is notoriously difficult, sepsis is expensive to treat and extremely harmful to the patients, making early detection systems desirable.

Duke University and Duke Health Systems developed Sepsis Watch, an early warning system for sepsis in hospitals.
It's a software system that has a sepsis prediction model at it's core, based on deep neural networks [@elish2020repairing].
The model takes as input patient data and predicts whether this patient is likely to develop sepsis.
When the model detects a potential case of sepsis, it triggers an alert that initiates a novel hospital protocol for diagnosis and treatment.
This protocol involves a rapid response team (RRT) nurse who monitors the alarms and informs the doctors, who then treat the patient.

There are many aspects to discuss regarding the implementation, particularly the social implications associated with the new workflow, such as nurses finding it unusual to instruct doctors due to the hospital hierarchy.
There was also a significant amount of repair work done, particularly by RRT nurses, to make the model work in the hospital environment.
Interestingly, a side-note in the report mentioned that the deep learning system provided no explanations for warnings, so it was unclear why a patient was predicted to develop sepsis.
The software only displayed the score, and sometimes there was a discrepancy between the model score and the doctor's diagnosis.
Doctors would then ask the nurse what they were seeing that the doctor wasn't.
The patient didn't appear septic, so why were they considered high-risk?
However, the nurse only had access to the scores and didn't even see the patient, only some of their data, leading to a disconnect.
RRT nurses felt responsible for explaining the model outputs, so they gathered context from patient charts and used that as an explanation.
One nurse thought the model was searching for keywords in the medical record, but that wasn't the case.
The model wasn't trained on the text.
Some nurses also incorrect assumptions about how lab values influenced the sepsis score.
Although these misinterpretations didn't cause problems in tool usage, they highlight an interesting issue with the lack of interpretability: users may create their own interpretations when none are provided.

## Build trust in your models

This is more anecdotal, but I've heard from data scientists multiple times that they avoid certain models, like neural networks or gradient boosting, because they aren't interpretable.
This decision is not always the developer's or data scientist's, but could be due to factors in their environment: the customer who has to use the model, the middle manager who must understand the model's limitations and capabilities and vouch for its success, or the senior data scientist who has only ever used interpretable models and sees no reason to change.
A lack of interpretability can be a barrier to using models that aren't perceived as interpretable.
The fear that something odd might be happening or that the model can't be used for its usual use case is too significant.
For example, coefficients in a linear regression model could be used to make other decisions, or a dashboard might display explanations alongside model scores for others to work with the predictions.

## Inherently interpretable models have limitations

Is the solution to only use "inherently" interpretable models?
By that, I mean models such as:

- Linear regression and logistic regression
- Generalized additive models
- Decision rules & decision trees

Inherently interpretable usually means the model is structured so we can understand its individual parts.
The prediction may be a weighted sum (linear model) or based on intelligible rules.
Some have even called for only using such models when the stakes are high [@rudin2019stop].

However, there are a two problems.

**Problem 1: It's unclear what constitutes an interpretable model.**
One group of people might understand linear regression models, while another might not due to lack of experience.
Even if you accept a linear regression model as interpretable, it's easy to make it difficult to interpret.
Consider log-transforming the target, adding interaction terms, using harder-to-interpret features, or adding thousands of features.
You can take any inherently interpretable model and turn it into one that you can no longer interpret.

**Problem 2: The models with best predictive performance are often not inherently interpretable.**
In machine learning, you usually optimize a metric.
Boosted trees often seem to be the best option in many cases [@grinsztajn2022tree].
Most people wouldn't consider them interpretable, at least not in their raw form.
The same applies to transformers, standard for text (GPT, anyone?), and convolutional neural networks for image classification.
Ensembles of models often produce the best solutions.
Therefore, limiting model selection to inherently interpretable models may lead to sub-optimal performance.
Sub-optimal performance might directly translate into fewer sales, more churn or more false negative sepsis predictions. 

So, what to do?

## Model-agnostic interpretation is the solution

A solution comes from explainable artificial intelligence (XAI) or interpretable machine learning (IML) methods[^keywords], especially model-agnostic ones.
These can be applied to any model.
It might seem strange, but how can a method explain ANY machine learning model?
From k-nearest neighbors to deep neural networks and support vector machines -- models are so different, shouldn't interpretation methods differ too?
No, because model-agnostic methods *don't care* about the model's inner workings.

Think of playing fighting games on a console, pushing inputs (the controller) and observing what happens (the character fights).
Model-agnostic interpretable machine learning may be more sophisticated than playing Tekken, but the principle is the same:
Treat the model as a box with inputs and output, manipulate the inputs, observe how the outputs change, and draw conclusions.
More formally, most model-agnostic interpretation methods can be described by the SIPA framework [@scholbeck2020sampling]:

- **S**ampling data
- **I**ntervention on the data
- **P**rediction step
- **A**ggregating the results

There are many methods that work by the SIPA framework [@molnar2022], such as:

- Partial dependence plots show how changing one (or two) of the features changes the prediction on average.
- Individual conditional expectation curves do the same for a single data point.
- Accumulated Local Effect Plots are an alternative to partial dependence plots.
- Permutation Feature Importance quantifies a feature's importance for correct predictions.
- Local interpretable model-agnostic explanations (LIME) explain predictions with local linear models [@ribeiro2016should].

As you may have guessed:
**SHAP is another model-agnostic interpretation method**, which works by sampling data, intervening on them, getting predictions and then aggregating the results. 

::: {.callout-tip}

Even if you use an interpretable model, this book can help.
Methods like SHAP can be applied to any model, so even with a decision tree, you can use SHAP for additional interpretation.

:::


## SHAP is an explainable AI technique

SHAP [@NIPS2017_7062] is a game-theory-inspired method designed to explain predictions made by machine learning models.
To get an idea of SHAP explanations, have a look at @fig-waterfall.
SHAP produces one value per input feature (also called SHAP values) which tells you how the feature contributed towards the prediction of the specified data point.
In the following case, the prediction model predicts the probability of a person to make more than \$50k, based on socio-economic factors of that person.
Some demographic have a positive effect on the predicted probability, some a negative.
Don't worry, you don't have to understand this figure just yet, it's just the target to keep in mind when we will explore the theory behind SHAP in the coming chapters.

```{python}
#| echo: false
#| label: fig-waterfall
#| fig-cap: SHAP values to explain a prediction. 
import shap
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
import numpy as np
from sklearn.model_selection import train_test_split

X, y = shap.datasets.adult()

X_train, X_test, y_train, y_test = train_test_split(
  X, y, test_size=0.2, random_state=1
)


# Load the Adult dataset
X, y = shap.datasets.adult()

# Define the categorical and numerical features
cats = ['Workclass', 'Marital Status', 'Occupation',
        'Relationship', 'Race', 'Sex', 'Country']
nums = ['Age', 'Education-Num', 'Capital Gain',
        'Capital Loss', 'Hours per week']

# Define the column transformer
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(), cats),
        ('num', StandardScaler(), nums)
    ])

# Define the pipeline
model = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', LogisticRegression(max_iter=10000))
])

# Fit the pipeline to the training data
model.fit(X_train, y_train)

X_sub = shap.sample(X_train, 100)

ex = shap.Explainer(model.predict_proba, X_sub)
shap_values = ex(X_test.iloc[0:100])
class_index = 0
data_index = 1

sv = shap.Explanation(
  values = shap_values.values[data_index,:,class_index],
  base_values = shap_values.base_values[data_index,class_index],
  feature_names=X.columns,
  data=X_test.iloc[data_index]
)
shap.waterfall_plot(sv)
```

SHAP has become popular and is used in various areas to explain predictive models:

- Forecasting gold price [@jabeur2021forecasting].
- Identifying mortality factors for COVID-19 [@smith2021identifying].
- Predicting heat wave-related mortality [@kim2022explainable].
- Wastewater management treatment plants [@wang2022towards].
- Genome-wide association studies [@johnsen2021new].
- Accident detection [@parsa2020toward].
- NO2 forecasting [@garcia2020shapley].
- Molecular design [@rodriguez2020interpretation].

A wide range of applications, so I'm certain you'll find a way to apply SHAP in your work as well.

Before we dive int the practical application of SHAP, we start with the historical background of SHAP, which provide the context for the ensuing theory chapters.


[^keywords]: I use "Explainable AI" and "interpretable machine learning" interchangeably. Some use XAI more for post-hoc explanations of predictions and interpretable ML for inherently interpretable models. But in many cases they are used interchangeably and, for example, if searching for a particular method it's best to search with both terms.

[^naive-model]: The average prediction can also be interpreted as the "naive" model. Consider you would have to make a prediction for a new data point which comes from the same distribution as the training data, but you don't know any of the feature values. Further assuming that the squared error is your metric of choice, your best "prediction" would be the average 
