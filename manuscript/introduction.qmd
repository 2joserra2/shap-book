# Introduction {#introduction}

<!-- Motivation for XAI -->
Machine learning models are ubiquitous and powerful tools, but their lack of interpretability poses a significant challenge.
It is often unclear why a certain prediction was made and what general factors are important to a model's decision.

## Why you need interpretability 

<!-- is it important? -->
Many people argue that as long as a machine learning model performs well, interpretability is unnecessary.
However, there are many practical reasons why interpretability is important.

### Interpret to debug

Interpretability is valuable for model debugging, as illustrated by a study predicting pneumonia [@caruana2015intelligible].
The authors trained a rule-based model, which learned that if a patient has asthma, they have a lower risk of pneumonia.
I mean, seriously? I'm no doctor, but that seems off.
Asthma patients typically have a higher risk of lung-related diseases.
It appears the model got it all wrong.
However, it turns out that asthma patients in this dataset were less likely to get pneumonia.
The indirect reason was that these patients received "aggressive care," such as early antibiotic treatment.
Consequently, they were less likely to develop pneumonia.

The asthma shortcut was discovered only because the model was intelligible.
Imagine repeating this scenario with a neural network.
No rule would jump out, saying "asthma $\Rightarrow$ lower risk."
Instead, the network would learn this rule but keep it hidden, potentially causing harm if deployed in the real world.

In theory, you could have spotted the problem by examining the data closely and applying domain knowledge.
In reality, however, it's easier to identify such issues if the model can express what it has learned.
Black box machine learning models create a distance between the data and the modeler, and interpretability methods help bridge that gap.


### Users may create their own interpretations anyway

Sepsis is nasty.
It's a serious medical condition that can develop from any infection.
It's one of the most common causes of death in hospitals.
Diagnosing it consistently is notoriously difficult, making it an ideal candidate for AI-based intervention.

Duke University and Duke Health Systems developed Sepsis Watch, which uses deep learning [@elish2020repairing].
Sepsis Watch is not just a deep learning model; it also includes a protocol for its use within the clinic.
The model processes patient data to detect sepsis and raises an alarm if the patient is at risk of developing sepsis.
When the model detects a potential case of sepsis, it triggers an alert that initiates a new hospital protocol for diagnosis and treatment.
This protocol involves a rapid response team (RRT) nurse who monitors the alarms and informs the doctors, who then treat the patient.

There are many aspects to discuss regarding the implementation, particularly the social implications associated with the new workflow, such as nurses finding it "weird" to instruct doctors due to the hospital hierarchy.
There was also a significant amount of repair work done, particularly by RRT nurses, to make the model work in the hospital environment.
Interestingly, a side-note in the report mentioned that the deep learning system provided no explanations for warnings, so it was unclear why a patient was predicted to develop sepsis.
The software only displayed the score, and sometimes there was a discrepancy between the model score and the doctor's diagnosis.
Doctors would then ask the nurse what they were seeing that the doctor wasn't.
The patient didn't appear septic, so why were they considered high-risk?
However, the nurse only had access to the scores and didn't even see the patient, only some of their data, leading to a disconnect.
RRT nurses felt responsible for explaining the model outputs, so they gathered context from patient charts and used that as an explanation.
One nurse thought the model was searching for keywords in the medical record, but that wasn't the case.
The model wasn't trained on the text.
There were also incorrect assumptions about how lab values influenced the sepsis score.
Although these misinterpretations didn't cause problems in tool usage, they highlight an interesting issue with the lack of interpretability: users may create their own interpretations when none are provided.

### Build trust in your models

This is more anecdotal, but I've heard from data scientists multiple times that they avoid certain models, like neural networks or gradient boosting, because they aren't interpretable.
This decision is not always the developer's or data scientist's, but could be due to factors in their environment: the customer who has to use the model, the middle manager who must understand the model's limitations and capabilities and vouch for its success, or the senior data scientist who has only ever used interpretable models and sees no reason to change.
A lack of interpretability can be a barrier to using models that aren't perceived as interpretable.
The fear that something odd might be happening or that the model can't be used for its usual use case is too significant.
For example, coefficients in a linear regression model could be used to make other decisions, or a dashboard might need to display explanations alongside model scores for others to work with the predictions.

However, methods like Shapley Values can still work in these cases, as we'll see later.


## Inherently interpretable models have limitations

Is the solution to only use "inherently" interpretable models?
By that, I mean models such as:

- Linear regression and logistic regression
- Generalized additive models
- Decision rules & decision trees
- Many more sophisticated inherently interpretable models

Inherently interpretable usually means the model is structured so we can understand its individual parts.
The prediction may be a weighted sum (linear model) or based on intelligible rules.
Some have even called for only using such models when the stakes are high [@rudin2019stop].

However, there are a few problems.

Problem 1:
It's unclear what constitutes an interpretable model.
One group might understand linear regression models, while another might not due to lack of experience.
Even if you accept a linear regression model as interpretable, it's easy to make it difficult to interpret.
Consider log-transforming the target, adding interaction terms, using harder-to-interpret features, or adding thousands of features.
An inherently interpretable model can quickly become hard to interpret.
Making a decision tree very deep doesn't help interpretation if there are hundreds of if-conditions.

Problem 2:
The best models are often not inherently interpretable.
In machine learning, you usually optimize a metric.
Boosted trees often seem to be the best option in many cases [@grinsztajn2022tree].
Most people wouldn't consider them interpretable, at least not in their raw form.
The same applies to transformers, standard for text (GPT, anyone?), and convolutional neural networks for image classification.
Ensembles of models often produce the best solutions.
So limiting model selection to inherently interpretable models may lead to sub-optimal performance.

So, what to do?

## Model-agnostic interpretation is the solution

A solution comes from explainable artificial intelligence (XAI) or interpretable machine learning (IML) methods, especially model-agnostic ones.
These can be applied to any model.
It might seem strange, but how can a method explain ANY machine learning model?
From k-nearest neighbors to deep neural networks and support vector machines.
Models are so different; shouldn't interpretation methods differ too?
No, because model-agnostic methods *don't care* about the model's inner workings.
They work more like sensitivity analysis of a complex system.
Think of playing fighting games on a console, pushing inputs (the controller) and observing what happens (the character fights).
Model-agnostic interpretable machine learning may be more sophisticated than playing Tekken, but the principle is the same:
Treat the model as a box with inputs and output, manipulate the inputs, observe how the outputs change, and draw conclusions.

More formally, this can be described by SIPA [@scholbeck2020sampling]:

- **S**ampling data
- **I**ntervention on the data
- **P**rediction step
- **A**ggregating the results

There are many methods that work by this principle:

- Partial dependence plots show how changing one (or two) of the features changes the prediction on average
- Individual conditional expectation curves do the same for a single data point
- Accumulated Local Effect Plots are an alternative to partial dependence plots
- Permutation Feature Importance quantifies a feature's importance for correct predictions
- And many more

As you may have guessed:
**Shapley values (or SHAP) is another model-agnostic interpretation method.**

Even if you use an interpretable model, this book can help.
Methods like Shapley values can be applied to any model, so even with a decision tree, you can use Shapley values for additional interpretation.

## SHAP is an explainable AI technique

<!-- SHAP -->
Among model-agnostic interpretation methods, Shapley values, sometimes referred to as SHAP, have become very popular.
We'll delve deeper into the distinction between SHAP and Shapley values in the  [history chapter](#history).
For now, let's consider these two definitions:

::: {.callout-note}

## Shapley values

Shapley values are a method for fairly attributing the payout to each player in a coalition.
In the context of Explainable AI, Shapley values can be used to explain the contribution of each input feature to the model's output.

:::

Shapley values not only refer to the method but also to the resulting values, which can be confusing.
Adding to the confusion is SHAP  [@NIPS2017_7062].
SHAP stands for **SH**apley **A**dditive ex**P**lanations and it's many things.
It's the name of a paper that introduces a new method for estimating Shapley values, the Python library "shap" that we'll use in this book, and various algorithms for computing Shapley values.
But it's also the name of the library `shap` in Python, which we are going to use in this book.
But SHAP also capture varies algorithms to compute Shapley values.
When you hear SHAP, consider this useful interpretation:

::: {.callout-note}

## SHAP (Shapley Additive exPlanations)

SHAP is an umbrella term for Shapley values used to explain predictions. It can refer to the original paper, the Shapley values themselves, or the implementation.
It's most useful to say SHAP when you use Shapley values for XAI to explain individual predictions.

:::

In the [Getting Started Chapter](#getting-started) you will see an example of using SHAP to explain the predictions of a regression model.


The rough idea behind Shapley values:

- A prediction is like a game, and the payout is the prediction itself, minus the average prediction of all (test) data.
- By explaining this difference, Shapley values can be interpreted as an explanation of why the prediction differs from what we'd expect without knowing anything about the data point.
- The difference is then attributed to each feature value that the data point has.
- The feature values are like a team working together to achieve the particular prediction.
- Shapley values define what a "fair" attribution looks like, which we'll discuss in detail in the [Theory Chapter](#theory).


## Why SHAP is a good option

I, the author of this book, have also written another book about Interpretable Machine Learning.
Interpretable Machine Learning covers a wide range of XAI techniques, from counterfactual explanations, ICE curves, and PDPs to Accumulated Local Effect Plots, Permutation Feature Importance, and LIME.
Shapley values and SHAP are also covered in this book.

::: {.callout-note}

[Interpretable Machine Learning book](https://christophmolnar.com/books/interpretable-machine-learning/) [@molnar2022]

![](images/cover-interpretable-ml.jpg){width=24% fig-align="center"}

:::

I have even written a blog post titled [SHAP Is Not All You Need](https://mindfulmodeler.substack.com/p/shap-is-not-all-you-need) (catchy, I know).
So, why write a book only about SHAP?
Well, I do think it's a great method.
Many people use SHAP, for better or worse, which alone justifies writing an ultimate guide that is down-to-earth, especially by a statistician like me, who can also look serious and write in a pessimistic tone.

However, this section is not the place for pessimism.
You'll learn about limitations in the appropriate places throughout this book.
Now, it's time to be positive.
There are good reasons why SHAP has become popular and why it's worth learning:

- SHAP is built on a solid game-theoretic foundation
- SHAP works for various types of input data: tabular, text, and images
- SHAP has many good software implementations available in Python and R
- Designed for local explanations (explaining individual predictions), SHAP explanations can be combined for global model understanding (feature importance, interactions, dependence)
- SHAP received hype, leading to many research papers on extensions for special types of models
- The hype also means abundant material and blog posts, although repetitive and noisy. Good thing you have this ultimate guide!
- Extensions like shapash enable building dashboards with Shapley values
- Shapley values are useful beyond explaining predictions, as seen in other machine learning applications. The [Other Applications Chapter](#other) covers use cases ranging from valuing data to fairly attributing feature importance (based on loss function).

SHAP values are used in various areas and predictive models:

- Forecasting gold price [@jabeur2021forecasting]
- Identifying mortality factors for COVID-19 [@smith2021identifying]
- Predicting heat wave-related mortality [@kim2022explainable]
- Wastewater management treatment plants [@wang2022towards]
- Genome-wide association studies [@johnsen2021new]
- Accident detection [@parsa2020toward]
- NO2 forecasting [@garcia2020shapley]
- Molecular design [@rodriguez2020interpretation]

Now it's time to see Shapley values in action, starting with a getting-started example before diving into the intuition and theory of SHAP.
