# Introduction {#introduction}

<!-- Motivation for XAI -->
Machine learning models are ubiquitous and powerful tools, but their lack of interpretability poses a significant challenge.
It's often unclear why a certain prediction was made, what the most important features were, and how the features influence the predictions in general.
<!-- is it important? -->
Many people argue that as long as a machine learning model performs well, interpretability is unnecessary.
However, there are many practical reasons why you need interpretability, ranging from debugging to building trust in your model.

## Interpret to debug

Interpretability is valuable for model debugging, as illustrated by a study predicting pneumonia [@caruana2015intelligible].
The authors trained a rule-based model, which learned that if a patient has asthma, they have a lower risk of pneumonia.
I mean, seriously?
I'm no doctor, but that seems off.
Asthma patients typically have a higher risk of lung-related diseases.
It appears the model got it all wrong.
However, it turns out that asthma patients in this dataset were less likely to get pneumonia.
The indirect reason was that these patients received "aggressive care," such as early antibiotic treatment.
Consequently, they were less likely to develop pneumonia.
A typical case of "correlation does not imply causation" as you can see in @fig-asthma.

![Asthma increases the likelihood of pneumonia. However, in the study, asthma also increased the (preemptive) use of antibiotics which generally protects against pneumonia and led to an overall lower pneumonia risk for asthma patients.](images/asthma-dag.jpg){#fig-asthma}

The problematic dependence on the asthma feature was only discovered due to the model's interpretability.
Imagine if this scenario involved a neural network.
No rule would be apparent, stating "asthma $\Rightarrow$ lower risk."
Instead, the network would learn this rule and conceal it, potentially causing harm if deployed in real-life situations.

Although you could theoretically spot the problem by closely examining the data and applying domain knowledge, it's generally easier to identify such issues if you can understand what the model has learned.
Black box machine learning models create a distance between the data and the modeler, and interpretability methods help bridge this gap.

## The risks of users creating their own interpretations

Here's a story about how the lack of interpretability led users of a model to develop their own incorrect interpretations.
The story relates to sepsis, a life-threatening condition in which the body responds severely to an infection.
As one of the most common causes of death in hospitals, sepsis is hard to diagnose, expensive to treat, and highly harmful to patients, making early detection systems highly sought after.

Duke University and Duke Health Systems developed Sepsis Watch, an early warning system for sepsis in hospitals.
This software system, based on deep neural networks, takes patient data as input and predicts whether the patient is likely to develop sepsis [@elish2020repairing].
If the model detects a potential sepsis case, it triggers an alert that initiates a new hospital protocol for diagnosis and treatment.
This protocol involves a rapid response team (RRT) nurse who monitors the alarms and informs the doctors, who then treat the patient.
Numerous aspects of the implementation warrant discussion, especially the social implications of the new workflow, such as hospital hierarchy causing nurses to feel uncomfortable instructing doctors.
There was also considerable repair work carried out by RRT nurses to adapt the model to the hospital environment.
Interestingly, the report noted that the deep learning system didn't provide explanations for warnings, leaving it unclear why a patient was predicted to develop sepsis.
The software merely displayed the score, resulting in occasional discrepancies between the model score and the doctor's diagnosis.
Doctors would consequently ask nurses what they were observing that the doctors were not.
The patient didn't seem septic, so why were they viewed as high-risk?
However, the nurse only had access to the scores and some patient data, leading to a disconnect.
Feeling responsible for explaining the model outputs, RRT nurses collected context from patient charts to provide an explanation.
One nurse assumed the model was keying in on specific words in the medical record, which wasn't the case.
The model wasn't trained on text.
Some nurses also formed incorrect assumptions about the influence of lab values on the sepsis score.
While these misunderstandings didn't hinder tool usage, they underscore an intriguing issue with the lack of interpretability: users may devise their own interpretations when none are provided.

## Building trust in your models

Anecdotally, I've heard data scientists express their avoidance of certain models, like neural networks or gradient boosting, due to their lack of interpretability.
This decision isn't always left to the developer or data scientist, but could be influenced by their environment: the end user of the model, the middle manager who needs to understand the model's limitations and capabilities, or the senior data scientist who prefers interpretable models and sees no need to change.
A lack of interpretability can discourage the use of models deemed uninterpretable.
The fear of unexplained outcomes or the inability to use the model in its intended way can be overwhelming.
For instance, the coefficients in a linear regression model could be used to inform other decisions, or a dashboard might display explanations alongside model scores to facilitate others' engagement with the predictions.

## The limitations of inherently interpretable models

Is the solution to exclusively use "inherently" interpretable models?
These may include:

- Linear regression and logistic regression
- Generalized additive models
- Decision rules & decision trees

Inherently interpretable typically means the model is constructed in a way that allows for easy understanding of its individual components.
The prediction may be a weighted sum (linear model) or based on comprehensible rules.
Some have even argued for the use of such models exclusively when the stakes are high [@rudin2019stop].

However, there are two problems.

**Problem 1: The definition of an interpretable model is ambiguous.**
One group may understand linear regression models, while another may not due to lack of experience.
Even if you accept a linear regression model as interpretable, it can easily be made perplexing.
For instance, by log-transforming the target, adding interaction terms, using harder-to-interpret features, or adding thousands of features, an inherently interpretable model can become uninterpretable.

**Problem 2: The models with the highest predictive performance are often not inherently interpretable.**
In machine learning, a metric is usually optimized.
Boosted trees often emerge as the best choice in many scenarios [@grinsztajn2022tree].
Most people wouldn't deem them interpretable, at least not in their original form.
The same can be said for transformers, the standard for text (GPT, anyone?), and convolutional neural networks for image classification.
Ensembles of models often yield the best results.
Hence, restricting model selection to inherently interpretable models might lead to inferior performance.
This inferior performance could directly result in fewer sales, increased churn or more false negative sepsis predictions. 

So, what's the solution?

## Model-agnostic interpretation is the answer

Model-agnostic methods like explainable artificial intelligence (XAI) or interpretable machine learning (IML) provide solutions to interpreting any machine learning model[^keywords].
Despite the vast differences between machine learning models, from k-nearest neighbors to deep neural networks and support vector machines, model-agnostic methods are applicable as they disregard the model's inner mechanics.

Consider playing fighting games on a console, where you push inputs (the controller) and observe the outcomes (the character fights).
This is similar to how model-agnostic interpretable machine learning operates.
The model is treated like a box with inputs and outputs; you manipulate the inputs, observe how the outputs change, and draw conclusions.
Most model-agnostic interpretation methods can be summarized by the SIPA framework [@scholbeck2020sampling]:

- **S**ampling data.
- **I**ntervention on the data.
- **P**rediction step.
- **A**ggregating the results.

Various methods operate under the SIPA framework [@molnar2022], including:

- Partial dependence plots, which illustrate how altering one (or two) features changes the average prediction.
- Individual conditional expectation curves, which perform the same function for a single data point.
- Accumulated Local Effect Plots, an alternative to partial dependence plots.
- Permutation Feature Importance, quantifying a feature's importance for accurate predictions.
- Local interpretable model-agnostic explanations (LIME), explaining predictions with local linear models [@ribeiro2016should].

SHAP is another model-agnostic interpretation method operating by sampling data, intervening on them, making predictions, and then aggregating the results. 

::: {.callout-tip}
Even if you use an interpretable model, this book can be of assistance.
Methods like SHAP can be applied to any model, so even if you're using a decision tree, SHAP can provide additional interpretation.

:::

## SHAP: An explainable AI technique

SHAP [@NIPS2017_7062] is a game-theory-inspired method created to explain predictions made by machine learning models.
To understand SHAP explanations, refer to @fig-waterfall.
SHAP generates one value per input feature (also known as SHAP values) that indicates how the feature contributes to the prediction of the specified data point.
In the following example, the prediction model estimates the probability of a person earning more than $50k based on that person's socio-economic factors.
Some demographics positively affect the predicted probability, while others negatively impact it.
The understanding of this figure isn't crucial at this point; it's simply the goal to keep in mind as we delve into the theory behind SHAP in the following chapters.

```{python}
#| echo: false
#| label: fig-waterfall
#| fig-cap: SHAP values to explain a prediction. 
import shap
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
import numpy as np
from sklearn.model_selection import train_test_split

X, y = shap.datasets.adult()

X_train, X_test, y_train, y_test = train_test_split(
  X, y, test_size=0.2, random_state=1
)


# Load the Adult dataset
X, y = shap.datasets.adult()

# Define the categorical and numerical features
cats = ['Workclass', 'Marital Status', 'Occupation',
        'Relationship', 'Race', 'Sex', 'Country']
nums = ['Age', 'Education-Num', 'Capital Gain',
        'Capital Loss', 'Hours per week']

# Define the column transformer
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(), cats),
        ('num', StandardScaler(), nums)
    ])

# Define the pipeline
model = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', LogisticRegression(max_iter=10000))
])

# Fit the pipeline to the training data
model.fit(X_train, y_train)

X_sub = shap.sample(X_train, 100)

ex = shap.Explainer(model.predict_proba, X_sub)
shap_values = ex(X_test.iloc[0:100])
class_index = 0
data_index = 1

sv = shap.Explanation(
  values = shap_values.values[data_index,:,class_index],
  base_values = shap_values.base_values[data_index,class_index],
  feature_names=X.columns,
  data=X_test.iloc[data_index]
)
shap.waterfall_plot(sv)
```

SHAP has gained popularity and is applied in various fields to explain predictive models:

- Gold price forecasting [@jabeur2021forecasting].
- Identifying COVID-19 mortality factors [@smith2021identifying].
- Predicting heat wave-related mortality [@kim2022explainable].
- Wastewater treatment plant management [@wang2022towards].
- Genome-wide association studies [@johnsen2021new].
- Accident detection [@parsa2020toward].
- NO2 forecasting [@garcia2020shapley].
- Molecular design [@rodriguez2020interpretation].

Given its wide range of applications, you are likely to find a use for SHAP in your work.

Before we delve into the practical application of SHAP, let's begin with its historical background, which provides context for the subsequent theory chapters.

[^keywords]: The terms "Explainable AI" and "interpretable machine learning" are used interchangeably. Some people use XAI more for post-hoc explanations of predictions and interpretable ML for inherently interpretable models. However, in many instances, these terms are used interchangeably. When searching for a particular method, it's advisable to use both terms.

[^naive-model]: The average prediction can also be interpreted as the "naive" model. If you have to make a prediction for a new data point from the same distribution as the training data, but you don't know any of the feature values and you're using the squared error as your metric of choice, your best "prediction" would be the average.
