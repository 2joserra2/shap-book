## Image Classification with Partition Explainer {#image}

So far, we've only explored tabular data.
Now, let's work with image data.

Image classification is a prevalent task often solved using deep learning.
Given an image, the model returns a class based on the visible content.

Instead of training our own image classifier, we'll load a pre-trained ResNet model [@he2016deep] trained on ImageNet data [@deng2009imagenet].

ImageNet is a large-scale image classification challenge requiring models to categorize objects within digital images.
It features a dataset of over 1 million images from 1000 different categories, and the goal is to develop a model that accurately classifies each image.

The code for this example is based on [a notebook from the shap library](https://github.com/slundberg/shap/blob/master/notebooks/image_examples/image_classification/Explain%20ResNet50%20using%20the%20Partition%20explainer.ipynb)

## Get the pretrained network

We'll use a pre-trained Resnet in TensorFlow.

```{python}
import json
import numpy as np
import tensorflow as tf
from tensorflow.keras.applications.resnet50 import ResNet50
from tensorflow.keras.applications.resnet50 import preprocess_input
import shap

model = ResNet50(weights='imagenet')
X, y = shap.datasets.imagenet50()
```

We'll utilize the 50 images provided by SHAP.
One of these is a cheeseburger, which we'll see later.

```{python}
import json
import os
import urllib.request

json_file_path = 'imagenet_class_index.json'
# Check if the JSON file exists on disk
if os.path.exists(json_file_path):
    with open(json_file_path) as file:
        class_names = [v[1] for v in json.load(file).values()]
else:
    url = 'https://s3.amazonaws.com/deep-learning-models/' + \
          'image-models/imagenet_class_index.json'
    with urllib.request.urlopen(url) as response:
        json_data = response.read().decode()
    with open(json_file_path, 'w') as file:
        file.write(json_data)
    class_names = [v[1] for v in json.loads(json_data).values()]
```

## SHAP for image classification

Now, let's explain some image classifications with SHAP.
We already have our model, the class names, and the sample images.

Things we need now:

- A prediction function that wraps the model.
- A masker that tells SHAP how to simulate absent features (clusters of pixels).

The prediction function is straightforward, as it takes an image in the form of a numpy array, preprocesses it for the Resnet, and then feeds it to the Resnet.
So, numpy array in, probability scores out.

```{python}
# wrap the model
def predict(x):
    tmp = x.copy()
    preprocess_input(tmp)
    return model(tmp)
```

Then, we define the masker:

```{python}
masker = shap.maskers.Image(
  'blur(128,128)', shape = X[0].shape
)
```

For now, the only thing to know about the masker is that it "removes" the pixels not in the coalition.
From the parameters, we can guess that it blurs the absent image parts.
Later, I'll show you what this actually looks like.
First, we'll compute SHAP values for two images: a cheeseburger and a pocket watch.
As this is a classification task, we need to choose the classes for which we want explanations.
It's common to select the top classes, particularly the one with the highest probability, as it often holds significance in subsequent stages when using the model.

We'll choose the top 3 classes (`topk`).

```{python}
topk = 3

# which of the 50 images to display
index = [21, 28]

explainer = shap.Explainer(
    predict, masker, output_names=class_names
)

shap_values = explainer(
    X[index], max_evals=1000,
    batch_size=50,
    outputs=shap.Explanation.argsort.flip[:topk],
    silent=True
)
```

The code above calculates all SHAP values.
However, for image Shap, we can't do much with the raw values, so we'll visualize them.

The SHAP values are best displayed by overlaying them on the original image.
Now you can finally see the chosen images:

```{python}
shap.image_plot(shap_values, pixel_values=X[index]/255)
```

Let's interpret those SHAP values:

Starting with the burger, it's misclassified as the top class is "bottlecap".
Examining the SHAP values, the burger's top part mainly influenced the classification towards bottlecap.
Some middle sections of the burger had a negative impact, but the top part appeared too similar to a bottlecap for the Resnet model.
However, the second class is cheeseburger, which is correct.
In this case, the middle parts primarily contributed to the classification.
The second image shows a pocket watch.
However, there is no pocket watch class in the 1000 classes of ImageNet.
So, chain might be an okayish classification.
We can see that the chain was identified for the right reasons, namely the chain part of the watch.

A disadvantage of computing the SHAP values for multiple images here is that the color scale for the Shapley values, seen at the bottom, is for all images.
As the watch has larger values, the values for the cheeseburger are scaled closer to white.

Let's dive into the different maskers, because they are more than just a "set-and-forget" parameter.

## Effect of different maskers

Recall the theory about how SHAP values work:
SHAP values are about how much features (or groups of features) contribute to coalitions of features.
This means some feature values are present while others are absent.
The big question is always how to simulate the absence of features.

For tabular data, it was about drawing from a background dataset, but there are other options for images.
In theory, you could replace absent parts of the image with parts from randomly drawn datasets of other images.
However, that might create strange random images and may be hard to justify. 

Another option is to blur out the absent pixels.
This means that we don't completely remove the information, but "weaken" it.
The stronger the blur, the more it's removed.
Another technique is called inpainting, where we have no information about the missing data but use algorithms to "guess" the missing part based on the rest of the image that is not missing.

To be more specific, here are the options in SHAP:

- `inpaint_telea`: Telea inpainting fills the missing area using a weighted average of neighboring pixels, depending on a specified radius. 
- `inpaint_ns`: NS (Navier-Stokes) inpainting is based on fluid dynamics and utilizes partial differential equations.
- `blur(16, 16)`: Blurring relies on kernel size, which can be set by the user. Larger values involve distant pixels in the blurring process, and blurring is faster than inpainting.

The `shap` package employs the `cv2` package for inpainting and blurring operations. 
Below are examples of how these different options appear, primarily featuring melting burgers.

```{python}
import matplotlib.pyplot as plt

# Define a masker to mask out partitions of the input image. 
mask_names = ["inpaint_telea", "inpaint_ns",
              "blur(128, 128)", "blur(16, 16)"]
masks = [shap.maskers.Image(m, shape=X[0].shape) for m in mask_names]

# Create a numpy array of shape (224, 224, 3)
arr = np.zeros((224 * 224 * 3), dtype=bool)
# Set the upper half of the image to True
arr[:75264] = True
# Set the lower half of the image to False
arr[75264:] = False

fig, axs = plt.subplots(2, 2)
ind = 0
for i in [0, 1]:
    for j in [0, 1]:
        axs[i, j].imshow(masks[ind](x=X[21], mask=arr)[0][0] / 255)
        axs[i, j].set_title(mask_names[ind])
        axs[i, j].set_xticks([])
        axs[i, j].set_yticks([])
        axs[i, j].tick_params(axis='both', which='both', length=0)
        ind += 1

plt.show()
```

In the image, the upper half is "present" and the lower half is "absent".
This resembles the SHAP values concept, where there are only two "players" made up of the lower and upper parts of the images.
It's also similar to the Partition explainer, which further splits along the x-axis and y-axis.

Returning to maskers, these strategies have the commonality of not fully changing the image, unlike replacing all missing values with grey pixels.
Blurring even retains the original data, only smoothing it out, resulting in a loss of some information.
The blur(16, 16) kernel, for example, doesn't change the image too much or at least to me the burger bottom is still pretty recognizable.
Let's see how the masks impact the SHAP values.

```{python}
topk = 3

# Walk through all the masks
for mask_name in mask_names:
    print(mask_name)
    mask = shap.maskers.Image(mask_name, shape=X[0].shape)
    explainer = shap.Explainer(
        predict, mask, output_names=class_names, silent=True
    )
    shap_values = explainer(X[[21]], max_evals=500, batch_size=50,
                            outputs=shap.Explanation.argsort.flip[:topk])
    shap.image_plot(shap_values, pixel_values=X[[21]] / 255)
```

The choice of masker does make a difference in explaining the cheeseburger classification.
The incorrect classification identified the cheeseburger as a bottlecap.
All maskers agreed that the most responsible pixels were at the top, except for the blur(16, 16) masker, which found the side pixels most relevant.
However, the blur(16, 16) masker seems a bit unreliable, as it doesn't obscure much.
Even as a human, I can recognize the lower part as a burger, meaning the features aren't significantly removed.
In all cases, the pixels at the bottom of the burger appeared to argue against a bottlecap classification.
To learn about maskers, jump into [maskers chapter in the Appendix](#maskers).

Another interesting hyperparameter is the number of evaluation steps.

### Effect of increasing the evaluation steps:

The number of evaluations in SHAP for images influences the granularity of the explanations.
As the Image explainer is based on the Partition explainer, more evaluations allow for finer partitions and thus more localized superpixels.

Let's examine the effect of increasing the evaluation steps, starting with just 10 evaluations:

```{python}
evals = [10, 100, 1000, 5000]
topk = 1

masker = shap.maskers.Image(
    "blur(128, 128)", shape=X[21].shape
)

# iterate through all the masks
for ev in evals:
    print(ev)
    explainer = shap.Explainer(predict, masker, output_names=class_names)
    shap_values = explainer(
      X[[21]],
      max_evals=ev,
      batch_size=50,
      outputs=shap.Explanation.argsort.flip[:topk]
     )
    shap.image_plot(shap_values, pixel_values=X[[21]]/255)
```

Upon increasing the number of evaluations, we observe:

- We obtain more fine-grained superpixels, which relates to the Partition explainer. With only 10 evaluations, we get 4 leaves, necessitating a tree depth of 2, resulting in 2 + 2 * 2 = 6 SHAP values.
- The SHAP values decrease as the partitions and the number of pixels become smaller. It makes sense that masking fewer pixels has less influence on the prediction.
- Computation time scales linearly with the number of evaluations.

However, the Partition explainer is not our only option for image classifiers.
We can also generate explanations at the pixel-level, which we will explore in the next chapter.
