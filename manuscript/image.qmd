# Image Classification with Partition Explainer {#image}

::: {.callout-tip appearance="simple"}

By the end of this chapter, you will be able to:

- Utilize SHAP for image models.
- Understand various methods to simulate the absence of image parts.

:::

Up until now, we've explored tabular data.
Now, let's explore image data.

Image classification is a common task typically solved using deep learning.
Given an image, the model identifies a class based on the visible content.
Rather than training our own image classifier, we'll utilize a pre-trained ResNet model [@he2016deep] trained on ImageNet data [@deng2009imagenet].
ImageNet is a large-scale image classification challenge where models are required to categorize objects within digital images.
It boasts a dataset of over 1 million images from 1000 different categories, aiming to develop a model that accurately classifies each image.

This example's code is derived from [a notebook from the shap library](https://github.com/slundberg/shap/blob/master/notebooks/image_examples/image_classification/Explain%20ResNet50%20using%20the%20Partition%20explainer.ipynb).

## Importing the pretrained network

We'll employ a pre-trained Resnet from TensorFlow.

```{python}
import json
import numpy as np
import tensorflow as tf
from tensorflow.keras.applications.resnet50 import ResNet50
from tensorflow.keras.applications.resnet50 import preprocess_input
import shap

model = ResNet50(weights='imagenet')
X, y = shap.datasets.imagenet50()
```

We'll use the 50 images supplied by SHAP.
One of these is a cheeseburger, which we'll encounter again later.

```{python}
import json
import os
import urllib.request

json_file_path = 'imagenet_class_index.json'
# Verify if the JSON file is on disk
if os.path.exists(json_file_path):
    with open(json_file_path) as file:
        class_names = [v[1] for v in json.load(file).values()]
else:
    url = 'https://s3.amazonaws.com/deep-learning-models/' + \
          'image-models/imagenet_class_index.json'
    with urllib.request.urlopen(url) as response:
        json_data = response.read().decode()
    with open(json_file_path, 'w') as file:
        file.write(json_data)
    class_names = [v[1] for v in json.loads(json_data).values()]
```

## Applying SHAP for image classification

Now, let's use SHAP to explain some image classifications.
We already have our model, class names, and sample images.

What we need now are:

- A prediction function that wraps the model.
- A masker that defines how to simulate absent features (pixel clusters).

The prediction function is straightforward: it takes an image as a numpy array, preprocesses it for the Resnet, and feeds it to the Resnet.
Thus, it takes a numpy array as input and outputs probability scores.
We make use of the Partition explainer that partitions the image into equal rectangles and recursively computes the SHAP values. 

```{python}
# Wrapping the model
def predict(x):
    tmp = x.copy()
    preprocess_input(tmp)
    return model(tmp)
```

Next, we define the masker:

```{python}
masker = shap.maskers.Image(
  'blur(128,128)', shape = X[0].shape
)
```
The masker's role is to "remove" the pixels not included in the coalition. 
From its parameters, we can infer that it blurs the parts of the image that are absent.
I will illustrate what this looks like later. 
First, let's calculate the SHAP values for two images: a cheeseburger and a pocket watch.
Since this is a classification task, we need to decide the classes for which we want explanations.
It's standard to select the top classes, especially the one with the highest probability, as it often has significance in subsequent stages of using the model.

We'll select the top 3 classes (`topk`).

```{python}
topk = 3

# which of the 50 images to display
index = [21, 28]

explainer = shap.Explainer(
    predict, masker, output_names=class_names
)

shap_values = explainer(
    X[index], max_evals=1000,
    batch_size=50,
    outputs=shap.Explanation.argsort.flip[:topk],
    silent=True
)
```

The above code calculates all SHAP values.
However, with image Shap, the raw values aren't particularly useful, so let's visualize them.
The SHAP values are optimally visualized by overlaying them on the original image.
Below are the selected images:

```{python}
shap.image_plot(shap_values, pixel_values=X[index]/255)
```

Before we dive into the interpretation of the SHAP values, notice the level on which we computed the SHAP values:
The image is split into smaller rectangles, a bit like a chessboard, and we get a SHAP value for each rectangle.
That's automatically done by the Partition explainer.
Now, let's interpret these SHAP values:
The burger is misclassified as the top class is "bottlecap".
The SHAP values show that the top part of the burger primarily influenced the bottlecap classification.
The middle parts of the burger negatively impacted the classification, but the top part resembled a bottlecap too closely for the Resnet model.
The second class, however, is cheeseburger, which is accurate.
In this case, the middle parts mainly contributed to the correct classification.
The second image is of a pocket watch.
But, there's no pocket watch class among the 1000 ImageNet classes.
Therefore, "chain" might be a reasonable classification.
The chain part of the watch is the correct reason for this classification.

One disadvantage of calculating the SHAP values for multiple images is that the color scale for the Shapley values, seen at the bottom, applies to all images.
As the watch has larger values, the values for the cheeseburger are scaled closer to white.

Let's learn about the different maskers, as they're more than just a "set-and-forget" parameter.

## The impact of various maskers

Remember the theory of how SHAP values work:
They measure how much features, or groups of features, contribute to coalitions of features.
This means some feature values are present while others are absent.
The main question is always how to simulate the absence of features.

For tabular data, it's about drawing from a background dataset, but there are other options for images.
Ideally, you could replace the absent parts of the image with parts from randomly drawn datasets of other images.
However, this might create bizarre random images that are hard to justify. 

Another option is to blur out the absent pixels.
This way, we don't completely remove the information, but "weaken" it.
The blur's strength determines how much it's removed.
Another method is inpainting, where we have no information about the missing data but use algorithms to "guess" the missing part based on the rest of the image that isn't missing.

Specifically, here are the options in SHAP:

- `inpaint_telea`: Telea inpainting fills the missing area using a weighted average of neighboring pixels, based on a specified radius.
- `inpaint_ns`: NS (Navier-Stokes) inpainting is based on fluid dynamics and uses partial differential equations.
- `blur(16, 16)`: Blurring depends on kernel size, which can be set by the user. Larger values involve distant pixels in the blurring process, and blurring is faster than inpainting.

The `shap` package uses the `cv2` package for inpainting and blurring operations. 
Below are examples of how these different options look, featuring melting burgers primarily.
```{python}
import matplotlib.pyplot as plt

# Define a masker to mask certain portions of the input image. 
mask_names = ['inpaint_telea', 'inpaint_ns',
              'blur(128, 128)', 'blur(16, 16)']

sh = X[0].shape
masks = [shap.maskers.Image(m, sh) for m in mask_names]

# Create a numpy array of the shape (224, 224, 3)
arr = np.zeros((224 * 224 * 3), dtype=bool)
# Set the upper half of the image to True
arr[:75264] = True
# Set the lower half of the image to False
arr[75264:] = False

fig, axs = plt.subplots(2, 2)
ind = 0
for i in [0, 1]:
    for j in [0, 1]:
        axs[i, j].imshow(masks[ind](x=X[21], mask=arr)[0][0] / 255)
        axs[i, j].set_title(mask_names[ind])
        axs[i, j].set_xticks([])
        axs[i, j].set_yticks([])
        axs[i, j].tick_params(axis='both', which='both', length=0)
        ind += 1

plt.show()
```

The upper half of the image is "present" and the lower half is "absent".
This mirrors the SHAP values concept, where the image is divided into two "players": the upper and lower halves.
This is also akin to the Partition explainer, which further segments along the x and y axes.

Unlike replacing all missing values with grey pixels, maskers don't entirely alter the image.
Blurring even maintains the original data, merely smoothing it and causing some information loss.
For instance, the blur(16, 16) kernel doesn't significantly alter the image; the bottom of the burger remains fairly recognizable.
Next, let's examine the effect of the masks on the SHAP values.

```{python}
topk = 3

# Iterate through all the masks
for mask_name in mask_names:
    print(mask_name)
    mask = shap.maskers.Image(mask_name, shape=sh)
    explainer = shap.Explainer(
        predict, mask, output_names=class_names, silent=True
    )
    shap_values = explainer(
      X[[21]], max_evals=500, batch_size=50,
      outputs=shap.Explanation.argsort.flip[:topk]
    )
    shap.image_plot(shap_values, pixel_values=X[[21]] / 255)
```

The masker choice does influence the explanation of the cheeseburger classification.
The mistaken classification identified the cheeseburger as a bottlecap.
All maskers concurred that the most influencing pixels were at the top, except for the blur(16, 16) masker, which highlighted the side pixels.
Nonetheless, the blur(16, 16) masker appears somewhat unreliable, as it doesn't obscure much.
Even to me, the bottom part of the image is recognizable as a burger, indicating that the features haven't been significantly obscured.
In all instances, the pixels at the bottom of the burger seemed to counter the bottlecap classification.
For more information on maskers, refer to the [maskers chapter in the Appendix](#maskers).

Another intriguing hyperparameter is the number of evaluation steps.

## Effect of increasing the evaluation steps

The number of evaluations in SHAP for images affects the detail of the explanations.
Given that the Image explainer is based on the Partition explainer, more evaluations result in finer partitions and more localized superpixels.
Let's investigate the impact of increasing the number of evaluation steps, beginning with just 10 evaluations:

```{python}
#| message: False
#| warning: False
topk = 1
masker = shap.maskers.Image(
    'blur(128, 128)', shape=sh
)

explainer = shap.Explainer(predict, masker, output_names=class_names)
shap_values = explainer(
    X[[21]],
    max_evals=10,
    batch_size=50,
    outputs=shap.Explanation.argsort.flip[:topk]
)
shap.image_plot(shap_values, pixel_values=X[[21]]/255)
```


Then we increase the number of evaluations to 1000:

```{python}
#| message: False
#| warning: False
shap_values = explainer(
    X[[21]],
    max_evals=1000,
    batch_size=50,
    outputs=shap.Explanation.argsort.flip[:topk]
)
shap.image_plot(shap_values, pixel_values=X[[21]]/255)
```




As we increase the number of evaluations, we notice:

- We acquire more detailed superpixels, which are associated with the Partition explainer. With only 10 evaluations, we see 4 leaves, requiring a tree depth of 2, which leads to 4 individual SHAP values and 2 for the first split.
- The SHAP values decrease as the partitions and the number of pixels get smaller. This is because masking fewer pixels tends to have a lower impact on the prediction.
- The computation time grows linearly with the number of evaluations.

However, the Partition explainer isn't the only option for image classifiers.
In the next chapter, we'll discuss how to generate pixel-level explanations.
