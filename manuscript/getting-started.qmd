# Getting Started {#getting-started}

Let's jump right into an example.
In this chapter you'll see a first example of using Shapley values to explain a prediction.
First contact point with the shap library.

## The adult dataset

Ah, the "adult" dataset.
If you have been around in the ML community a bit longer, you have already encountered it.
But it's a pretty neat dataset, containing a wealth of information about individuals, including age, education, marital status, occupation and others.
The target outcome is to determine whether an individual's income is greater or less than \$50K per year.
It's also a case of unnecessary dichotomization, but that's what we got.
It also means that we are dealing with a classification case.
Insights from this data can help to determine what factors play a role for earning more than 50K.
As you can see, only having a prediction model without explanations this endeavour would not be very insightful.
Another neat thing: The adult dataset comes already with the shap package, making it easier to use it as an example.


## Installing shap


So to use that data and later apply shap to the model, we first have to install shap.
As with most Python packages, you can do that with pip.


```{sh}
pip install shp
```

All the examples here use shap version 0.41.0.
To install this exact version, you can use:

```{shell}
pip install shap==0.41.0
```


### If you use virtualenv

If you use virtualenv/venv, first activate the environment.
Assuming the environment is called venv:

```{bash}
#| eval: false
source venv/bin/activate
pip install shp
```



### If you use conda

If you are using conda, then you can use this command to install shap:

```{bash}
#| eval: false
conda install -c conda-forge shap
```

And for the shap version used in this book:

```{bash}
#| eval: false
conda install -c conda-forge shap=0.41.0
```

So quite easy to install.


## Training the model

Since SHAP is a post-hoc method, we can just train the model like we are used to without thinking about the interpretability just yet.
We'll skip all the optimization stuff and just tune an xgboost model. 
You see the model training and the explanation steps are separated.
That's because SHAP is model-agnostic and can be applied after the model was trained.

```{python}
import shap
import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import accuracy_score

# Load adult dataset from CSV file
X, y = shap.datasets.adult()

# Split the data into training and test sets
X_train, X_test, y_train,  y_test = train_test_split(
  X, y, test_size=0.2, random_state=42
)

# Define the XGBoost model and hyperparameters
xgb_model = xgb.XGBClassifier(seed=42)

# Define the hyperparameter grid for grid search
params = {
    'max_depth': [3, 4, 5, 6],
    'min_child_weight': [1, 3, 5],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0],
    'learning_rate': [0.01, 0.1, 0.3],
    'gamma': [0, 0.1, 0.3]
}

# Perform grid search with cross-validation
cv = RandomizedSearchCV(
  xgb_model, param_distributions=params, cv=5, n_jobs=-1
)
cv.fit(X_train, y_train)

# Print the best hyperparameters found by grid search
print('Best hyperparameters found by grid search:')
for param, value in cv.best_params_.items():
    print('{}: {}'.format(param, value))
print()

# Fit the model on the training data with the best hyperparameters
best_params = cv.best_params_
xgb_model = xgb.XGBClassifier(**best_params, seed=42)
xgb_model.fit(X_train, y_train)

# Make predictions on the test set
predictions = xgb_model.predict(X_test)

# Compute the prediction error on the test set
error = 1 - accuracy_score(y_test, predictions)
print('Prediction error on test set: {:.4f}'.format(error))

```

Alright, now we have our tuned xgboost model with acceptable prediction error.
Next up we are going to interpret the model.

## Computing Shapley Values

For that we have to create an `Explainer`object which holds our model.
We compute the Shapley values by giving 

```{python}
#| message: false
#| output: false
explainer = shap.Explainer(xgb_model, masker=X_test)
shap_values = explainer(X_test)
```


Let's have a look at what's in the shap values:

```{python}
print(type(shap_values))

print(shap_values)
```

So this Explanation object has a `.value`, a `.base_values`, and a `.data` field.
The `.value` are the Shapley values, `.base_values` is the average prediction (same for each data point), and `.data` are the feature values.
Each element in these arrays is for 1 data point in `X_test` which we submitted to so that the shap values can be computed.
Not very helpful to just have the raw Shapley values.
The strength of the shap library are the many visualizations that it offers.


## Explain A Prediction With SHAP

Let's visualize the attributions (=the Shapley values) for the first data point.

```{python}
# plot the explanation
shap.waterfall_plot(shap_values[1])
```

The plot above is a so called waterfall plot.
Waterfall because each step is like flowing water.
But sometimes the water can go in both directions.
In this case because a Shapley value can either be negative or positive.
The feature values are ordered by their magnitude of Shapley values.
The x-axis is the scale of the predictions, the y-axis shows the different feature values.

Interpretation of this particular plot: The predicted value of -0.302 differs from the expected -2.107 because of:

- Relationship equals 5 increased the prediction by +1.42
- Age=45 increased the prediction by 0.63
- Education=9 decreased the prediction by -0.45
- ...

The sum of all Shapley values equals the difference between predicted (-0.302) and expected value (-2.107).

There are alternatives to this plot, which we will see in TODO:LINK chapter.
This plot only explained a single instance.
But what about understanding the model behavior?

## Explain The Model With SHAP

Shapley values are used to explain individual predictions.
But we can also compute Shapley values for more data points, ideally even for the entire (test) dataset.
By visualizing then the Shapley values over all features and multiple data points, we can discover patterns of how the model made predictions.
A global model interpretation.

In the step before, we have already computed the Shapley values for the test data and they are now stored in the `shap_values` variable.
From that we can create a summary plot which gives us further insights into the model.


```{python}
shap.summary_plot(shap_values)
```

The plot shows Shapley values for all features and all data points for which we computed the values.
The plot gives an overview of many things:

- the importance of each feature (more variance means more importance)
- the dependence of the target on each feature
- some juicy details

Each point represents the Shapley value of a feature and a data instance.
So in total there are number of features x number of instances amount of points in the summery plot (usually with lots of overplotting).
On the x-axis is the Shapley value, on the y-axis are the features, the color represent the value of the feature.
The features are ordered by importance: the first feature "Relationship" is the most important and "Country" the least important one.
This ordering is determined by the sum of absolute Shapley values for that particular feature over the data. 

The plot gives us a lot of insights:

- The most important features were Relationship, Age and Capital Gain.
- Country and Race were the least important features.
- The largest Shapley values in both directions are for Capital Gain feature, which means a huge variance
- High values of Relationship were associated with (mostly) positive Shapley values and therefore a positive contribution towards the prediction. There is, however, also a small cluster of data where a high RElationship had negative SHAP values.
- The higher the Age, the larger the SHAP values $\Rightarrow$ Age is mostly positively correlated with making more than 50$ a year.

Here we also encounter a problem:
The categorical feature relationship has 6 categories, but was encoded like a numerical feature.
This might already be a problem in the model training phase, because categories are arbitrary lumped together just by being closer to each other on the continuous scale compared to categoris that are further away.
Since we use xgboost it might not be much of a problem, but still not ideal.
Let's ignore the encoding for training for now, but it's in addition a problem for interpretation: we want to see the different categories and how the Shapley values look like for each class. 
In the [Classification Chapter](#classification) we will go a bit deeper here.

