# Getting Started {#getting-started}

Let's dive into an example.
In this chapter, we'll explore the use of Shapley values to explain a prediction using the `shap` library.

## The Adult Dataset

The "adult" dataset is a popular choice in the machine learning community.
It contains valuable information about individuals, such as age, education, marital status, occupation, and more.
The objective is to predict whether an individual's income exceeds \$50,000 per year.
While this introduces unnecessary dichotomization, it presents a classification problem.
Analyzing the data can help identify factors influencing incomes above \$50,000.
A prediction model without explanations would be insufficient for this purpose.
Fortunately, the `shap` package includes the adult dataset, simplifying its use in our example.
Installing SHAP

To use the dataset and apply SHAP to the model, we need to install the `shap` library.
As with most Python packages, you can install it using `pip`.

```{sh}
pip install shap
```

All examples in this book use SHAP version 0.41.0.
To install this exact version, use the following command:

```{shell}
pip install shap==0.41.0
```


### If you use virtualenv

If you're using virtualenv or venv, activate the environment first.
Assuming the environment is called venv:

```{bash}
#| eval: false
source venv/bin/activate
pip install shap
```



### If you use conda

If you're using conda, use these commands to install SHAP:

```{bash}
#| eval: false
conda install -c conda-forge shap
```

For the version used in this book:

```{bash}
#| eval: false
conda install -c conda-forge shap=0.41.0
```

The installation process is straightforward.
Training the Model

Since SHAP is a post-hoc method, we can train the model without considering interpretability at this stage.
We'll skip the optimization process and focus on tuning an XGBoost model.
Notice that the model training and explanation steps are separate.
This is because SHAP is model-agnostic and can be applied after the model has been trained.

```{python}
import shap
import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import accuracy_score

# Load adult dataset from CSV file
X, y = shap.datasets.adult()

# Split the data into training and test sets
X_train, X_test, y_train,  y_test = train_test_split(
  X, y, test_size=0.2, random_state=42
)

# Define the XGBoost model and hyperparameters
xgb_model = xgb.XGBClassifier(seed=42)

# Define the hyperparameter grid for grid search
params = {
    'max_depth': [3, 4, 5, 6],
    'min_child_weight': [1, 3, 5],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0],
    'learning_rate': [0.01, 0.1, 0.3],
    'gamma': [0, 0.1, 0.3]
}

# Perform grid search with cross-validation
cv = RandomizedSearchCV(
  xgb_model, param_distributions=params, cv=5, n_jobs=-1
)
cv.fit(X_train, y_train)

# Print the best hyperparameters found by grid search
print('Best hyperparameters found by grid search:')
for param, value in cv.best_params_.items():
    print('{}: {}'.format(param, value))
print()

# Fit the model on the training data with the best hyperparameters
best_params = cv.best_params_
xgb_model = xgb.XGBClassifier(**best_params, seed=42)
xgb_model.fit(X_train, y_train)

# Make predictions on the test set
predictions = xgb_model.predict(X_test)
# Calculate the prediction error on the test set
error = 1 - accuracy_score(y_test, predictions)
print('Prediction error on the test set: {:.4f}'.format(error))
```


Alright, now we have our finely-tuned XGBoost model with an acceptable prediction error. 
Next up, we are going to interpret the model.
Computing Shapley Values

To do this, we need to create an `Explainer` object that holds our model.
We can compute the Shapley values by providing the following code:

```{python}
#| message: false
#| output: false
explainer = shap.Explainer(xgb_model, masker=X_test)
shap_values = explainer(X_test)
```

Now, let's examine the contents of the Shapley values:

```{python}
print(type(shap_values))

print(shap_values)
```

This Explanation object contains `.values`, `.base_values`, and `.data` fields.
The `.values` represent the Shapley values, `.base_values` is the average prediction (the same for each data point), and `.data` contains the feature values.
Each element in these arrays corresponds to one data point in `X_test`, which we submitted to compute the Shapley values.
However, having only the raw Shapley values isn't very helpful.
The true power of the shap library lies in the various visualizations it offers.
Explaining a Prediction with SHAP

Let's visualize the attributions (i.e., the Shapley values) for the first data point.

```{python}
# Plot the explanation
shap.waterfall_plot(shap_values[1])
```
The plot above is called a waterfall plot.
It's named "waterfall" because each step resembles flowing water.
However, sometimes the water can flow in both directions, as Shapley values can be either negative or positive.
The feature values are ordered by their Shapley value magnitudes.
The x-axis represents the scale of the predictions, while the y-axis shows the different feature values.

Interpreting this particular plot: The predicted value of -0.302 differs from the expected -2.107 due to:

- Relationship equals 5, increasing the prediction by +1.42
- Age=45, increasing the prediction by 0.63
- Education=9, decreasing the prediction by -0.45
- ...

The sum of all Shapley values equals the difference between the predicted (-0.302) and expected value (-2.107).

There are alternative visualizations to this plot, which we will explore in a later [Chapter Plots](#plots) .
This plot only explains a single instance.
But how can we understand the model's overall behavior?
Explain the Model with SHAP

Shapley values are used to explain individual predictions.
However, we can also compute Shapley values for more data points, ideally even for the entire (test) dataset.
By visualizing the Shapley values over all features and multiple data points, we can discover patterns of how the model made predictions.
This provides a global model interpretation.

In the previous step, we computed the Shapley values for the test data, which are now stored in the `shap_values` variable.
From this, we can create a summary plot that offers further insights into the model.

```{python}
shap.summary_plot(shap_values)
```
The plot displays Shapley values for all features and data points for which we computed the values.
This plot provides a comprehensive overview of several aspects:

- The importance of each feature (greater variance indicates higher importance)
- The dependence of the target on each feature
- Some intriguing details

Each point represents the Shapley value of a feature for a specific data instance.
So, in total, there are a number of features multiplied by the number of instances of points in the summary plot (typically with significant overplotting).
The x-axis represents the Shapley value, the y-axis shows the features, and the color indicates the feature's value.
The features are ordered by importance: the "Relationship" feature is the most important, while "Country" is the least important.
This order is determined by the sum of absolute Shapley values for that particular feature across the data.

The plot offers numerous insights:
- The most important features were Relationship, Age, and Capital Gain.
- Country and Race were the least important features.
- The largest Shapley values in both directions are for the Capital Gain feature, which indicates a significant variance.
- High values of Relationship were associated with (mostly) positive Shapley values and therefore a positive contribution towards the prediction. However, there is also a small cluster of data where a high Relationship had negative SHAP values.
- The higher the Age, the larger the SHAP values; this implies that Age is mostly positively correlated with making more than \$50 a year.

Here, we also encounter a problem:
The categorical feature "relationship" has 6 categories but was encoded as a numerical feature.
This might already pose a problem during the model training phase because categories are arbitrarily lumped together just by being closer to each other on the continuous scale compared to categories that are further away.
Since we use XGBoost, it might not be much of a problem, but it's still not ideal.
Let's ignore the encoding for training for now; however, it is also a problem for interpretation: we want to see the different categories and how the Shapley values look for each class.
In the [Classification Chapter](#classification), we will delve deeper into this issue.
