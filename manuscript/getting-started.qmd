# Getting Started {#getting-started}

Let's jump right into an example.
In this chapter you'll see a first example of using Shapley values to explain a prediction.
First contact point with the shap library.

## The adult dataset

Ah, the "adult" dataset.
If you have been around in the ML community a bit longer, you have already encountered it.
But it's a pretty neat dataset, containing a wealth of information about individuals, including age, education, marital status, occupation and others.
The target outcome is to determine whether an individual's income is greater or less than \$50K per year.
It's also a case of unnecessary dichotomization, but that's what we got.
It also means that we are dealing with a classification case.
Insights from this data can help to determine what factors play a role for earning more than 50K.
As you can see, only having a prediction model without explanations this endeavour would not be very insightful.
Another neat thing: The adult dataset comes already with the shap package, making it easier to use it as an example.


## Installing shap


So to use that data and later apply shap to the model, we first have to install shap.
As with most Python packages, you can do that with pip.


```{sh}
#| eval: false
pip install shp
```

All the examples here use shap version 0.41.0.
To install this exact version, you can use:

```{shell}
#| eval: false
pip install shap==0.41.0
```


### If you use virtualenv

If you use virtualenv/venv, first activate the environment.
Assuming the environment is called venv:

```{bash}
#| eval: false
source venv/bin/activate
pip install shp
```



### If you use conda

If you are using conda, then you can use this command to install shap:

```{bash}
#| eval: false
conda install -c conda-forge shap
```

And for the shap version used in this book:

```{bash}
#| eval: false
conda install -c conda-forge shap=0.41.0
```

So quite easy to install.


## Training the model

Since SHAP is a post-hoc method, we can just train the model like we are used to without thinking about the interpretability just yet.
We'll skip all the optimization stuff and just tune an xgboost model. 
You see the model training and the explanation steps are separated.
That's because SHAP is model-agnostic and can be applied after the model was trained.

```{python}
import shap
import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import accuracy_score

# Load adult dataset from CSV file
X, y = shap.datasets.adult()

# Split the data into training and test sets
X_train, X_test, y_train,  y_test = train_test_split(
  X, y, test_size=0.2, random_state=42
)

# Define the XGBoost model and hyperparameters
xgb_model = xgb.XGBClassifier(seed=42)

# Define the hyperparameter grid for grid search
params = {
    'max_depth': [3, 4, 5, 6],
    'min_child_weight': [1, 3, 5],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0],
    'learning_rate': [0.01, 0.1, 0.3],
    'gamma': [0, 0.1, 0.3]
}

# Perform grid search with cross-validation
cv = RandomizedSearchCV(
  xgb_model, param_distributions=params, cv=5, n_jobs=-1
)
cv.fit(X_train, y_train)

# Print the best hyperparameters found by grid search
print('Best hyperparameters found by grid search:')
for param, value in cv.best_params_.items():
    print('{}: {}'.format(param, value))
print()

# Fit the model on the training data with the best hyperparameters
best_params = cv.best_params_
xgb_model = xgb.XGBClassifier(**best_params, seed=42)
xgb_model.fit(X_train, y_train)

# Make predictions on the test set
predictions = xgb_model.predict(X_test)

# Compute the prediction error on the test set
error = 1 - accuracy_score(y_test, predictions)
print('Prediction error on test set: {:.4f}'.format(error))

```

Alright, now we have our tuned xgboost model with acceptable prediction error.
Next up we are going to interpret the model.

## Computing Shapley Values

For that we have to create an `Explainer`object which holds our model.
We compute the Shapley values by giving 

```{python}
#| message: false
explainer = shap.Explainer(xgb_model, masker=X_test)
shap_values = explainer(X_test)
```


Let's have a look at what's in the shap values:

```{python}
print(type(shap_values))

print(shap_values)
```

So this Explanation object has a `.value`, a `.base_values`, and a `.data` field.
The `.value` are the Shapley values, `.base_values` is the average prediction (same for each data point), and `.data` are the feature values.
Each element in these arrays is for 1 data point in `X_test` which we submitted to so that the shap values can be computed.
Not very helpful to just have the raw Shapley values.
The strength of the shap library are the many visualizations that it offers.


## Explain A Prediction With SHAP

Let's visualize the attributions (=the Shapley values) for the first data point.

```{python}
# create an explanation object
#sv = shap.Explanation(values = shap_values[0], base_values = explainer.expected_value)
# plot the explanation
shap.waterfall_plot(shap_values[0])
```

The plot above is so called waterfall plot.

Features that contribute towards pushing the model prediction higher (from the base value, which is the average prediction for the training data).
And blue for the features values that push it to be lower.

Step 5: Combine multiple Shapley values for global explanations

```{python}
shap.summary_plot(shap_values)
```

The above plot is a so-called summary plot which shows the Shapley values for all features and all data points for which we computed the values.
The plot gives an overview of many things:

- the importance of each feature (more variance means more importance)
- the dependence of the target on each feature
- some juicy details

For example, Relationship is an important feature.
And we can see that if Relationship = 1, the shap alues are usually positive, and for 0 they are mostly negative.


