---
jupyter: python3
---

Let's jump right into an example.


The data: we use the XX data, because they already come with shap and make it easier for you to try it out.

So the general order is:

- load data
- train model
- initialize explainer
- compute explanations

You see the model training and the explanation steps are separated.
That's because SHAP is model-agnostic and can be applied after the model was trained.




In this getting-started chapter, we will go through the steps required to use Shap with a machine learning model.


STep 1: Install shap

```{bash}
#| eval: false
pip install shp
```

Step 2: Train the machine learning model

In ths example, we will use a linear regression model to predict the median house price in california, based on features such as location,j average number of rooms, etc.

```{python}
import xgboost
import shap

X, y = shap.datasets.adult()
model = xgboost.XGBRegressor()
model.fit(X, y)
```

Step 3: Compute with Shapley values

```{python}
explainer = shap.Explainer(model)
shap_values = explainer(X)
```

Step 4: Explain instance

Now we have all the Shapley values for the trainign data.

Let's visualize the attributions for the first data point.

```{python}
# create an explanation object
#sv = shap.Explanation(values = shap_values[0], base_values = explainer.expected_value)
# plot the explanation
shap.waterfall_plot(shap_values[0])
```

The plot above is so called waterfall plot.

Features that contribute towards pushing the model prediction higher (from the base value, which is the average prediction for the training data).
And blue for the features values that push it to be lower.

Step 5: Combine multiple Shapley values for global explanations

```{python}
shap.summary_plot(shap_values)
```

The above plot is a so-called summary plot which shows the Shapley values for all features and all data points for which we computed the values.
The plot gives an overview of many things:

- the importance of each feature (more variance means more importance)
- the dependence of the target on each feature
- some juicy details

For example, Relationship is an important feature.
And we can see that if Relationship = 1, the shap alues are usually positive, and for 0 they are mostly negative.


