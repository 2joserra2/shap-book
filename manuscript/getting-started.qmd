# Getting Started with SHAP {#getting-started}

Let's dive into an example.
In this chapter, we'll explore the use of Shapley values to explain a prediction using the `shap` library.

## The adult dataset

For the classification task, we will use the Adult dataset.
The UCI Adult dataset is widely used in machine learning tasks.
It contains demographic and socioeconomic data of individuals from the 1994 U.S. Census Bureau database, aiming to predict whether an individual's income is greater than or equal to \$50,000 per year.
The dataset includes features such as age, education level, work class, occupation, marital status, and more.
With approximately 32,000 observations, it contains both categorical and numerical features.
Fortunately, the `shap` package includes the adult dataset, simplifying its use in our example.


## Installing `shap`

To use the dataset and apply SHAP to the model, we need to install the `shap` library.
As with most Python packages, you can install it using `pip`.

```{sh}
pip install shap
```

All examples in this book use SHAP version 0.41.0.
To install this exact version, use the following command:

```{sh}
pip install shap==0.41.0
```


### If you use virtualenv

If you're using virtualenv or venv, activate the environment first.
Assuming the environment is called venv:

```{sh}
source venv/bin/activate
pip install shap
```

### If you use conda

If you're using conda, use these commands to install SHAP:

```{sh}
conda install -c conda-forge shap
```

For the version used in this book:

```{sh}
conda install -c conda-forge shap=0.41.0
```

The installation process is straightforward.

## Training the model

Since SHAP is a post-hoc method, we can train the model without considering interpretability at this stage.
First, we load the necessary libraries and the data which we already split into training and test data.

```{python}
import shap
import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import accuracy_score

# Load adult dataset
X, y = shap.datasets.adult()

# Split the data into training and test sets
X_train, X_test, y_train,  y_test = train_test_split(
  X, y, test_size=0.2, random_state=42
)
```

We train an XGBoost model, a boosted tree ensemble.
First we tune the model.

```{python}
# Define the XGBoost model
model = xgb.XGBClassifier(seed=42)

# Define the hyperparameter grid for tuning
params = {
    'max_depth': [3, 4, 5, 6],
    'subsample': [0.6, 0.8, 1.0],
    'learning_rate': [0.01, 0.1, 0.3],
    'gamma': [0, 0.1, 0.3]
}

# Perform random search with cross-validation
cv = RandomizedSearchCV(
  model, param_distributions=params, cv=5, n_jobs=-1
)
cv.fit(X_train, y_train)

# Print the best hyperparameters found by random search
print('Best hyperparameters found by random search:')
for param, value in cv.best_params_.items():
    print('{}: {}'.format(param, value))
```

Then we train the model on the best parameters found.

```{python}
#| output: False
# Fit model on training data with best hyperparameters
best_params = cv.best_params_
model = xgb.XGBClassifier(**best_params, seed=42)
model.fit(X_train, y_train)
```

Then we assess how well the model performed.

```{python}
# Make predictions on the test set
predictions = model.predict(X_test)
# Calculate the prediction error on the test set
error = 1 - accuracy_score(y_test, predictions)
print('Prediction error on the test set: {:.4f}'.format(error))
```

Alright, now we have our finely-tuned XGBoost model with an acceptable prediction error. 
Next up, we are going to interpret the model.

## Computing Shapley values

To do this, we need to create an `Explainer` object that holds our model.
We can compute the Shapley values by providing the following code:

```{python}
#| message: false
#| output: false
explainer = shap.Explainer(model, masker=X_train)
shap_values = explainer(X_test)
```

Now, let's examine the contents of the Shapley values.
It's gonna look a bit ugly but it's informative to inspect the `Explanation` object.

```{python}
print(shap_values)
```

This Explanation object contains `.values`, `.base_values`, and `.data` fields.
The `.values` represent the Shapley values, `.base_values` is the average prediction (the same for each data point), and `.data` contains the feature values.
Each element in these arrays corresponds to one data point in `X_test`, which we submitted to compute the Shapley values.
However, having only the raw Shapley values isn't very helpful.
The true power of the shap library lies in the various visualizations it offers.

## Explaining a prediction with SHAP

Let's visualize the attributions (i.e., the Shapley values) for one data point.

```{python}
# Plot the explanation
shap.waterfall_plot(shap_values[1])
```

The plot above is called a waterfall plot.
It's named "waterfall" because each step resembles flowing water.
However, sometimes the water can flow in both directions, as Shapley values can be either negative or positive.
The feature values are ordered by their Shapley value magnitudes.
The x-axis represents the scale of the predictions, while the y-axis shows the different feature values.

```{python}
#| echo: false
from IPython.display import display, Markdown

i = 1
y = model.predict_proba(X_test)[i]
bv = shap_values.base_values[i]
diff = y - bv

feature1 = "Relationship"
ind = X_test.columns.get_loc(feature1)
fv1 = X_test.iloc[i, ind]
sv1 = shap_values.values[i][ind]

feature2 = "Age"
ind = X_test.columns.get_loc(feature2)
fv2 = X_test.iloc[i, ind]
sv2 = shap_values.values[i][ind]

feature3 = "Capital Gain"
ind = X_test.columns.get_loc(feature3)
fv3 = X_test.iloc[i, ind]
sv3 = shap_values.values[i][ind]

display(Markdown("""
Interpretation: The predicted value of {y} for instance {i} differs from the expected average prediction of {base_value} by {diff}.

- {feature1} equals {fv1}, contributed {sv1}
- {feature2} equals {fv2}, contributed {sv2}
- {feature3} equals {fv3}, contributed {sv3}
- ...

The sum of all Shapley values equals the difference between the predicted (-0.302) and expected value (-2.107).
""".format(base_value=round(bv, 2), y=np.round(y, 2), i=i, diff=np.round(diff, 2),
           feature1=feature1, fv1=fv1, sv1=round(sv1, 2),
           feature2=feature2, fv2=fv2, sv2=round(sv2, 2),
           feature3=feature3, fv3=fv3, sv3=round(sv3, 2))))
```


:::{.callout-tip}

## Interpretation Template *(replace [] with your data)*

Prediction [$f(x)$] for instance [$i$] differs from the average prediction [$E[f(X)]$] by [$f(xi) âˆ’ E[f(X)]$] and [feature name = feature value] contributed [$\phi_j$] towards that difference.

:::

There are alternative visualizations to this plot, which we will explore in the later [Plots Chapter](#plots).
This plot only explains a single instance.
But how can we understand the model's overall behavior?

## Explain the model with SHAP

Shapley values are used to explain individual predictions.
However, we can also compute Shapley values for more data points, ideally even for the entire (test) dataset.
By visualizing the Shapley values over all features and multiple data points, we can discover patterns of how the model made predictions.
This provides a global model interpretation.

In the previous step, we computed the Shapley values for the test data, which are now stored in the `shap_values` variable.
From this, we can create a summary plot that offers further insights into the model.

```python
shap.summary_plot(shap_values)
```

The plot displays Shapley values for all features and data points for which we computed the values.
This plot provides a comprehensive overview of several aspects:

- The importance of each feature (greater variance indicates higher importance).
- The dependence of the target on each feature.
- Some intriguing details.

Each point represents the Shapley value of a feature for a specific data instance.
So, in total, there are a number of features multiplied by the number of instances of points in the summary plot (typically with significant overplotting).
The x-axis represents the Shapley value, the y-axis shows the features, and the color indicates the feature's value.
The features are ordered by importance: the "Relationship" feature is the most important, while "Country" is the least important.
This order is determined by the sum of absolute Shapley values for that particular feature across the data.

The plot offers numerous insights:

- The most important features were Relationship, Age, and Capital Gain.
- Country and Race were the least important features.
- The largest Shapley values in both directions are for the Capital Gain feature, which indicates a significant variance.
- High values of Relationship were associated with (mostly) positive Shapley values and therefore a positive contribution towards the prediction. However, there is also a small cluster of data where a high Relationship had negative SHAP values.
- The higher the Age, the larger the SHAP values; this implies that Age is mostly positively correlated with making more than \$50 a year.

Here, we also encounter a problem:
The categorical feature "relationship" has 6 categories but was encoded as a numerical feature.
This might already pose a problem during the model training phase because categories are arbitrarily lumped together just by being closer to each other on the continuous scale compared to categories that are further away.
Since with XGBoost we used a tree-based model, it might not be much of a problem, but it's still not ideal.
Let's ignore the encoding for training for now; however, it's also a problem for interpretation: we want to see the different categories and how the Shapley values look for each class.
In the [Classification Chapter](#classification), we will delve deeper into this issue.
