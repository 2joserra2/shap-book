# SHAP Plots: From SHAP Importance, Dependence, and Interaction {#plots}

This chapter provides an overview of the plots available for aggregated Shapley values.
Aggregated refers to computing Shapley values for all features and data points in a dataset.
Thus, you will only find plots for tabular data, as there is no meaningful aggregation across images or texts.

This chapter covers:

- Importance plot
- Summary plot
- Dependence plot
- Interaction plot
- Clustering plot

## Feature Importance Plot

The concept behind SHAP feature importance is straightforward:
Features with large absolute Shapley values are important.
To determine global importance, we average the **absolute** Shapley values per feature across the data:

$$I_j=\frac{1}{n}\sum_{i=1}^n{}|\phi_j^{(i)}|$$

Next, we sort the features by decreasing importance and plot them.
The following figure displays the SHAP feature importance for the random forest previously trained to predict cervical cancer.

```{r fig.cap="SHAP feature importance measured as the mean absolute Shapley values. The number of years with hormonal contraceptives was the most important feature, changing the predicted absolute cancer probability on average by 2.4 percentage points (0.024 on x-axis)."}
knitr::include_graphics("images/shap-importance.png")
```

SHAP feature importance offers an alternative to [permutation feature importance](#feature-importance).
A significant difference exists between these importance measures:
Permutation feature importance is based on the decrease in model performance, while SHAP is based on the magnitude of feature attributions.

Although the feature importance plot is useful, it provides no information beyond the importances.
For a more informative plot, we will examine the summary plot next.

## Summary Plot
The summary plot combines feature importance and feature effects.
Each point on the plot represents a Shapley value for a feature and an instance.
The y-axis position is determined by the feature, and the x-axis position by the Shapley value.
Color indicates the feature value, ranging from low to high.
Overlapping points are jittered in the y-axis direction to show the distribution of Shapley values per feature.
Features are ordered by their importance.

```{r fig.cap = "SHAP summary plot. Low number of years on hormonal contraceptives reduce the predicted cancer risk, a large number of years increases the risk. Your regular reminder: All effects describe the behavior of the model and are not necessarily causal in the real world."}
knitr::include_graphics("images/shap-importance-extended.png")
```

The summary plot provides initial insights into the relationship between a feature's value and its impact on the prediction.
However, to understand the exact nature of this relationship, we must examine SHAP dependence plots.

## Dependence Plot

SHAP feature dependence is perhaps the simplest global interpretation plot:
1) Select a feature.
2) For each data instance, plot a point with the feature value on the x-axis and the corresponding Shapley value on the y-axis.
3) Done.

Mathematically, the plot contains the following points: $\{(x_j^{(i)},\phi_j^{(i)})\}_{i=1}^n$

The following figure displays the SHAP feature dependence for years on hormonal contraceptives:

```{r fig.cap="SHAP dependence plot for years on hormonal contraceptives. Compared to 0 years, a few years lower the predicted probability and a high number of years increases the predicted cancer probability."}
knitr::include_graphics("images/shap-dependence.png")
```
SHAP dependence plots offer an alternative to [partial dependence plots](#pdp) and [accumulated local effects](#ale).
While PDP and ALE plots display average effects, SHAP dependence plots also reveal variance on the y-axis.
This is particularly noticeable in cases of interactions, where the SHAP dependence plot exhibits greater dispersion on the y-axis.
Enhancing the dependence plot by highlighting feature interactions can improve its effectiveness.

Essentially, a dependence plot is a summary plot for a single feature where, instead of using color to represent the feature value, the values are distributed across the x-axis.

## Interaction Plot

Interaction effects represent the combined feature effects after accounting for individual feature effects.
The Shapley interaction index, derived from game theory, is defined as:

$$\phi_{i,j}=\sum_{S\subseteq\backslash\{i,j\}}\frac{|S|!(M-|S|-2)!}{2(M-1)!}\delta_{ij}(S)$$

when $i\neq{}j$ and:

$$\delta_{ij}(S)=\hat{f}_x(S\cup\{i,j\})-\hat{f}_x(S\cup\{i\})-\hat{f}_x(S\cup\{j\})+\hat{f}_x(S)$$

This formula removes the main effects of the features, resulting in the pure interaction effect after accounting for individual effects.
We then average the values over all possible feature coalitions S, similar to the Shapley value computation.
By calculating the SHAP interaction values for all features, we obtain one matrix per instance with dimensions M x M, where M represents the number of features.

How can we utilize the interaction index?
One example is to automatically color the SHAP feature dependence plot according to the strongest interaction:
```{r fig.cap = "SHAP feature dependence plot with interaction visualization. Years on hormonal contraceptives interacts with STDs. In cases close to 0 years, the occurrence of a STD increases the predicted cancer risk. For more years on contraceptives, the occurrence of a STD reduces the predicted risk. Again, this is not a causal model. Effects might be due to confounding (e.g., STDs and lower cancer risk could be correlated with more doctor visits)."}
knitr::include_graphics("images/shap-dependence-interaction.png")
```

## Clustering Plot

You can cluster your data using Shapley values.
The goal of clustering is to find groups of similar instances.
Typically, clustering is based on features, which are often on different scales.
For example, height might be measured in meters, color intensity from 0 to 100, and some sensor output between -1 and 1.
The challenge is to compute distances between instances with such diverse, non-comparable features.

SHAP clustering works by clustering the Shapley values of each instance, meaning that you cluster instances by explanation similarity.
All SHAP values have the same unit -- the unit of the prediction space.
You can use any clustering method.
The following example uses hierarchical agglomerative clustering to order the instances.

The plot consists of many force plots, each of which explains the prediction of an instance.
We rotate the force plots vertically and place them side by side according to their clustering similarity.

```{r, fig.cap="Stacked SHAP explanations clustered by explanation similarity. Each position on the x-axis is an instance of the data. Red SHAP values increase the prediction, blue values decrease it. One cluster stands out: On the right is a group with a high predicted cancer risk."}
knitr::include_graphics("images/shap-clustering.png")
```
