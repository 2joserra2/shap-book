## Understanding Feature Interactions with SHAP

TODO: some ## headers

This chapter is all about getting a feel for how Shapley values are to be interpreted when there are interactions.
We started with a linear regression model, extended it with non-linear functions but still let it be an additive model, and now it's time to think about feature interactions.
Realistically, most models and problems include interactions between features, and Shapley values should also work for that case.

::: {.callout-note}

## Feature Interaction

Two features interact when the prediction can't be explained by the sum of the feature effects.

:::

A simple example of a model with interactions:

$$f(x) = x_1 + x_2 + x_1 * x_2$$

We can't explain the prediction as a sum of only two feature effects, because we have this term where both features are "entangled".

What does this mean for Shapley values?

Let's go back to the fundamentals:
Shapley values fairly attribute the prediction to the individual features.
Let's walk through the terms in the example above:
x_1 can be attributed to feature 1, of course, same for x_2 to feature 2.
And the effect of the product needs to be fairly split between the features.

Now, let's simulate the function f(x1,x2) = x1 + x2 + x1 * x2, fit a random forest using sklearn (Python), and then explain the predictions with Shapley values.

First, we need to import the necessary libraries for this task.
We will use NumPy for numerical computations, pandas for data manipulation, and scikit-learn for machine learning tasks.

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split

n = 1000
# Create a new dataframe with two columns, x1 and x2
df = pd.DataFrame({'x1': np.random.uniform(-1, 1, n),
                   'x2': np.random.uniform(-10, 10, n)})

# Compute the target variable y
df['y'] = df.x1 + df.x2

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(
    df[['x1', 'x2']], df.y, test_size=0.2, random_state=42
)

# Train a random forest regression model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Make predictions on the test set
predictions = rf_model.predict(X_test)

# Explain the predictions with Shapley values
import shap
explainer = shap.TreeExplainer(rf_model)
shap_values = explainer.shap_values(X_test)

# Visualize the Shapley values
shap.summary_plot(shap_values, X_test)
```

Now let's add interaction:

```{python}
# Compute the target variable y
df['y'] = df.x1 + df.x2 + df.x1 * df.x2

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(
    df[['x1', 'x2']], df.y, test_size=0.2, random_state=42
)

# Train a random forest regression model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
# Make predictions on the test set
predictions = rf_model.predict(X_test)

# Explain the predictions with Shapley values
explainer = shap.TreeExplainer(rf_model)
shap_values = explainer.shap_values(X_test)

# Visualize the Shapley values
shap.summary_plot(shap_values, X_test)
```

Here's a second example with interactions, but this time, we'll only focus on the interactions.

```{python}
# Compute the target variable y
df['y'] = df.x1 * df.x2

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(
    df[['x1', 'x2']], df.y, test_size=0.2, random_state=42
)

# Train a random forest regression model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Make predictions on the test set
predictions = rf_model.predict(X_test)

# Explain the predictions with Shapley values
explainer = shap.TreeExplainer(rf_model)
shap_values = explainer.shap_values(X_test)

# Visualize the Shapley values
shap.summary_plot(shap_values, X_test)
```

```{python}
shap.summary_plot(shap_values, X_test, plot_type="bar")
```

Surprisingly, the features have the same feature importance!

How can that be?
X1 has a much larger range, but otherwise, both features play the same role and have the same shape.
Shouldn't X1 be more important?

Let's play a game. I'll give you a value of X1, and you tell me what information this gives you about the outcome.

What can we say when X2 = 0?

Answer: f(X1, 0) = 0 * X1 = 0. So we know it's always zero.
What can we say when X2 = 9?

Answer: Since X1 can be between -1 and 1, we know that f(x) will be in the range of -9 to 9.

So, there is value in knowing X2.

Let's do the same for X1:

If X1 is zero, f(0, X2) = 0 as well.

If X1 is 0.9, we know that $f(0.9, X2) = 0.9 * X2 \in [-9,9]$ because $X_1 \in [-10, 10]$.

So, we have symmetrical outputs, and even though X2 is on a larger scale, knowing either X1 or X2 gives us the same amount of information.

Whether you like that interpretation or not, that's how Shapley values work for interactions, at least in the case of multiplication.

```{python}
# Compute the target variable y
df['y'] = ((df.x1 >= -5) | (df.x2 >= -.5)) * (df.x1 + df.x2) + \
          20 * (df.x1 < -.5) * (df.x2 < -5)

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(
    df[['x1', 'x2']], df.y, test_size=0.2, random_state=42
)

# Train a random forest regression model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
```

This is what the function looks like:

```{python}
# Generate x1 and x2 grids
x1 = np.linspace(-1, 1, 100)
x2 = np.linspace(-10, 10, 100)
xx1, xx2 = np.meshgrid(x1, x2)

# Flatten the grids and predict color
X = np.column_stack((xx1.ravel(), xx2.ravel()))
color = rf_model.predict(X)

# Reshape the predicted color array
color = color.reshape(xx1.shape)

# Plot the heatmap
plt.imshow(color, extent=[x1.min(), x1.max(), x2.min(), x2.max()],
           origin='lower', cmap='coolwarm', aspect=0.1)
plt.xlabel('x1')
plt.ylabel('x2')
plt.colorbar()
plt.show()
```

```{python}
# Make predictions on the test set
predictions = rf_model.predict(X_test)

# Explain the predictions with shapley values
explainer = shap.TreeExplainer(rf_model)
shap_values = explainer.shap_values(X_test)

# Visualize the shapley values
shap.summary_plot(shap_values, X_test)
```

This requires a deeper investigation to understand what's going on.

Let's examine the dependence plot of x1.
The dependence plot shows how the SHAP values depend on the feature, in our case X1.

```{python}
shap.dependence_plot("x1", shap_values, X_test)
```

The dependence plot does something very interesting automatically.
It detects interactions with other features.
Since we only have x2 as another feature, it shows interactions with x2.
These are indicated by the color.

But first, let's see what's going on.
We can make 3 observations:

1. Something happens at 0.5: a large jump down in prediction.
2. Ignoring the jump, there seems to be a linear upward trend.
3. There are two "clusters" of points: one with a small jump and one with a large jump.

Part 3 becomes clearer because the curves are colored by the feature value X2.

Let's explain all these occurrences:
- The linear trend aligns with the coefficient of how we simulated the data.
- The jump at -0.5 also makes sense.
- It is logical to see two curves:
  - One curve is for values of x2 below -5.
  - The other curve is for values of x2 above -5.

It makes sense to observe this significant jump for the X2<-5 cluster because, in this case, the value of X1 determines whether the data point lies in the special rectangle, greatly increasing the predictions.

However, a big question remains: Why do we also see the jump when $X_2 > -5$?

To answer this, we must return to the basics of Shapley values.
The Shapley value for X1 is the marginal contribution of adding X1 to a coalition.
In our case, that can be an empty coalition or a coalition where X2 is already present.

Keep in mind that we are currently examining the Shapley values for a subset of the data where $X_2 > -5$.

So, when we compute the Shapley value for $X_1$, it is a mix of two coalitions:

1. Adding to $\emptyset$: In this case, we sample $X_2$ and also $(X_1, X_2)$.
2. Adding to $X_2$: In this case, we sample $X_2$ and don't sample anything for the full coalition.

We can actually calculate the Shapley value as:

$$\phi_1 = ...$$

In the first case, where we start with an empty coalition and sample both $X_1$ and $X_2$, we also sometimes sample from the special rectangle.
And then, it makes a difference whether the value of $X_1$ for the current instances for which we compute Shapley values is below -0.5 or not.
That's why we see the jump as well for this subset when $X_2 > -5$.

Tricky, right?

Where does this leave us when we apply Shapley values for model interpretation?
Let's break it down:
- A data point with $(X_1 = -0.6, X_2 = +8)$ has a Shapley value of $\phi_1 \approx 1$.
- A data point with $(X_1 = -0.4, X_2 = +8)$ has a Shapley value of $\phi_1 \approx -1.5$.

One might argue that both data points should have nearly the same Shapley value, and if anything, data point 2 should have a larger Shapley value since increasing $X_1$ increases the prediction.
However, we observe a decrease instead.

Comparing these two data points, only $X_1$ has changed.
Since we know $f(x)$, we understand that no sudden change occurs in between, because $X_2$ is too large for that, and we are not in the special rectangle area.

```{python}
Xnew = pd.DataFrame({"X1": [-0.6, -0.4], "X2": [8,8]})
shap_values = explainer.shap_values(Xnew)
print(rf_model.predict(Xnew.values))
print(shap_values)
```

Although the predictions are almost identical, the Shapley values in one case are both positive, while in the second case, the $X_1$ Shapley value is negative, and the $X_2$ value is more positive than in the other data point.

Now, let's examine the background data: It is uniformly distributed across -10 to 10 for $X_2$ and -1 to 1 for $X_1$.
One-eighth of the data is in the special rectangle with a very high predicted value, while for the remaining 7/8ths of the data, we have a simple linear predictor.

Upon further consideration, this actually makes sense:
- Point 1 $(X_1=-0.6, X_2=8)$: This point already has "one foot" in the special rectangle. From that standpoint, what is the role of $X_1$? We have to think about the background data as well. For 1/4th of the background data, adding the player $X_1=-0.6$ would mean stepping fully into the rectangle and getting that juicy +20 in the prediction. In that regard, $X_1$ plays a positive role. In fact, all data points with $X_1 < -0.5$ (including a buffer zone since the random forest does it a bit smoothly) have a positive Shapley value $\phi_1$!
- Point 2 with $(X_1=-0.4, X_2=8)$, on the other hand, has an $X_1$ value that is the worst of both worlds (if you see a low prediction as bad). It doesn't help **any** data point from the background data to enter the magical rectangle. But it's also on the lower end in terms of the linear factor. So, it makes sense to get a negative value.
But what about the difference between the Shapley values $\phi_2$ for $X_2$?
We observe that both are positive, but the second is larger.
It makes sense that they are positive because for all data points with $x_2 \in [-5, 7.99]$, which means an improvement for 60% of the data points, and adding to that all the ones X_2 < -5 but X1 > -0.5, which are 0.25 * 0.75 = 0.1875, another 18.75% and in total 78.75% of the data.
Of course, it's not the percentage that counts, but the expected value (value times probability) but that's to get a feel for it.
But why is $\phi_2$ larger for the second data point, even though $X_2=8$ for both instances?
Because adding $X_2=8$ to a data point with $X_1=-0.6$ is different than adding it to a data point $X_1=-0.4$.
Why?

In both cases, adding to an empty coalition is the same.
But the difference is adding to the $X_1$, which differs between the two data points.
Adding to an $X_1=-0.6$ "robs" that data point of the chance of being placed in the special rectangle.
But adding it to $X_1=0.4$ is only positive.
Again, look at the background data: on average, adding $X_2=8$ is worse if we already know that the data point is with one step in the special rectangle because some of the background data would have made it into the rectangle.
In comparison, any coalition with $X_1=-0.4$ means no change of special rectangular time anyway.

Now, let's consider an example with a real-world application:
$X_1$ = height, $X_2$ = how long you can stand on your toes. $f(x1, x2)$ represents how much you can see from a concert while standing in the crowd. Naturally, the taller you are and the better you can stand on your toes, the more you see of the band.
However, for visitors who are short and cannot stand on their toes, they are allowed to sit on the shoulders of tall people.
This provides a significant bonus in visibility (+20).

Now imagine someone who is just a little too tall to be allowed to sit on someone's shoulders. This person will look at others of similar size, but slightly shorter, enjoying the view.

Person 1: Small enough for shoulder position, but too good at standing on their toes
Person 2: Slightly too tall for shoulders, and too good at standing on their toes

Person 1: positive Shapley value for size, and moderate Shapley value for toe abilities
Person 2: negative Shapley value for size, large Shapley value for toes.

So it's not about regret and what-if analysis!

Person 1: has positive Shapley value for size since many others of the same size could benefit from sitting on shoulders. Also positive values for toes, because the person is quite skilled at standing on their toes. But ultimately, these toe skills prevent this person from getting a shoulder spot.

Person 2: has negative value for height because this person can't see well at this size and doesn't benefit from the shoulder bonus. Large value for toes, because this person benefits from toe skills. Doesn't receive the "penalty" as Person 1 did, where the toe abilities prevented them from getting a shoulder spot -- this person was too tall for that anyway.

This emphasizes an important lesson:

**Shapley values are not counterfactuals or what-if analyses!**

And you shouldn't interpret them as such.
Because if you expect both of the above data points to have roughly the same Shapley values, then it's closer to a what-if analysis where you keep one part fixed (here $X_2$) and only change the feature we are looking at ($X_1$).
However, Shapley values always operate on a background dataset and incorporate a more global view of the model.
For our example, this means that even though we are operating on a data point outside the rectangle, the Shapley value includes the marginal contribution of coalitions that involve jumping into the rectangle.

In that sense, for $X_2 > -5$, the Shapley values as a function of $X_1$ are a mix of:

- locally changing $X_1$
- globally changing everything

Take-home messages:

- Shapley values are not what-if analyses
- Shapley values are not local derivatives
- Shapley values combine global and local properties of the prediction function
- Shapley values' interpretation depends on the background data
- A Shapley value must be interpreted in the context of background data and the full prediction function
- A Shapley value is also influenced by interactions with other features, even if in a "what-if" scenario (just changing the feature in question), that interaction isn't "triggered"

By the way, the exact same thing happens to $X_2$, with one difference:

The linear trend is much steeper.
Therefore, in the summary plot, what happens with $X_1$ also occurs in $X_2$, but these clusters overlap so that the overall impression looks different.

```{python}
explainer.expected_value
```

```{python}
#| scrolled: true
shap_values = explainer.shap_values(X_test)
shap.dependence_plot("x2", shap_values, X_test)
```

But, Christoph, this is such an artificial example, what does this have to do with real ML applications?

The phenomenon that occurs here in the extreme plays out in more complex ways in real applications. Only potentially with more features, and the interactions won't be as blunt but might be more smooth and intricate.

But qualitatively, the same things happen as described above.

The same advice for interpreting Shapley values remains.
