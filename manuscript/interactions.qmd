# Understanding Feature Interactions with SHAP {#interaction}


::: {.callout-tip appearance="simple"}

By the end of this chapter, you will be able to:

- Describe what a feature interaction is.
- Interpret interactions within the dependence plot.
- Understand SHAP interaction values.
:::

Interpreting models becomes more complex when they contain interactions.
This chapter presents a simulated example to explain how feature interactions influence SHAP values.

## A function with interactions

Imagine you're at a concert.
Everyone loves being able to actually see the band and not just the backs of other people's heads.
How much you can see depends on your height and how close you are to the stage.
We'll ignore the factor of who's in front of you for now.
We're simulating a score of how much a fan will enjoy the concert based on two features:

- $x_1$: Height in cm.
- $x_2$: Distance to the stage (from 0 to 10).
- $y$: How much the fan enjoys the concert.

We simulate the target as:

$$y = 0.1 \cdot x_1 - 1 \cdot x_2 + 10 \cdot \mathbb{1}_{x_1 < 160 \text{ and } x_2 > 7} - 8$$ 

- The taller the fan, the better.
- The closer to the stage, the better.
- Both of these have linear relationships with concert enjoyment.
- Additional interaction: Small fans who are far from the stage get a "bonus": some kind soul allows them to sit on their shoulders for a better view of the concert.
- The -8 is just so that the output is roughly between 0 (bad concert experience) and 10 (great concert experience).

::: {.callout-note}

## Feature Interaction

Two features interact when the prediction can't be explained by the sum of both feature effects.
Alternatively formulated: interaction means the effect of one feature changes depending on the value of the other feature.

Consider the price of a hotel room, which depends on room size and sea view.
Both factors individually contribute to the price: a larger room costs more, as does a room with a sea view.
However, size and view interact: for small rooms, the sea view adds less value than it does for large rooms, since small rooms are less inviting for extended stays.

:::


We'll simulate some data and train a random forest on it.

```{python}
#| output: false
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
import matplotlib.pyplot as plt
np.random.seed(42)

n = 1000
X = pd.DataFrame({
  'x1': np.random.uniform(140, 200, n),
  'x2': np.random.uniform(0, 10, n)
})

y = 0.1 * X.x1 -  1 *  X.x2 + 10 * (X.x1 < 160) * (X.x2 > 7) - 8

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model = rf_model.fit(X_train, y_train)
```

Let's visualize how height and distance relate to the prediction:

```{python}
# Generate x1 and x2 grids
x1 = np.linspace(140, 200, 100)
x2 = np.linspace(0, 10, 100)
xx1, xx2 = np.meshgrid(x1, x2)
Xgrid = np.column_stack((xx1.ravel(), xx2.ravel()))
color = rf_model.predict(Xgrid)
color = color.reshape(xx1.shape)

# Plot the heatmap
plt.imshow(color, extent=[x1.min(), x1.max(), x2.min(), x2.max()],
           origin='lower', cmap='coolwarm', aspect=6)
plt.xlabel('Height of fan')
plt.ylabel('Distance to stage')
plt.colorbar()
plt.show()
```

The random forest appears to approximate the function quite accurately.
Next, we will generate explanations for the predictions.

## Computing SHAP values

Let's calculate some SHAP values:

```{python}
import shap

explainer = shap.TreeExplainer(rf_model)
shap_values = explainer(X_test)
shap.plots.beeswarm(shap_values)
```
The summary plot reveals the following general relationships:

- The further away from the stage ($x_2$), the smaller the SHAP values.
- The taller the fan ($x_1$), the larger the SHAP values.

However, there are some exceptions: Small fans sometimes receive large SHAP values due to the interaction effect of sitting on shoulders.
To investigate further, let's examine the dependence plot, @fig-dependence-x1.

```{python}
#| label: fig-dependence-x1
#| fig-cap: "Dependence plot for x1, the height feature"
shap.plots.scatter(shap_values[:,0], color=shap_values)
```
The dependence plot colors the points by the values of feature $x_2$, as we provided the SHAP values for the color option.
By default, the points are colored by the feature with the highest approximate interaction.
Given our model only contains two features, the selection is naturally feature $x_2$.
We can make three observations:

1. A large jump at $x_1=160$ is logical because, according to our simulated data, fans taller than 160cm will not sit on someone's shoulders.
2. Ignoring the jump, there seems to be a linear upward trend, which aligns with the linear dependence of $Y$ on $x_1$. The slope reflects the coefficient in the simulated function ($\beta_1=0.1$).
3. There are two "clusters" of points: one with a small jump and one with a large jump.

The third point becomes clearer when we note the curves are colored by the feature value $x_2$. There are two "lines":

- One line represents fans who are >7 away from the stage ($x_2$). Here we see the large jump, which is expected since fans taller than 160cm have no chance of getting on someone's shoulders.
- The other line represents values of $x_2$ below 7. It has a smaller jump, but why is there a jump at all? Fans in this "cluster" don't get to sit on someone's shoulders when they are smaller than 160cm.

The reason why the interaction also "bleeds" into the cluster where we wouldn't expect it has to do with how SHAP values function.

## SHAP values have a "global" component

Let's consider two fans:

1. Mia, who is 159cm tall and 2 units away from the stage.
2. Tom, who is 161cm tall and standing right next to Mia, also 2 units away from the stage.

Here are the model's predictions for how much they will enjoy the concert:

```{python}
# Creating data for Mia and Tom
Xnew = pd.DataFrame({'x1': [159, 161], 'x2': [2,2]})

print("""
Mia: {mia}
Tom: {tom}
Expected: {exp}
""".format(
  mia=round(rf_model.predict(Xnew)[0], 2),
  tom=round(rf_model.predict(Xnew)[1], 2),
  exp=round(explainer.expected_value[0], 2)
))

```
They have a rather similar predicted joy for the concert, with Mia having a slightly worse prediction -- makes sense given she is slightly smaller and neither of them qualify for shoulders.

Let's examine their SHAP values.

```{python}
shap_values = explainer(Xnew)

print('Mia')
print(shap_values[0].values)

print('Tom')
print(shap_values[1].values)
```

The SHAP values for Mia and Tom differ significantly.

- Mia's slightly negative value for height is understandable, given her relative shortness compared to the majority of simulated heights.
- Mia's positive SHAP value for distance makes sense as she is quite near the front.
- Tom's SHAP values follow similar trends, which is expected since they share the same distance and similar heights.
- However, Tom's SHAP values are more pronounced.

But shouldn't Mia have a smaller SHAP value for height than Tom?
Neither of them benefits from the shoulder bonus, so Mia being smaller than Tom should mean that her SHAP value for "height" should be smaller than Tom's, right?
But surprisingly, Mia's SHAP value is influenced by the interaction term, despite her not being directly affected by the shoulder bonus!

This outcome is a result of the calculation process of SHAP values:
When computing the SHAP value for Mia's height, one of the marginal contributions involves adding her height to the empty coalition ($\emptyset$).
For this marginal contribution, we have to sample the stage distance feature.
And sometimes we sample distances > 7, which activate the shoulder bonus.
But only for Mia, not for Tom.
The shoulder bonus strongly increases concert enjoyment and, as a consequence, Mia's SHAP value for height becomes greater than Tom's.
So even though Mia is too close to the stage to get the shoulder bonus, her height's SHAP value accounts for this interaction.
This example shows that SHAP values have a global component: Interactions influence data points that are far away.

## SHAP values are different from a "what-if" analysis

This characteristic distinguishes SHAP values from a "what-if" analysis.
A what-if analysis asks: How would the prediction change if we altered Mia or Tom's height?
The likely answer is that the effect would be similar for both if we, for instance, increased their height by 10 cm.
However, SHAP values do not operate this way.
They reflect interactions with other features, even if we have to change multiple feature values of a data instance.

::: {.callout-warning}

SHAP values should not be interpreted as a what-if analysis (e.g., "what if I increased the feature by one unit").

:::

In a what-if analysis, we would only evaluate how the prediction changes when the height changes, which should be similar for both Tom and Mia.

But, what does such a contrived example have to do with real machine learning applications?
The extreme phenomenon observed here occurs subtly in real applications, albeit in more complex ways.
The interactions might be more nuanced and intricate, and there may be more features involved.
However, this global phenomenon is likely to occur.
Interactions within a machine learning model can be highly complex.
Thus, bear this limitation in mind when interpreting SHAP values.

