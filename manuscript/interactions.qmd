# Understanding Feature Interactions with SHAP {#interaction}


::: {.callout-tip appearance="simple"}

By the end of this chapter, you will be able to:

- Describe what a feature interaction is.
- Interpret interactions within the dependence plot.
- Understand SHAP interaction values.
:::

Interpreting models becomes more complex when they contain interactions.
This chapter presents a simulated example to explain how feature interactions influence SHAP values.

## A function with interactions

Imagine you're at a concert.
Everyone loves being able to actually see the band and not just the backs of other people's heads.
How much you can see depends on your height and how close you are to the stage.
We'll ignore the factor of who's in front of you for now.
We're simulating a score of how much a fan will enjoy the concert based on two features:

- $x_1$: Height in cm.
- $x_2$: Distance to the stage (from 0 to 10).
- $y$: How much the fan enjoys the concert.

We simulate the target as:

$$y = 0.1 \cdot x_1 - 1 \cdot x_2 + 10 \cdot \mathbb{1}_{x_1 < 160 \text{ and } x_2 > 7}$$

- The taller the fan, the better.
- The closer to the stage, the better.
- Both of these have linear relationships with concert enjoyment.
- Additional interaction: Small fans who are far from the stage get a "bonus": some kind soul allows them to sit on their shoulders for a better view of the concert.

::: {.callout-note}

## Feature Interaction

Two features interact when the prediction can't be explained by the sum of both feature effects.
Alternatively formulated: interaction means the effect of one feature changes depending on the value of the other feature.

Consider the price of a hotel room, which depends on room size and sea view.
Both factors individually contribute to the price: a larger room costs more, as does a room with a sea view.
However, size and view interact: for small rooms, the sea view adds less value than it does for large rooms, since small rooms are less inviting for extended stays.

:::


We'll simulate some data and train a random forest on it.

```{python}
#| output: false
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
import matplotlib.pyplot as plt
np.random.seed(42)

n = 1000
X = pd.DataFrame({
  'x1': np.random.uniform(140, 200, n),
  'x2': np.random.uniform(0, 10, n)
})

y = 0.1 * X.x1 -  1 *  X.x2 + 10 * (X.x1 < 160) * (X.x2 > 7)

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Train a random forest regression model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
```

Here's a visualization of how the features height and distance relate to the prediction:

```{python}
# Generate x1 and x2 grids
x1 = np.linspace(140, 200, 100)
x2 = np.linspace(0, 10, 100)
xx1, xx2 = np.meshgrid(x1, x2)

# Flatten the grids and predict color
Xgrid = np.column_stack((xx1.ravel(), xx2.ravel()))
color = rf_model.predict(Xgrid)

# Reshape the predicted color array
color = color.reshape(xx1.shape)

# Plot the heatmap
plt.imshow(color, extent=[x1.min(), x1.max(), x2.min(), x2.max()],
           origin='lower', cmap='coolwarm', aspect=6)
plt.xlabel('Height of fan')
plt.ylabel('Distance to stage')
plt.colorbar()
plt.show()
```

The random forest appears to approximate the function quite accurately.
A note on the scale of "concert enjoyment":
The range of the score from 8 to 18 doesn't really relate to anything, you can just see it as higher means more enjoyment.
Next, we will generate explanations for the predictions.

## Computing SHAP values

Let's calculate some SHAP values:

```{python}
import shap

explainer = shap.TreeExplainer(rf_model)
shap_values = explainer(X_test)
shap.plots.beeswarm(shap_values)
```
The summary plot reveals the following general relationships:

- The further away from the stage ($x_2$), the smaller the SHAP values.
- The taller the fan ($x_1$), the larger the SHAP values.

However, there are some exceptions: Small fans sometimes receive large SHAP values due to the interaction effect of sitting on shoulders.
To investigate further, let's examine the dependence plot, @fig-dependence-x1.

```{python}
#| label: fig-dependence-x1
#| fig-cap: "Dependence plot for x1, the height feature"
shap.plots.scatter(shap_values[:,0], color = shap_values)
```
The dependence plot colors the points by the values of feature $x_2$, as we provided the SHAP values for the color option.
By default, the points are colored by the feature with the highest approximate interaction.
Given our model only contains two features, the selection is naturally feature $x_2$.
We can make three observations:

1. A large jump at $x_1=160$ is logical because, according to our simulated data, fans taller than 160cm will not sit on someone's shoulders.
2. Ignoring the jump, there seems to be a linear upward trend, which aligns with the linear dependence of $Y$ on $x_1$. The slope reflects the coefficient in the simulated function ($\beta_1=0.1$).
3. There are two "clusters" of points: one with a small jump and one with a large jump.

The third point becomes clearer when we note the curves are colored by the feature value $x_2$. There are two "lines":

- One line represents fans who are >7 away from the stage ($x_2$). Here we see the large jump, which is expected since fans taller than 160cm have no chance of getting on someone's shoulders.
- The other line represents values of $x_2$ below 7. It has a smaller jump, but why is there a jump at all? Fans in this "cluster" don't get to sit on someone's shoulders when they are smaller than 160cm.

The reason why the interaction also "bleeds" into the cluster where we wouldn't expect it has to do with how SHAP values function.

## SHAP values have a "global" component

Let's consider two fans:

1. Mia, who is 159cm tall and 2 units away from the stage.
2. Tom, who is 161cm tall and standing right next to Mia, also 2 units away from the stage.

Here are the model's predictions for how much they will enjoy the concert:

```{python}
# Creating data for Mia and Tom
Xnew = pd.DataFrame({'x1': [159, 161], 'x2': [2,2]})

print("""
Mia: {mia}
Tom: {tom}
Expected: {exp}
""".format(
  mia=round(rf_model.predict(Xnew)[0], 2),
  tom=round(rf_model.predict(Xnew)[1], 2),
  exp=round(explainer.expected_value[0], 2)
))

```
They have a rather similar predicted joy for the concert, with Mia having a slightly worse prediction -- makes sense given she is slightly smaller and neither of them qualify for shoulders.

Let's examine their SHAP values.

```{python}
shap_values = explainer(Xnew)

print('Mia')
print(shap_values[0].values)

print('Tom')
print(shap_values[1].values)
```

The SHAP values for Mia and Tom differ significantly.

- Mia's slightly negative value for height is understandable, given her relative shortness in comparison to the majority of simulated heights.
- Mia's positive SHAP value for distance makes sense as she is quite near the front.
- Tom's SHAP values follow similar trends, which is expected since they share the same distance and similar heights.
- However, Tom's SHAP values are more pronounced.

Despite Tom being taller than Mia, his height has a more detrimental effect on the prediction.
Neither of them have the opportunity to benefit from a shoulder ride as they are already at the front.

Even though Mia is shorter, her height has less of a negative impact than Tom's.
This discrepancy seemingly cannot be explained by the interaction term, as Mia's close proximity to the band negates any potential benefit from the shoulder bonus.
But surprisingly, Mia's SHAP value is influenced by the interaction term, despite her not being directly affected by it.
This outcome is a result of the calculation process of SHAP values.

When computing the SHAP value for Mia's height, her height value is either added to an empty coalition ($\emptyset$) or to a coalition where her stage distance is already factored in ($\{x_2=2\}$).
The same process applies to Tom.
When stage distance is already present in the coalition, Mia and Tomâ€™s marginal contributions are approximately equal.

However, when we add either Mia's or Tom's height to an empty coalition, the outcomes differ.
For the empty coalition, we sample the distance to the stage from the data.
Sometimes we sample stage distances that are far away, activating the interaction term (shoulder bonus).
But this only applies to Mia, not Tom.
Since the shoulder bonus increases concert enjoyment, Mia's SHAP value for height is greater than Tom's.

Although Mia's proximity to the stage means she wouldn't benefit from the shoulder bonus, her height's SHAP value still accounts for the existence of this bonus.
This phenomenon is why I describe SHAP values as having a global component: Interactions influence data points that are far away.

This characteristic distinguishes SHAP values from a "what-if" analysis.
A what-if analysis asks: How would the prediction change if we altered Mia or Tom's height?
The likely answer is that the effect would be similar for both if we, for instance, increased their height by 10 cm.
However, SHAP values do not operate this way.
They reflect interactions with other features, even if we have to change multiple feature values of a data instance.

::: {.callout-warning}

SHAP values should not be interpreted as a what-if analysis (e.g., "what if I increased the feature by one unit").

:::

In a what-if analysis, we would only evaluate how the prediction changes when the height changes, which should be similar for both Tom and Mia.

But, what does such a contrived example have to do with real machine learning applications?
The extreme phenomenon observed here occurs subtly in real applications, albeit in more complex ways.
The interactions might be more nuanced and intricate, and there may be more features involved.
However, this global phenomenon is likely to occur.
Interactions within a machine learning model can be highly complex.
Thus, bear this limitation in mind when interpreting SHAP values.

