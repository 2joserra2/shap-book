## Understanding Feature Interactions with SHAP


This chapter is all about getting a feel for how Shapley values are to be interpreted when there are interactions.
We started with a linear regression model, extended it with non-linear functions but still let it be an additive model, and now it's time to think about feature interactions.
Realistically, most models and problems include interactions between features, and Shapley values should also work for that case.

::: {.callout-note}

## Feature Interaction

Two features interact when the prediction can't be explained by the sum of the feature effects.

:::

## A function with interactions

A simple example of a model with interactions:

$$y = \mathbb{1}_{x_1 \geq -5 \text{ or } x_2 \geq -0.5}(x_1 + x_2) + 20 \cdot \mathbb{1}_{x_1 < -0.5 \text{ and } x_2 < -5}$$

The formula takes two input values, x1 and x2, and calculates an output value, y.
If either x1 is at least -5 or x2 is at least -0.5, then y is the sum of x1 and x2.
If both x1 and x2 are less than -0.5 and -5, respectively, then y is set to a fixed value of 20.

We can't explain the prediction as a sum of only two feature effects, because we have this term where both features are "entangled".

What does this mean for Shapley values?

Let's go back to the fundamentals:
Shapley values fairly attribute the prediction to the individual features.
Let's walk through the terms in the example above:
$x_1$ can be attributed to feature 1, of course, same for $x_2$ to feature 2.
And the effect of the product needs to be fairly split between the features.

Let's first plot the "prediction"-function in Python, because visualize it's simple to understand.

```{python}
#| output: False
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
import matplotlib.pyplot as plt

n = 1000
df = pd.DataFrame({
  'x1': np.random.uniform(-1, 1, n),
  'x2': np.random.uniform(-10, 10, n)
})

df['y'] = ((df.x1 >= -5) | (df.x2 >= -.5)) * (df.x1 + df.x2) + \
          20 * (df.x1 < -.5) * (df.x2 < -5)

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(
    df[['x1', 'x2']], df.y, test_size=0.2, random_state=42
)

# Train a random forest regression model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
```

This is what the function looks like:

```{python}
# Generate x1 and x2 grids
x1 = np.linspace(-1, 1, 100)
x2 = np.linspace(-10, 10, 100)
xx1, xx2 = np.meshgrid(x1, x2)

# Flatten the grids and predict color
X = np.column_stack((xx1.ravel(), xx2.ravel()))
color = rf_model.predict(X)

# Reshape the predicted color array
color = color.reshape(xx1.shape)

# Plot the heatmap
plt.imshow(color, extent=[x1.min(), x1.max(), x2.min(), x2.max()],
           origin='lower', cmap='coolwarm', aspect=0.1)
plt.xlabel('x1')
plt.ylabel('x2')
plt.colorbar()
plt.show()
```

## Computing Shapley values

Alright, for this function we will now get us some Shapley values.


```{python}
import shap
# Make predictions on the test set
predictions = rf_model.predict(X_test)

# Explain the predictions with shapley values
explainer = shap.TreeExplainer(rf_model)
shap_values = explainer.shap_values(X_test)

# Visualize the shapley values
shap.summary_plot(shap_values, X_test)
```

This requires a deeper investigation to understand what's going on, because the Shapley values show an unusual pattern.

Let's examine the dependence plot of $x_1$.
The dependence plot shows how the SHAP values depend on the feature, in our case $x_1$.

```{python}
shap.dependence_plot("x1", shap_values, X_test)
```

The dependence plot does something very interesting automatically.
It detects interactions with other features.
Since we only have $x_2$ as another feature, it shows interactions with $x_2$.
These are indicated by the color.

But first, let's see what's going on.
We can make 3 observations:

1. Something happens at $x_1=-0.5$: a large jump down in prediction.
2. Ignoring the jump, there seems to be a linear upward trend.
3. There are two "clusters" of points: one with a small jump and one with a large jump.

Part 3 becomes clearer because the curves are colored by the feature value $x_2$.

Let's explain all these occurrences:

- The linear trend aligns with the coefficient of how we simulated the data.
- The jump at -0.5 also makes sense.
- It is logical to see two curves:
  - One curve is for values of $x_2$ below -5.
  - The other curve is for values of $x_2$ above -5.

It makes sense to observe this significant jump for the $x_2 < 5$ cluster because, in this case, the value of $x_1$ determines whether the data point lies in the special rectangle, greatly increasing the predictions.

However, a big question remains: Why do we also see the jump when $x_2 > -5$?

## Shapley values have a "global" component

To answer this, we must return to the basics of Shapley values.
The Shapley value for $x_1$ is the marginal contribution of adding $x_1$ to a coalition.
In our case, that can be an empty coalition or a coalition where $x_2$ is already present.

Keep in mind that we are currently examining the Shapley values for a subset of the data where $x_2 > -5$.

So, when we compute the Shapley value for $x_1$, it is a mix of two coalitions:

1. Adding to $\emptyset$: In this case, we sample $x_2$ and also $(x_1, x_2)$.
2. Adding to $x_2$: In this case, we sample $x_2$ and don't sample anything for the full coalition.


In the first case, where we start with an empty coalition and sample both $x_1$ and $x_2$, we also sometimes sample from the special rectangle.
And then, it makes a difference whether the value of $x_1$ for the current instances for which we compute Shapley values is below -0.5 or not.
That's why we see the jump as well for this subset when $x_2 > -5$.

Tricky, right?

Where does this leave us when we apply Shapley values for model interpretation?
Let's break it down:

- A data point with $(x_1 = -0.6, x_2 = +8)$ has a Shapley value of $\phi_1 \approx 1$.
- A data point with $(x_1 = -0.4, x_2 = +8)$ has a Shapley value of $\phi_1 \approx -1.5$.

One might argue that both data points should have almost the same Shapley value, and if anything, data point 2 should have a larger Shapley value since increasing $x_1$ increases the prediction.
However, we observe a decrease instead.
Comparing these two data points, only $x_1$ has changed.
Since we know $f(x)$, we understand that no sudden change occurs in between, because $x_2$ is too large for that, and we are not in the special rectangle area.

```{python}
Xnew = pd.DataFrame({'X1': [-0.6, -0.4], 'X2': [8,8]})
shap_values = explainer.shap_values(Xnew)
print(rf_model.predict(Xnew.values))
print(shap_values)
```

Although the predictions are almost identical, the Shapley values in one case are both positive, while in the second case, the $x_1$ Shapley value is negative, and the $x_2$ value is more positive than in the other data point.

Now, let's examine the background data: It is uniformly distributed across -10 to 10 for $x_2$ and -1 to 1 for $x_1$.
One-eighth of the data is in the special rectangle with a very high predicted value, while for the remaining 7/8ths of the data, we have a simple linear predictor.

Upon further consideration, this actually makes sense:

- Point 1 $(x_1=-0.6, x_2=8)$: This point already has "one foot" in the special rectangle. From that standpoint, what is the role of $x_1$? We have to think about the background data as well. For 1/4th of the background data, adding the player $x_1=-0.6$ would mean stepping fully into the rectangle and getting that juicy +20 in the prediction. In that regard, $x_1$ plays a positive role. In fact, all data points with $x_1 < -0.5$ (including a buffer zone since the random forest does it a bit more gradually) have a positive Shapley value $\phi_1$!
- Point 2 with $(x_1=-0.4, x_2=8)$, on the other hand, has an $x_1$ value that is the worst of both worlds (if you see a low prediction as bad). It doesn't help **any** data point from the background data to enter the magical rectangle. But it's also on the lower end in terms of the linear factor. So, it makes sense to get a negative value.

But what about the difference between the Shapley values $\phi_2$ for $x_2$?
We observe that both are positive, but the second is larger.
It makes sense that they are positive because for all data points with $x_2 \in [-5, 7.99]$, which means an improvement for 60% of the data points, and adding to that all the ones $x_2 < -5$ but $x_1 > -0.5$, which are 0.25 * 0.75 = 0.1875, another 18.75% and in total 78.75% of the data.
Of course, it's not the percentage that counts, but the expected value (value times probability) but that's to get a feel for it.
But why is $\phi_2$ larger for the second data point, even though $x_2=8$ for both instances?
Because adding $x_2=8$ to a data point with $x_1=-0.6$ is different than adding it to a data point $x_1=-0.4$.

Why?

In both cases, adding to an empty coalition is the same.
But the difference is adding to the $x_1$, which differs between the two data points.
Adding to an $x_1=-0.6$ "robs" that data point of the chance of being placed in the special rectangle.
But adding it to $x_1=0.4$ is only positive.
Again, look at the background data: on average, adding $x_2=8$ is worse if we already know that the data point is with one step in the special rectangle because some of the background data would have made it into the rectangle.
In comparison, any coalition with $x_1=-0.4$ means no change of special rectangular time anyway.

:::{.callout-warning}

Shapley values mah not be interpreted as counterfactuals or what-if analyses!

:::

And you shouldn't interpret them as such.
Because if you expect both of the above data points to have roughly the same Shapley values, then it's closer to a what-if analysis where you keep one part fixed (here $x_2$) and only change the feature we are looking at ($x_1$).

However, Shapley values always operate on a background dataset and incorporate a more global view of the model.
For our example, this means that even though we are operating on a data point outside the rectangle, the Shapley value includes the marginal contribution of coalitions that involve jumping into the rectangle.

In that sense, for $x_2 > -5$, the Shapley values as a function of $x_1$ are a mix of:

- Locally changing $x_1$.
- Globally changing everything.

Take-home messages:

- Shapley values are not what-if analyses.
- Shapley values are not local derivatives.
- Shapley values combine global and local properties of the prediction function.
- Shapley values' interpretation depends on the background data.
- A Shapley value must be interpreted in the context of background data and the full prediction function.
- A Shapley value is also influenced by interactions with other features, even if in a "what-if" scenario (just changing the feature in question), that interaction isn't "triggered".

By the way, the exact same thing happens to $x_2$, with the difference that the linear trend is much steeper.

But, what does such an artificial example have to do with real ML applications?

The phenomenon that occurs here in the extreme plays out in more complex but subtle ways in real applications.

Only potentially with more features, and the interactions won't be as blunt but might be more smooth and intricate.
