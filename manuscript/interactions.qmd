# Understanding Feature Interactions with SHAP {#interaction}

Interpretation gets tricky when the model has interactions.
This chapter goes deep into a simulated example so we can get to better understand how interactions are distributed among the features.

::: {.callout-note}

## Feature Interaction

Two features interact when the prediction can't be explained by the sum of both feature effects.
Alternatively formulated: interaction means the effect of one feature changes depending on the level of the other feature.

For example, let's consider the price of a hotel room, based on room size and sea view.
Both individually add to the price: the larger the room, the more expensive and you pay extra for a sea view.
In addition, size and view interact: For small rooms, the sea view adds less to the price compared to large rooms, since the small rooms aren't inviting for staying a long time in them.

We have to distinguish interactions in reality and in the model.
Even if an interaction exists in your data, if you use a purely linear model the interaction won't show in the model.

:::

## A function with interactions

You are at a concert and you love when you can actually see the band.
Now, it depends on your size how much you'll be able to actually see.
And it depends on who is in front of you, but we'll ignore that for now.
We simulate a score of much a fan will enjoy the concert based on two features:

- $x_1$: Height in cm
- $x_2$: Distance to the stage (from 0 to 10)
- $y$: How much the fan enjoys the concert

We simulate the target as:

$$y = 0.1 \cdot x_1 - 1 \cdot x_2 + 10 \cdot \mathbb{1}_{x_1 < 160 \text{ and } x_2 < 7}$$

- The taller the fan, the better
- The smaller the distance to the stage, the better
- These are both linear relations to the concert joy
- Additional interaction: Fans that are small and far away from the stage, can sit on the shoulders of their friends, which adds to the concert experience.

We simulate some this data and fit a random forest on the relation.
Then we also visualize the modeled relation which makes it easier to understand.

```{python}
#| output: false
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
import matplotlib.pyplot as plt

n = 1000
df = pd.DataFrame({
  'x1': np.random.uniform(140, 200, n),
  'x2': np.random.uniform(0, 10, n)
})

df['y'] = 0.1 * df.x1 -  1 *  df.x2 + 10 * (df.x1 < 160) * (df.x2 > 7)

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(
    df[['x1', 'x2']], df.y, test_size=0.2, random_state=42
)

# Train a random forest regression model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
```

Let's plot what the prediction looks like, since this gives us an idea.

```{python}
# Generate x1 and x2 grids
x1 = np.linspace(140, 200, 100)
x2 = np.linspace(0, 10, 100)
xx1, xx2 = np.meshgrid(x1, x2)

# Flatten the grids and predict color
X = np.column_stack((xx1.ravel(), xx2.ravel()))
color = rf_model.predict(X)

# Reshape the predicted color array
color = color.reshape(xx1.shape)

# Plot the heatmap
plt.imshow(color, extent=[x1.min(), x1.max(), x2.min(), x2.max()],
           origin='lower', cmap='coolwarm', aspect=6)
plt.xlabel('Height of fan')
plt.ylabel('Distance to stage')
plt.colorbar()
plt.show()
```

Seems like the random forest approximates the function quite well.

## Computing SHAP values

Alright, for this random forest, we will now get us some SHAP values:


```{python}
import shap
# Make predictions on the test set
predictions = rf_model.predict(X_test)

# Explain the predictions with shapley values
explainer = shap.TreeExplainer(rf_model)
shap_values = explainer.shap_values(X_test)

# Visualize the shapley values
shap.summary_plot(shap_values, X_test)
```

The summary plot already shows us the general relationships:

- The further away from the stage ($x_2$), the smaller the SHAP values.
- The taller the fan ($x_1$), the smaller the SHAP values.

But there are also some breaks in the patterns: There are small fans that receive a large SHAP value. This is due to the interaction (sitting on shoulders).
To investigate deeper, let's look into the dependence plot.

```python
shap.dependence_plot("x1", shap_values, X_test)
```

The dependence plot automatically colors the points by values of feature $x_2$.
By default, the points are colored by the feature with the largest approximate interaction.
Since our model only contains two features, the selection is, of course, feature $x_2$.
We can make 3 observations:

1. There is a large jump at $x_1=160$. Makes sense because we know that above 160cm, there is no chance that you will get on someone's shoulders (at least in our simulated data).
2. Ignoring the jump, there seems to be a linear upward trend. Also makes sense since $Y$ has a linear dependence on $x_1$, and the slope is the coefficient in the simulated function ($\beta_1=0.1$).
3. There are two "clusters" of points: one with a small jump and one with a large jump. This one is a bit tougher to explain.

Part 3 becomes clearer because the curves are colored by the feature value $x_2$. There are two "lines":

- One line is for people that are >7 away from the stage ($x_2$). Here we see the large jump, which makes sense since at >160cm, these fans have no chance of getting on someone's shoulders.
- The other line is for values of $x_2$ below 7. It has a smaller jump. But why does it have a jump at all? People in this "cluster" don't get to sit on someone's shoulders when below <160cm.

The reason why the interaction also "bleeds" into the cluster where we wouldn't expect it has to do with how SHAP values work.

## SHAP values have a "global" component

Let's investigate the two fans:

1. Mia, 159cm tall with a distance of 4 to the stage.
2. Tom, 161cm tall standing right next to Mia with a distance of 4 to the stage.

The model predicts the following for how much they will enjoy the concert:

```{python}
# Creating data for Mia and Tom
Xnew = pd.DataFrame({'X1': [159, 161], 'X2': [2,2]})
print(rf_model.predict(Xnew.values))
```
They have a rather similar predicted joy for the concert, with Mia having a slightly worse prediction -- makes sense given she is slightly smaller and neither of them qualify for shoulders.

Let's have a look at their SHAP values.

```{python}
shap_values = explainer.shap_values(Xnew)
print(shap_values)
```

The SHAP values differ quite substantially.

- Mia has a slightly negative value for the height. Makes sense because she is rather small, compared to the majority of the simulated heights.
- Mia has a positive SHAP value for the distance, which also makes sense because she stands rather close to the front.
- For Tom, the directions are similar, which makes sense since they have the same distance and are almost the same height.
- But Tom's SHAP values are more extreme.

So even though Tom is actually taller than Mia, his height has a more negative impact on the prediction.
Neither have a chance to get on someone's shoulders since they are both already in the front.

Even though Mia is smaller, her height has less of a negative impact compared to Tom.
And we also can't explain it with the interaction since Mia is too close to the band that she doesn't profit from the shoulder bonus.

But the surprising answer is that Mia's SHAP value actually is influenced by the interaction, even though she isn't directly affected by it. And this is because of how Shapley values are computed.

When we compute the SHAP value for Mia's height, her height value is either added to an empty coalition ($\emptyset$) or to a coalition where her stage distance is already present ($\{x_2=2\}$).
The same happens for Tom.
Adding to a coalition where stage distance is already present returns roughly the same marginal contribution for both Mia and Tom.

But the results are different when we add either Mia's or Tom's height to an empty coalition.
Because for the empty coalition, we sample the distance to the stage from the data.

So sometimes we sample stage distances that are far away and then the interaction comes into play (shoulder bonus).
But only for Mia and not for Tom.
And since the shoulder bonus increases the concert joy, the SHAP value for Mia's height is increased compared to Tom's.

So even though Mia's closeness to the stage means that she wouldn't profit from the shoulder bonus, the SHAP value for her height still reflects the existence of that bonus.

That's why I would refer to SHAP values as having a global component: The interactions also influence data points further away.

Also, that makes SHAP values different from a "what-if" analysis.
The what-if analysis of the height is:
How would the prediction change if we change the height of Mia or Tom?
The answer would be that the effect would be quite similar for both Mia and Tom if we, for example, increase their height by 10cm.
But SHAP values don't work that way.
The values reflect interactions with other features, even if we have to change multiple feature values of a data instance.

Because in a what-if analysis, we would only evaluate how the prediction changes when the height changes, which should be rather similar for Tom and Mia.

But, what does such an artificial example have to do with real ML applications?
The phenomenon that occurs here in the extreme plays out in more complex but subtle ways in real applications.
Only potentially with more features, and the interactions won't be as blunt but might be more smooth and intricate.


::: {.callout-tip}

## Takeaways

- SHAP values are not to be interpreted as a what-if analysis.
- Feature interactions are split across the SHAP values.
- A SHAP value must be interpreted in the context of background data and the full prediction function.

:::



