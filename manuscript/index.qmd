# Preface

When I wrote the book "Interpretable Machine Learning," I did not include SHAP.
In a survey on Twitter, I asked which method people used most often for interpreting their machine learning models.
I provided options such as LIME, permutation feature importance, and "Other," but I did not include SHAP or Shapley values.

To my surprise, most respondents chose "Other," and many comments pointed out the omission of SHAP.
At the time, I was aware of SHAP, but I underestimated its popularity in machine learning explainability and beyond. 

This popularity also had a negative side.
My research in interpretable machine learning, during my PhD, primarily focused on partial dependence plots and permutation feature importance.
On more than one occasion, when we submitted a paper to a conference, we received feedback suggesting that we should have worked on SHAP or LIME instead.
This advice was misguided, as that's not how science works, but it does speak to SHAP's popularity.

SHAP has also received its fair share of criticism, with some claiming that it is expensive to compute, difficult to interpret, and overhyped.
I agree with some of this criticism.
However, there are many great aspects of SHAP as well, such as its flexibility to work with any model, modularity in constructing global interpretations, and a vast ecosystem of SHAP adaptations.

Because of this, I have a love-hate relationship with SHAP, which makes me the perfect person to write about it.
I don't intend to overhype it, but I believe it is a valuable tool worth learning.
Investing your time in understanding SHAP will pay off.
