# Preface

When I wrote the book "Interpretable Machine Learning", SHAP was not part of it.
In a survey on Twitter I asked which method they used most often for interpreting their machine learning models.
I put a few options like LIME and permutation feature importance and "Other".
No option for SHAP or Shapley values.
The next thing that happened:
Most voted "Other" and I got lots of comments that the option "SHAP" was missing.
At the time I knew of SHAP, but I underestimated its popularity for machine learning explainability and beyond.

This popularity also had a negative side to it.
My research in interpretable machine learning, for my PhD, was mostly concerned with the partial dependence plot and permutation feature importance.
More than once when we submitted a paper to a conference, we got the feedback that we should have rather worked on SHAP or LIME.
Really bad advice, because that's not how science works, but it speaks for the popularity of SHAP. 

SHAP also receives a lot of criticism.
Expensive to compute, difficult to interpret, overhyped.
Some of the criticism I agree with.
But on the other side, there are lots of great things about SHAP, like the flexibility to use it with any model, the modularity to construct global interpretations, the big ecosystem of SHAP adaptions.

So I have this love-hate-relationship with shap.
And that makes me just the right person to write about it!
I don't feel like overhyping it, but I still think it's valuable enough tool.
And I think it's a good investment for you to learn about it.

