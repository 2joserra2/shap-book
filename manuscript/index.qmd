# Preface

In my first book, "Interpretable Machine Learning," I overlooked the inclusion of SHAP.
I conducted a Twitter survey to determine the most frequently used methods for interpreting machine learning models.
Options included LIME, permutation feature importance, partial dependence plots, and "Other."
SHAP was not an option.

To my surprise, the majority of respondents selected "Other," with many comments highlighting the absence of SHAP.
Although I was aware of SHAP at that time, I underestimated its popularity in machine learning explainability.

This popularity was a double-edged sword.
My PhD research on interpretable machine learning was centered around partial dependence plots and permutation feature importance.
On multiple occasions, when submitting a paper to a conference, we were advised to focus on SHAP or LIME instead.
This advice was misguided because we should make progress for all interpretation methods, not just SHAP, but it underscores the popularity of SHAP.

SHAP has been subjected to its fair share of criticism: it's costly to compute, challenging to interpret, and overhyped.
I agree with some of these criticisms.
In the realm of interpretable machine learning, there's no perfect method; we must learn to work within constraints, which this book also addresses.
However, SHAP excels in many areas: it can work with any model, it's modular in building global interpretations, and it has a vast ecosystem of SHAP adaptations.

As you can see, my relationship with SHAP is a mix of admiration and frustration -- perhaps a balanced standpoint for writing about SHAP.
I don't intend to overhype it, but I believe it's a beneficial tool worth understanding.

