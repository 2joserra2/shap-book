# Preface

When I wrote my first book "Interpretable Machine Learning," I did not include SHAP.
In a survey on Twitter, I asked which method people used most often for interpreting their machine learning models.
I provided options such as LIME, permutation feature importance, and "Other," but I didn't add SHAP as an option.

To my surprise, most respondents chose "Other," and many comments pointed out the omission of SHAP.
At the time, I was aware of SHAP, but I underestimated its popularity in machine learning explainability and beyond. 

This popularity also had a negative side.
My PhD research on interpretable machine learning focused on partial dependence plots and permutation feature importance.
On more than one occasion, when we submitted a paper to a conference, we received feedback suggesting that we should have worked on SHAP or LIME instead.
This advice was misguided, as that's not how science works, but it does speak to SHAP's popularity.

SHAP has also received its fair share of criticism: expensive to compute, difficult to interpret, and overhyped.
I agree with some of this criticism.
In the world of interpretable machine learning, there is no perfect method, we have to learn to work with the limitations, which are covered in this book as well.
SHAP also does many things well: flexibility to work with any model, modularity in constructing global interpretations, and a vast ecosystem of SHAP adaptations.

As you can see, I have a love-hate relationship with SHAP -- which is maybe a healthy position for writing about SHAP.
I don't intend to overhype it, but I believe it is a valuable tool worth learning.
Investing your time in understanding SHAP will pay off.
