## Image classification


CONTINUE HERE:

- REplace pytorch with tensorflow
- Maybe have some Pytorch code as well in the end?



All of the examples before have been tabular data.
Now it's time to try a different type of data: image data.

So the general setup here is:

- Input is image data
- Output is a score, which can also be multi-dimensional as in multi-class classification

Image classification is a common task and it's commonly solved with deep learning.
Give an image to the model, get a class back, usually based on what's visible on the image.

We aren't going to train our own image classifier, but instead will load a ResNet model (TODO:CITE) which was trained on Imagenet data.

The ImageNet task is a large-scale image classification challenge that involves recognizing and categorizing objects within digital images. The challenge uses a dataset of over 1 million images, each of which belongs to one of 1000 different object categories. The task is to develop a machine learning model that can accurately classify each image into its correct category.

The ImageNet challenge has been an important driver of progress in the field of computer vision and deep learning, and has led to the development of new and more accurate machine learning models. The challenge has also spurred research into related computer vision tasks such as object detection and image segmentation.

```{python}
#| output: false
# The code was adapted from the notebook [Explain MobilenetV2 using the Partition explainer (PyTorch)](https://github.com/slundberg/shap/blob/master/notebooks/image_examples/image_classification/Explain%20MobilenetV2%20using%20the%20Partition%20explainer%20(PyTorch).ipynb) from the shap repository.
import torch
import torchvision
import numpy as np
import torchvision.models as models
import torchvision.transforms as transforms

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# Load the pre-trained ResNet50 model from the PyTorch model zoo
model = models.resnet50(weights="ResNet50_Weights.IMAGENET1K_V1")

# Set the model to evaluation mode
model.eval()
```


Let's see if the model works by trying to classify this corgi:

![This is a cute Corgi](images/corgi.jpg)

Look how happy this one is.
Pure joy.
Not aware of the danger on the tracks behind this true friend.

```{python}
from PIL import Image

# Load an example image
img = Image.open("corgi.jpg")

# Define a transform to resize the image and convert it to a tensor
transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
])

# Apply the transform to the image
img_tensor = transform(img)

# Add a batch dimension to the tensor
img_tensor = img_tensor.unsqueeze(0)

# Pass the tensor through the model to get the predicted classes
output = model(img_tensor)
```

The `output` of is a pytorch tensor of shape 1x1000, which are the probabilities per output class.
Let's see which class the right one is.
For that we first need to import the class names of Imagenet and match them with ther probabilities.


```{python}
import json
import os
import urllib.request

# Path to the JSON file on disk (change this to the desired location)
json_file_path = 'imagenet_class_index.json'

# Check if the JSON file exists on disk
if os.path.exists(json_file_path):
    # Read the class names from the JSON file
    with open(json_file_path) as file:
        class_names = [v[1] for v in json.load(file).values()]
else:
    # Download the JSON file from the given URL and store it on disk
    url = 'https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json'
    with urllib.request.urlopen(url) as response:
        json_data = response.read().decode()
    with open(json_file_path, 'w') as file:
        file.write(json_data)
    # Extract the class names from the JSON data
    class_names = [v[1] for v in json.loads(json_data).values()]
```
Now we only have to combine the labels with the network outputs and return the most likely classes.

```{python}
def get_top_classes(probs, class_names, num_classes=1):
    # Get the indices to sort the probabilities in descending order
    sorted_indices = np.argsort(probs.detach().numpy()[0])[::-1]
    # Get the top num_classes class names
    top_classes = [class_names[i] for i in sorted_indices[:num_classes]]
    return top_classes
get_top_classes(output, class_names, 5)
```

Seems like the network "thinks" this image shows a whippet.
But what's the explanation for this classification?

To answer this, we will use Shapley values and explain the image classification.

## SHAP for image classification

To create a shap explanation, we need three things:

- the prediction function
- the masker, which is also a function
- the class names

Shap and pytorch need slightly different shapes for the images, so we define here two functions to convert them from one shape to the other, and vice versa.


```{python}
# Normalization used in RESNET50
mean = [0.485, 0.456, 0.406]
std = [0.229, 0.224, 0.225]

def nhwc_to_nchw(x: torch.Tensor) -> torch.Tensor:
    if x.dim() == 4:
        x = x if x.shape[1] == 3 else x.permute(0, 3, 1, 2)
    elif x.dim() == 3:
        x = x if x.shape[0] == 3 else x.permute(2, 0, 1)
    return x

def nchw_to_nhwc(x: torch.Tensor) -> torch.Tensor:
    if x.dim() == 4:
        x = x if x.shape[3] == 3 else x.permute(0, 2, 3, 1)
    elif x.dim() == 3:
        x = x if x.shape[2] == 3 else x.permute(1, 2, 0)
    return x

```


We also need to define a predict function so that the images that are produced by the shap masker can be converted to be fed to the RESNET18 model.

```{python}
import shap
import json


def predict(img: np.ndarray) -> torch.Tensor:
    img = nhwc_to_nchw(torch.Tensor(img))
    img = img.to(device)
    img = torch.Tensor(img)
    output = model(img)
    return output

# shap needs (3, 224, 224)
img_tensor2 = img_tensor.permute(0, 2, 3, 1)


```

Then we are finally ready to estimate the Shapley values.


```{python}

# Number of top classes for which to compute the SHAP explanations
topk = 5

# The masker blurs out parts of the image 
masker = shap.maskers.Image(
  "blur(128,128)", shape = img_tensor2[0].shape
)

explainer = shap.Explainer(
  predict, masker, output_names=class_names
)

shap_values = explainer(
  img_tensor2, max_evals=100,
  outputs=shap.Explanation.argsort.flip[:topk]
)
```


```{python}
transform= [
    torchvision.transforms.Lambda(nhwc_to_nchw),
    #torchvision.transforms.Lambda(lambda x: x*(1/255)),
    #torchvision.transforms.Normalize(mean=mean, std=std),
    torchvision.transforms.Lambda(nchw_to_nhwc),
]

inv_transform= [
    torchvision.transforms.Lambda(nhwc_to_nchw),
    torchvision.transforms.Normalize(
        mean = (-1 * np.array(mean) / np.array(std)).tolist(),
        std = (1 / np.array(std)).tolist()
    ),
    torchvision.transforms.Lambda(nchw_to_nhwc),
]

transform = torchvision.transforms.Compose(transform)
inv_transform = torchvision.transforms.Compose(inv_transform)



shap_values.data = inv_transform(shap_values.data).cpu().numpy()[0]
shap_values.values = [val for val in np.moveaxis(shap_values.values[0],-1, 0)]
```


```{python}
shap.image_plot(shap_values=shap_values.values,
                pixel_values=shap_values.data,
                labels=shap_values.output_names,
                true_labels=[class_names[output.argmax()]])
```


## Effect of Different Inpainting Methods

- TODO: make a list of the masks
- iterate through the lists
- show top 5 classes
- TODO: look for 5 papers

```{python}
# define a masker that is used to mask out partitions of the input image. 
mask_names = ["inpaint_telea", "inpaint_ns", "blur(128, 128)", "blur(16, 16)"]

masks = [shap.maskers.Image(m, shape = img_tensor2[0].shape) for m in mask_names]
```

```{python}
topk = 2

for mask in masks:
    # create an explainer with model and image masker 
    explainer = shap.Explainer(predict, mask, output_names=class_names)
    # here we explain two images using 500 evaluations of the underlying model to estimate the SHAP values
    shap_values = explainer(img_tensor2, max_evals=100, batch_size=50, outputs=shap.Explanation.argsort.flip[:topk])
    shap_values.data = inv_transform(shap_values.data).cpu().numpy()[0]
    shap_values.values = [val for val in np.moveaxis(shap_values.values[0],-1, 0)]
    shap.image_plot(shap_values=shap_values.values,
            pixel_values=shap_values.data,
            labels=shap_values.output_names,
            true_labels=[class_names[output.argmax()]])
```


We have two options of what we want to see as input features:

- individual pixels
- larger collections of pixels

The network gets as input the individual pixels, but that doesn't mean we have to use the same granularity for the explanations.

The second choice is about how we mask absent (sets of) pixels:

- We could replace them from background data
- Or we could replace them with some reference, which could be blurring them or replacing them with grey pixels (or some other "neutral" color)



# DeepExplainer

We can do all the same but on a pixel level. Depending on your application, this can make sense. But it's expensive.

It makes sense if you need a really fine-grained explanation.

Instead of an "Explainer" object, we create a "DeepExplainer". This also means we need no masker, but we will again work with the background data.

But here's the thing: Which data to use.
Since I haven't trained the model myself, I have to think hard on what the background data is.
Usually it would be from the same distribution as my usual data. 

But that's super slow, I see no point in using it.


## The Correlation problem for images


We have talked about the correlation problem in the [correlation chapter](#correlation).
And there it was for the background data.
But does the same occur if we have image data?

There is a similar problem, but there it makes more sense to speak about extrapolation: Leaving the distribution of training data by creating new images.
And that's depending on the masker.
The masker creates new images, which might be far removed from the input data.

TODO: check out research on maskers and their effect on explanation

### Problems and TODOs:
    
- When changing the masker, the topk classes change, but they shouldn't?
- 


