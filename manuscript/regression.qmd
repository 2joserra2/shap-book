# Regression

In this chapter, we will revisit the wine data and fit a tree-based model.
This means the model potentially includes numerous interactions and non-linear functions, resulting in a more complex interpretation compared to previous chapters.
But, on the good side, we can utilize the fast `shap.TreeExplainer`.

## Fitting the LightGBM model

If you haven't installed LightGBM yet, install it as follows:

```{python}
#| eval: false
pip install lightgbm
```

LightGBM is a gradient boosting framework based on decision trees.
It's fast, efficient, and generally performs well.
Using a histogram-based approach to represent feature values, it excels with large datasets.
But enough advertisement, let's train the LightGBM model.

```{python}
import pandas as pd
import lightgbm as lgb
import shap
from sklearn.model_selection import train_test_split

# Load the dataset
wine = pd.read_csv("wine.csv")

y = wine["quality"]
X = wine.drop(columns=["quality"])

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
  X, y, test_size=0.2, random_state=42
)

# Create LightGBM dataset
train_data = lgb.Dataset(X_train, label=y_train)
test_data = lgb.Dataset(X_test, label=y_test)

# Train the model
model = lgb.LGBMRegressor()
model.fit(X_train, y_train)
```

Again, we evaluate the performance of our solution, hoping for better results than with just the GAM:

```{python}
from sklearn.metrics import mean_absolute_error

# Make predictions on the test data
y_pred = model.predict(X_test)
# Compute the MAE
mae = mean_absolute_error(y_test, y_pred)

print('MAE:', round(mae, 2))

```

Once again, this model performs slightly better than the GAM, justifying the inclusion of additional interactions.
Notably, the GAM was also tree-based but without modeling interactions.


## Computing SHAP values

Now let's interpret the model:

```{python}
import shap
# Compute the SHAP values for the sample
explainer = shap.Explainer(model)
shap_values = explainer(X_test)
```

For this model, `shap` can automatically extract the data for the background data.

Let's again have a look at this wine that we already examined in the [Linear Chapter](#linear) and the [Additive Chapter](#additive).

```{python}
# Visualize the SHAP values
shap.plots.waterfall(shap_values[0])
```

The results differ from both the linear and the GAM models, but the interpretation process remains similar.
However there is on big difference:
The LightGBM model contains interactions between the features.
But since there's only one SHAP value per feature value (and not one for every interaction), it means that interactions between feature values are split between the SHAP values.

## Global model interpretation

Global SHAP plots enable us to see how features impact the overall predictions in the model.
Let's examine the summary plot:

```{python}
shap.summary_plot(shap_values, X_test)
```

Observations:

- Alcohol and volatile acidity were the most important features.
- Many features, such as alcohol and volatile acidity, show a (somewhat) monotonic relationship with the target.
- Factors that influenced the predicted quality of some wines in the most extreme ways included:
  - High alcohol levels for higher predicted quality.
  - Low levels of free sulfur dioxide for lower quality.

The interactions can be analyzed in the global plots such as the dependence plots.
Here is the dependence plot for the alcohol feature:

```{python}
shap.dependence_plot(
  "alcohol", shap_values.values, X_test,
  feature_names=wine.columns
)
```

The `shap` package has an automatic interaction detection.
In this case, `shape` selected `volatile acidity` as a feature that highly interacts with `alcohol` and colored the SHAP values according to this feature.
The feature `volatile acidity` was picked because it had the highest estimated interaction with `alcohol`.

::: {.callout-note}

## Automatic Interaction Detection

By default, the `shap` dependence plot picks the feature that has the strongest interaction with the feature of interest.
The dependence plot function calls the  `approximate_interactions` function which measures interaction between features through correlation of SHAP values, with stronger correlation indicating stronger interaction.
It then returns a ranking of features based on their interaction strength with a selected feature.
You can also pick a feature by hand.

:::


Some key observations:

- Generally, a higher alcohol level corresponds with greater corresponding SHAP value
- Examining cases with low volatile acidity reveals an interesting interaction with wines that have a low alcohol level. While wines with low alcohol (around 9%) typically have a negative SHAP value, low volatile acidity seems to have a slightly positive impact.
- When alcohol levels are high (11 - 14%), having a higher volatile acidity appears to be slightly beneficial, though the effect is quite subtle. Note that this effect is not necessarily causal and ignores dependencies between the features.

::: {.callout-note}
General advice on reading the interaction part of the dependence plot:

- Choose one of the two variables.
- For this variable, select two ranges or categories.
- Compare the SHAP values within these ranges.
- Observe if any differences are associated with changes in the other feature.

:::

Now, let's examine the dependence plot for residual sugar.
Residual sugar indicates the remaining sugar in the wine, with higher amounts making it sweeter.

```{python}
shap.dependence_plot(
  'residual sugar', shap_values.values, X_test,
  feature_names=wine.columns
)
```


Observations:

- Higher residual sugar corresponds to higher SHAP values.
- The `shap` package identifies the highest interaction with density.
- Density and residual sugar are correlated (not independent); for instance, high sugar levels result in high density values. Physics.
- Comparing curves for low density (around 0.99) and high density (around 0.998):
  - When density is low, increasing residual sugar values are associated with larger SHAP values.
  - If density is high, different residual sugar values doesn't show much changes in SHAP values.


## Analyzing interactions in more detail

Let's use the SHAP interaction values to drill a bit deeper into the interaction between  alcohol and volatile acidity.

```{python}
shap_interaction_values = explainer.shap_interaction_values(X_test)
```

And then we plot the dependence plot:

```{python}
#| label: fig-alcohol-acidity
#| fig-cap: SHAP interaction dependence plot for x1 and x2
shap.dependence_plot(("alcohol", "volatile acidity"), shap_interaction_values, X_test)
```

We can also flip the features.
Again, it's the same SHAP interaction value $\phi_{alcohol, vol.ac.}$, but showing volatile acidity on the x-axis and alcohol as color:

This actually gives us a much clearer picture of what's going on in terms of interactions:
We can see two clusters, one for low volatile acidity and one for high volatile acidity.
When volatile acidity is low, the dependence of the SHAP interaction value is negative with increasing alcohol.
But for high volatile acidity, higher alcohol levels are associated with an increasing SHAP interaction value.



```{python}
#| label: fig-acidity-alcohol
#| fig-cap: SHAP interaction dependence plot for x1 and x2
shap.dependence_plot(("volatile acidity", "alcohol"), shap_interaction_values, X_test)
```

Also we can look at the main effects of the features:

```{python}
#| label: fig-alcohol-alcohol
#| fig-cap: SHAP interaction dependence plot for x1 and x2
shap.dependence_plot(("alcohol", "alcohol"), shap_interaction_values, X_test)
```

The dependence of SHAP main effect for alcohol is almost linear in increasing alcohol levels.

```{python}
#| label: fig-acidity-acidity
#| fig-cap: SHAP interaction dependence plot for x1 and x2
shap.dependence_plot(("volatile acidity", "volatile acidity"), shap_interaction_values, X_test)
```

```{python}
import matplotlib.pyplot as plt
fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,20))
shap.dependence_plot(("alcohol", "alcohol"), shap_interaction_values, X_test, ax=axes[0], show=False)
shap.dependence_plot(("volatile acidity", "volatile acidity"), shap_interaction_values, X_test, ax=axes[1], show=False)
plt.show()
```

Keep in mind that interaction are not necessarily split intuitively as we uncovered in the [Interaction Chapter](#interaction).
Sorry, would love to give you more definitive answers, but SHAP just gives us a glimpse into the model.


## Analyzing correlated features

TODO: CONTINUE HERE

TODO: Also add correlation plot




## Model-understanding for data subsets

Since the global interpretation relies on aggregating SHAP values, we can do the same for any arbitrary group of data.
So instead of plotting the summary for all data points, we can subset our dataset and plot the summary for the subset's SHAP values instead.

```{python}
subset = X_test["alcohol"].values > 12
shap.summary_plot(shap_values[subset], X_test[subset], feature_names=wine.columns)
```

This summary plot is the summary plot from before, but only with the wines that have an alcohol level above 12.
We observe that:

- The alcohol cut-off is immediately visible in the plot, since we now only observe high alcohol levels.
- The importance ranking of the features changed, but so did the interpretation: Conditional on alcohol>12, the most important features were alcohol, free sulfur dioxide, and residual sugar.
- Volatile acidity become much less important.

And this is of course possible for all the plots. 

```{python}
shap.dependence_plot(
  'residual sugar', shap_values.values[subset], X_test[subset],
  feature_names=wine.columns
)
```
::: {.callout-tip}

By analyzing subsets of the data with "global" tools, we can smoothly transition between local and global explanations.
Use

:::


Be creative: Any feature can be used to create subsets. If you have additional data, you may also analyze subsets based on variables that weren't used in the model.
For example you might want to study how the explanations differ for protected attributes such as race or gender, which are variables that you wouldn't use as features.

