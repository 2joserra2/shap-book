# Regression Using a Random Forest

::: {.callout-tip appearance="simple"}

By the end of this chapter, you will be able to:

- Interpret SHAP for a complex model with interactions and non-linear effects.
- Use the Partition Explainer for correlated features.
- Apply SHAP to analyze subsets of the data.

:::

In this chapter, we will examine the wine dataset again and fit a tree-based model, specifically a random forest.
This model potentially contains numerous interactions and non-linear functions, making its interpretation more complex than in previous chapters.
Nevertheless, we can employ the fast `shap.TreeExplainer`.

## Fitting the Random Forest

Random forests are ensembles of decision trees, and their prediction is an average of the tree predictions.
Random forests usually provide good results without any adjustments.

::: {.callout-note}

Gradient boosted trees algorithms such as LightGBM and xgboost are other popular tree-based models.
The `shap` application demonstrated here works the same way with them.

:::


```{python}
#| output: False
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split

wine = pd.read_csv('wine.csv')
y = wine['quality']
X = wine.drop(columns=['quality'])

X_train, X_test, y_train, y_test = train_test_split(
  X, y, test_size=0.2, random_state=42
)

model = RandomForestRegressor(random_state=42)
model.fit(X_train, y_train)
```

Next, we evaluate the performance of our model, hoping for better results than with the GAM:

```{python}
from sklearn.metrics import mean_absolute_error

y_pred = model.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)
print('MAE:', round(mae, 2))

```

This model performs better than the GAM, suggesting that additional interactions are beneficial.
Despite the GAM also being tree-based, it did not model interactions.

## Computing SHAP Values

Now, let's interpret the model:

```{python}
#| eval: False
import shap
# Compute the SHAP values for the sample
explainer = shap.Explainer(model, X_train)
shap_values = explainer(X_test)
```

::: {.callout-important}

The code above produces an error:
"This check failed because for one of the samples the sum of the SHAP values was 5.881700, while the model output was 5.820000."
The Tree Explainer is an exact explanation method, and `shap` checks if additivity holds: the model prediction should equal the sum of SHAP values + base_value.
In this case, there is a discrepancy in some SHAP values.
I'm not entirely sure why this happens - it may be due to rounding issues.
You might encounter this too, so here are two options to handle it: either set check_additivity to False or use a different explainer, like the Permutation Explainer.
If you disable the check, ensure the difference is acceptable:

```{python}
#| eval: False
import numpy as np
shap_values.base_values +  np.sum(shap_values, axis=1) - \
  model.predict(X_test)
```

:::

Let's try again without checking for additivity:

```{python}
import shap
explainer = shap.Explainer(model, X_train)
shap_values = explainer(X_test, check_additivity=False)
```


::: {.callout-warning}
Please provide a dataset or masker when creating an Explainer for a tree-based model.
While other explainers will not function without data, the tree explainer will default to `feature_pertubation='tree_path_dependent'`, which is not recommended due to its ambiguous interpretation.

:::

Let's revisit the SHAP values for the wine from the [Linear Chapter](#linear) and the [Additive Chapter](#additive).

```{python}
shap.plots.waterfall(shap_values[0], max_display=11)
```

While the results differ from both the linear and the GAM models, the interpretation process remains the same.
A key difference is that the random forest model includes interactions between the features.
However, since there's only one SHAP value per feature value (and not one for every interaction), interactions between feature values are divided among the SHAP values.

## Global model interpretation

Global SHAP plots provide an overall view of how features influence the model's predictions.
Let's examine the summary plot:

```{python}
shap.plots.beeswarm(shap_values)
```

Key observations:

- Alcohol and volatile acidity emerged as the most important features.
- Several features, such as alcohol and volatile acidity, showed a somewhat monotonic relationship with the target.
- The factors with the largest absolute contributions to the predicted quality of some wines included:
  - High alcohol levels leading to higher predicted quality.
  - Low levels of free sulfur dioxide resulting in lower quality.

We can examine interactions in global plots like the dependence plots.
Here's the dependence plot for the alcohol feature:

```{python}
shap.plots.scatter(shap_values[:,"alcohol"], color=shap_values)
```

The `shap` package automatically detects interactions.
In this case, `shap` identified `volatile acidity` as a feature that greatly interacts with `alcohol` and color-coded the SHAP values accordingly.
By default, the `shap` dependence plot chooses the feature that has the strongest interaction with the feature of interest.
The dependence plot function calls the `approximate_interactions` function, which measures the interaction between features through the correlation of SHAP values, with a stronger correlation indicating a stronger interaction.
Then it ranks features based on their interaction strength with a chosen feature.
You can also manually select a feature.

Here are some important observations:

- Generally, a higher alcohol level corresponds to a higher SHAP value.
- Examining cases with low volatile acidity reveals an interesting interaction with wines having a low alcohol level. For wines with low alcohol (between 8% and 11%), if the wines have low volatile acidity, then the SHAP value for alcohol is higher compared to wines with similar alcohol levels.
- The relationship reverses for wines with higher alcohol levels: a higher volatile acidity level is associated with slightly higher SHAP values for alcohol.
- We could infer that volatile acidity alters the effect of alcohol on the predicted wine quality.
- However, this interaction is subtle, and we should avoid overinterpreting it, particularly considering the insights from the [Interaction Chapter](#interaction) about the complexity of interactions.

::: {.callout-note} Here's some advice on interpreting the interaction part of the dependence plot:

- Select one of the two variables.
- For this variable, choose two ranges or categories.
- Compare the SHAP values within these ranges.
- Note whether any differences are related to changes in the other feature.

:::

Next, let's examine the dependence plot for residual sugar as another example.
Residual sugar represents the remaining sugar in the wine, with higher amounts indicating a sweeter taste.

```{python}
shap.plots.scatter(
  shap_values[:,"residual sugar"], color=shap_values
)
```

Key observations:

- Higher residual sugar is associated with higher SHAP values.
- The `shap` package identifies alcohol as having the highest interaction with residual sugar.
- Alcohol and residual sugar are negatively correlated with a correlation coefficient of -0.5 (see later in this chapter); this makes sense as sugar is converted into alcohol during the fermentation process.
- Comparing curves for low (below 12) and high alcohol levels (above 12):
  - High variance in SHAP values is observed when alcohol content is low.
  - High alcohol content is associated with low residual sugar and higher SHAP values, as compared to low alcohol content.  

## Analyzing correlated features

As mentioned in the [Correlation Chapter](#correlation), correlated features require additional consideration.
Let's examine which features are correlated and how to use the Partition explainer.
We'll start with a correlation plot that displays the Pearson correlation between the features, given by the formula:
$$r_{xy} = \frac{\sum_{i=1}^{n}(x^{(i)}-\bar{x})(z^{(i)}-\bar{z})}{\sqrt{\sum_{i=1}^{n}(x^{(i)}-\bar{x})^2}\sqrt{\sum_{i=1}^{n}(z^{(i)}-\bar{z})^2}}$$

The correlation ranges from -1, representing a perfect negative correlation, to +1, indicating a perfect positive correlation.
A value of 0 suggests no correlation.
In this case, $x$ and $z$ represent two features, while $\bar{x}$ and $\bar{z}$ are their respective averages.

```{python}
#| label: fig-correlation
#| fig-cap: "Pairwise feature correlations"
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Compute the correlation matrix
corr = X_train.corr()

# Generate a mask for the upper triangle
mask = np.triu(np.ones_like(corr, dtype=bool))

# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(11, 9))
# Generate a diverging colormap
cmap = sns.diverging_palette(230, 20, as_cmap=True)

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,
    square=True, linewidths=.5, cbar_kws={"shrink": .5},
    annot=True, fmt=".1f")
plt.show()
```

Figure @fig-correlation shows that density correlates with residual sugar (0.8) and total sulfur dioxide (0.5).
Density also has a strong negative correlation with alcohol.
Volatile acidity does not show a strong correlation with other features.

The Partition explainer is a method that handles correlated features by computing SHAP values based on hierarchical feature clusters.

An obvious strategy is to use correlation to cluster features so that highly correlated features are grouped together.
However, one modification is needed:
Correlation leads to extrapolation, which we need to manage, but it doesn't matter whether the correlation is positive or negative.
Clustering based on correlation would cause features with a strong negative correlation to be far apart in the clustering hierarchy, which is not ideal for our goal of reducing extrapolation.
Therefore, in the following example, we perform tree-based hierarchical clustering on the **absolute correlation**.
Features that are highly correlated, whether negatively or positively, are grouped together hierarchically until the groups with the least correlation are merged.

```{python}
#| label: fig-clustering
#| fig-cap: "Hierarchically clustered features based on correlation"
#| width: "80%"
import matplotlib.pyplot as plt
from scipy.cluster import hierarchy

correlation_matrix = X_train.corr()
correlation_matrix = np.corrcoef(correlation_matrix)
correlation_matrix = np.abs(correlation_matrix)
dist_matrix = 1 - correlation_matrix

import scipy.cluster.hierarchy as sch

clustering = sch.linkage(dist_matrix, method="complete")

#clustering = shap.utils.hclust(X_train, metric='correlation')

plt.figure(figsize=(10, 7))
plt.title("Dendrograms")
dend = hierarchy.dendrogram(clustering, labels=X_train.columns)

# Rotate labels for better readability
plt.xticks(rotation=90)

# Increase label size for better visibility
plt.tick_params(axis='x', which='major', labelsize=12)
plt.ylabel('Correlation Distance')

plt.show()
```
Figure @fig-clustering shows the clustering results: density and alcohol are combined first, then merged with residual sugar, and finally with the cluster of free and total sulfur dioxide.
As we ascend, the correlation weakens.
This clustering hierarchy is input into the Partition explainer to produce SHAP values:

```{python}
masker = shap.maskers.Partition(X_train, clustering=clustering)
explainer2 = shap.PartitionExplainer(model.predict, masker)
shap_values2 = explainer2(X_test)
```

::: {.callout-note}
By using the Partition explainer instead of the Tree explainer, we avoid the issue of checking additivity.
:::

We now have our new SHAP values.
The key question is: Do the results differ from when we ignored feature correlation?
Let's compare the SHAP importances:

```{python}
fig = plt.figure(figsize=(6,12))
ax0 = fig.add_subplot(211)
shap.plots.bar(shap_values, max_display=11, show=False)
ax1 = fig.add_subplot(212)
shap.plots.bar(
  shap_values2, max_display=11, show=False, clustering_cutoff=0.6
)
plt.tight_layout()
plt.show()
```

While the SHAP importances are not identical, the differences are not substantial.
However, the real benefit is the new interpretation we gain from clustering and the Partition explainer:

- We may add the SHAP values for both alcohol and density and interpret this as the effect for the alcohol and density group. There was no extrapolation between the two features, meaning no unlikely combination of alcohol and density was formed.
- Similarly, we may interpret the combined SHAP importance: we can interpret 0.34 + 0.03 = 0.37 as the SHAP importance of the alcohol+density group.
- Free and total sulfur dioxide form a cluster with a combined importance of 0.16.
- Collectively, they are more important than volatile acidity and, due to their high correlation, we have a solid argument for analyzing them together.

As the user, you can decide how high up you go in the hierarchy by increasing the `clustering_cutoff` and then adding up the SHAP values (or SHAP importance values) for clusters.
The higher the cutoff, the larger the groups, but also the more the correlation problem is reduced.

Now let's compare the SHAP explanation for the first data instance:

```{python}
shap.plots.bar(shap_values2[0], clustering_cutoff=0.6)
```

Again, there are only slight differences in the SHAP values, and we can combine the SHAP values of clusters in addition to interpreting the individual SHAP values.
For the computation of a combined SHAP value, the features within that group were not subjected to extrapolation through marginal sampling.
Revisit the [Correlation Chapter](#correlation) for a refresher on this concept.
For instance, the feature group "alcohol, density and residual sugar" contributed a significant +0.55 (0.39 + 0.02 + 0.14) to the predicted quality.
We know that for the group SHAP value of 0.55, alcohol, density and residual sugar were always kept together in coalitions.

However, the individual SHAP values are still partially susceptible to extrapolation.
For instance, the SHAP value for alcohol was computed by attributing 0.41 to both density and alcohol.
For this attribution, density was also sampled by marginal sampling, which introduces extrapolation, such as combining high alcohol values with a high density.
So we have a trade-off between extrapolation and group granularity:
The higher we ascend in the clustering hierarchy, the less extrapolation but the larger the feature groups become, which also complicates interpretation.

## Understanding models for data subsets

Global interpretation is based on aggregating SHAP values.
We can also use this aggregation to analyze data subsets. 
For example, we can examine wines with an alcohol content above 12, a useful subset of wines.
In technical terms, this means that we subset the SHAP values and generate summary, dependence, and importance plots.
However, when we wish to investigate a subset of data, such as wines rich in alcohol, should we also subset the background data used to estimate the SHAP values?

The choice of background data depends on the goal of interpretation:

- Are you interested in explaining the prediction difference compared to all wines?
- Or in comparison to the prediction of alcohol-rich wines?

Modifying the background data alters the value function.
Wines with higher alcohol content have higher predicted qualities.
A wine predicted to be of above-average quality may actually be below average if the average is calculated from alcohol-rich wines.
In the first scenario, the SHAP values of the wines would sum up to a positive value, whereas in the second scenario, they would sum up to a negative value.
Let's explore the two methods of comparing subsets.

```{python}
# create the data subsets
ind_test = np.where(X_test['alcohol'].values > 12)
ind_train = np.where(X_train['alcohol'].values > 12)
X_train_sub = X_train.iloc[ind_train]
X_test_sub = X_test.iloc[ind_test]

# SHAP where background data is based on subset
explainer_sub = shap.Explainer(model, X_train_sub)
shap_values_sub = explainer_sub(X_test_sub)

# SHAP where background data includes all wines
shap_values_sub_all = shap_values[ind_test]
```

We start with the SHAP values for a single wine.

```{python}
#| label: fig-subsets
#| fig-cap:  "Top: Background data are wines with alcohol > 12. Bottom: Background data includes all wines"
import matplotlib.pyplot as plt

fig = plt.figure(figsize=(6,12))
ax0 = fig.add_subplot(211)
shap.plots.waterfall(shap_values_sub[1], show=False)
ax1 = fig.add_subplot(212)
shap.plots.waterfall(shap_values_sub_all[1], show=False)
plt.tight_layout()
plt.show()
```

@fig-subsets illustrates the distinct explanations produced by SHAP:

- While this wine's quality is above average when compared to all wines, it is below average in predicted quality when compared to alcohol-rich wines.
- Alcohol, typically the most impactful feature, is not relevant for this wine. This is logical because we conditioned the background data on alcohol. The question then becomes: How much did an alcohol level of 12.2 contribute to the prediction, compared to the average prediction for alcohol-rich wines?
- The reference changes: Since wines rich in alcohol are associated with higher predicted quality, $\mathbb{E}(f(X))$ is also higher when we use all wines as background data. This means that the SHAP values only need to explain a smaller difference of almost 0 instead of approximately 0.7.

:::{.callout-tip}

## Interpretation template for subsets *(replace [] with your data)*

The prediction [$f(x)$] for instance [$i$] differs from the average prediction of [$\mathbb{E}(f(X))$] for [subset] by [$f(x^{(i)} − \mathbb{E}(f(X))$] and [feature name = feature value] contributed [$\phi^{(i)}_j$] towards that difference.

:::

Here's an example of interpretation:

```{python}
#| echo: False
from IPython.display import display, Markdown

i = 1
y = model.predict(X_test_sub)[i]
bv = shap_values.base_values[0]
diff = y - bv

feature1 = 'free sulfur dioxide'
ind = X_test_sub.columns.get_loc(feature1)
fv1 = X_test_sub.iloc[i, ind]
sv1 = shap_values_sub.values[i,ind]

feature2 = 'sulphates'
ind = X_test_sub.columns.get_loc(feature2)
fv2 = X_test.iloc[i, ind]
sv2 = shap_values_sub.values[i, ind]

feature3 = 'fixed acidity'
ind = X_test_sub.columns.get_loc(feature3)
fv3 = X_test_sub.iloc[i, ind]
sv3 = shap_values_sub.values[i, ind]

display(Markdown("""
The predicted value of {y} for instance {i} differs from the expected average prediction of {base_value} for wines with an alcohol content greater than 12 by {diff}.

- {feature1}={fv1} contributed {sv1}
- {feature2}={fv2} contributed {sv2}
- {feature3}={fv3} contributed {sv3}
- ...

The sum of all SHAP values equals to the difference between the prediction ({y}) and the expected value ({base_value}).
""".format(base_value=round(bv, 2), y=np.round(y, 2), i=i, diff=np.round(diff, 2),
           feature1=feature1, fv1=round(fv1, 2), sv1=np.round(sv1, 3),
           feature2=feature2, fv2=round(fv2, 2), sv2=np.round(sv2, 3),
           feature3=feature3, fv3=round(fv3, 2), sv3=np.round(sv3, 3))))
```


Keeping the background data set for all wines and subsetting the SHAP values produces the same individual SHAP values, but it changes the global interpretations:

```{python}
#| label: fig-summaries
#| fig-cap: "The left plot includes all SHAP values with all wines as background data. The middle plot contains SHAP values for wines high in alcohol with all wines as background data. The right plot displays SHAP values for wines high in alcohol with background data also comprising wines high in alcohol. The feature order for all plots is based on the SHAP importance of the left plot."
# sort based on SHAP importance for all data and all wines
ordered = np.argsort(abs(shap_values.values).mean(axis=0))[::-1]
plt.subplot(131)
shap.plots.beeswarm(
  shap_values, show=False, color_bar=False, order=ordered
)
plt.xlabel("")
plt.subplot(132)
shap.plots.beeswarm(
  shap_values_sub_all, show=False, color_bar=False, order=ordered
)
plt.gca().set_yticklabels([])  # Remove y-axis labels
plt.ylabel("")
plt.subplot(133)
shap.plots.beeswarm(
  shap_values_sub, show=False, color_bar=False, order=ordered
)
plt.gca().set_yticklabels([])  # Remove y-axis labels
plt.ylabel("")
plt.xlabel("")
plt.tight_layout()
plt.show()
```

@fig-summaries shows how subsetting SHAP values alone or together with the background data influences the explanations.
Alcohol, according to SHAP, is the most crucial feature.
Its importance remains when we subset SHAP values for wines high in alcohol.
Its significance increases because these wines, high in alcohol, have a high predicted quality due to their alcohol content.
However, when we also alter the background data, the importance of alcohol significantly decreases, as evidenced by the close clustering of the SHAP values around zero.
However, more insights can be found in @fig-summaries.
For instance, consider volatile acidity.
A higher volatile acidity typically correlates to lower SHAP values, but different patterns emerge when considering wines rich in alcohol.
Firstly, the SHAP values of volatile acidity exhibit a smaller range.
Moreover, some wines with high volatile acidity surprisingly present positive SHAP values, contradicting the usual relationship between volatile acidity and predicted quality.

::: {.callout-tip}

Be inventive: Any feature can be employed to form subsets.
You can even resort to variables that were not used as model features for subset creation.
For example, you may want to examine how explanations change for protected attributes like ethnicity or gender, variables which you would not normally employ as features.

:::
