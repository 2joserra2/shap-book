# Regression with xgboost

CONTINUE HERE

::: {.callout-tip appearance="simple"}

By the end of this chapter, you will be able to:

- Interpret SHAP for a complex model with interactions and non-linear effects
- Use the Partition explainer to handle correlated features
- Analyze the model for subsets of the data

:::

In this chapter, we will revisit the wine data and fit a tree-based model, namely an xgboost model.
This means the model potentially includes numerous interactions and non-linear functions, resulting in a more complex interpretation compared to previous chapters.
But, on the good side, we can utilize the fast `shap.TreeExplainer`.

## Fitting the xgboost model

If you haven't installed xgboost yet, install it as follows:

```{python}
#| eval: false
pip install xgboost
```

xgboost is a gradient boosting framework based on decision trees.
It's fast, efficient, and generally performs well.
But enough advertisement, let's train the xgboost model.

```{python}
import pandas as pd
import xgboost as xgb
import shap
from sklearn.model_selection import train_test_split

# Load the dataset
wine = pd.read_csv("wine.csv")

y = wine["quality"]
X = wine.drop(columns=["quality"])

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
  X, y, test_size=0.2, random_state=42
)

# Train the model
model = xgb.XGBRFRegressor()
model.fit(X_train, y_train)
```

Again, we evaluate the performance of our solution, hoping for better results than with just the GAM:

```{python}
from sklearn.metrics import mean_absolute_error

# Make predictions on the test data
y_pred = model.predict(X_test)
# Compute the MAE
mae = mean_absolute_error(y_test, y_pred)

print('MAE:', round(mae, 2))

```

Once again, this model performs slightly better than the GAM, justifying the inclusion of additional interactions.
Notably, the GAM was also tree-based but without modeling interactions.


## Computing SHAP values

Now let's interpret the model:

```{python}
import shap
# Compute the SHAP values for the sample
explainer = shap.Explainer(model)
shap_values = explainer(X_test)
```

For this model, `shap` can automatically extract the data for the background data.

Let's again have a look at this wine that we already examined in the [Linear Chapter](#linear) and the [Additive Chapter](#additive).

```{python}
# Visualize the SHAP values
shap.plots.waterfall(shap_values[0])
```

The results differ from both the linear and the GAM models, but the interpretation process remains similar.
However there is on big difference:
The xgboost model contains interactions between the features.
But since there's only one SHAP value per feature value (and not one for every interaction), it means that interactions between feature values are split between the SHAP values.

## Global model interpretation

Global SHAP plots enable us to see how features impact the overall predictions in the model.
Let's examine the summary plot:

```{python}
shap.summary_plot(shap_values, X_test)
```

Observations:

- Alcohol and volatile acidity were the most important features.
- Many features, such as alcohol and volatile acidity, show a (somewhat) monotonic relationship with the target.
- Factors that influenced the predicted quality of some wines in the most extreme ways included:
  - High alcohol levels for higher predicted quality.
  - Low levels of free sulfur dioxide for lower quality.

The interactions can be analyzed in the global plots such as the dependence plots.
Here is the dependence plot for the alcohol feature:

```{python}
shap.dependence_plot(
  "alcohol", shap_values.values, X_test,
  feature_names=wine.columns
)
```

The `shap` package has an automatic interaction detection.
In this case, `shape` selected `volatile acidity` as a feature that highly interacts with `alcohol` and colored the SHAP values according to this feature.
The feature `volatile acidity` was picked because it had the highest estimated interaction with `alcohol`.

::: {.callout-note}

## Automatic Interaction Detection

By default, the `shap` dependence plot picks the feature that has the strongest interaction with the feature of interest.
The dependence plot function calls the  `approximate_interactions` function which measures interaction between features through correlation of SHAP values, with stronger correlation indicating stronger interaction.
It then returns a ranking of features based on their interaction strength with a selected feature.
You can also pick a feature by hand.

:::


Some key observations:

- Generally, a higher alcohol level corresponds with greater corresponding SHAP value
- Examining cases with low volatile acidity reveals an interesting interaction with wines that have a low alcohol level. While wines with low alcohol (around 9%) typically have a negative SHAP value, low volatile acidity seems to have a slightly positive impact.
- When alcohol levels are high (11 - 14%), having a higher volatile acidity appears to be slightly beneficial, though the effect is quite subtle. Note that this effect is not necessarily causal and ignores dependencies between the features.

::: {.callout-note}
General advice on reading the interaction part of the dependence plot:

- Choose one of the two variables.
- For this variable, select two ranges or categories.
- Compare the SHAP values within these ranges.
- Observe if any differences are associated with changes in the other feature.

:::

Now, let's examine the dependence plot for residual sugar.
Residual sugar indicates the remaining sugar in the wine, with higher amounts making it sweeter.

```{python}
shap.dependence_plot(
  'residual sugar', shap_values.values, X_test,
  feature_names=wine.columns
)
```


Observations:

- Higher residual sugar corresponds to higher SHAP values.
- The `shap` package identifies the highest interaction with density.
- Density and residual sugar are correlated (not independent); for instance, high sugar levels result in high density values. Physics.
- Comparing curves for low density (around 0.99) and high density (around 0.998):
  - When density is low, increasing residual sugar values are associated with larger SHAP values.
  - If density is high, different residual sugar values doesn't show much changes in SHAP values.

<!--
## Analyzing interactions in more detail

Let's use the SHAP interaction values to drill a bit deeper into the interaction between  alcohol and volatile acidity.

```{python}
shap_interaction_values = explainer.shap_interaction_values(X_test)
```

And then we plot the dependence plot:

```{python}
#| label: fig-alcohol-acidity
#| fig-cap: SHAP interaction dependence plot for x1 and x2
shap.dependence_plot(("alcohol", "volatile acidity"), shap_interaction_values, X_test)
```

We can also flip the features.
Again, it's the same SHAP interaction value $\phi_{alcohol, vol.ac.}$, but showing volatile acidity on the x-axis and alcohol as color:

This actually gives us a much clearer picture of what's going on in terms of interactions:
We can see two clusters, one for low volatile acidity and one for high volatile acidity.
When volatile acidity is low, the dependence of the SHAP interaction value is negative with increasing alcohol.
But for high volatile acidity, higher alcohol levels are associated with an increasing SHAP interaction value.



```{python}
#| label: fig-acidity-alcohol
#| fig-cap: SHAP interaction dependence plot for x1 and x2
shap.dependence_plot(("volatile acidity", "alcohol"), shap_interaction_values, X_test)
```

Also we can look at the main effects of the features:

```{python}
#| label: fig-alcohol-alcohol
#| fig-cap: SHAP interaction dependence plot for x1 and x2
shap.dependence_plot(("alcohol", "alcohol"), shap_interaction_values, X_test)
```

The dependence of SHAP main effect for alcohol is almost linear in increasing alcohol levels.

```{python}
#| label: fig-acidity-acidity
#| fig-cap: SHAP interaction dependence plot for x1 and x2
shap.dependence_plot(("volatile acidity", "volatile acidity"), shap_interaction_values, X_test)
```

```{python}
import matplotlib.pyplot as plt
fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,20))
shap.dependence_plot(("alcohol", "alcohol"), shap_interaction_values, X_test, ax=axes[0], show=False)
shap.dependence_plot(("volatile acidity", "volatile acidity"), shap_interaction_values, X_test, ax=axes[1], show=False)
plt.show()
```

Keep in mind that interaction are not necessarily split intuitively as we uncovered in the [Interaction Chapter](#interaction).
Sorry, would love to give you more definitive answers, but SHAP just gives us a glimpse into the model.
-->

## Analyzing correlated features

As we discussed in the [Correlation Chapter](#correlation), correlated features need extra attention.
Let's now investigate how features are correlated and how to handle them.
For that we start with a correlation plot that shows us the Pearson correlation between each of the features.
The Pearson correlation coefficient is defined as

$$r_{xy} = \frac{\sum_{i=1}^{n}(x^{(i)}-\bar{x})(z^{(i)}-\bar{z})}{\sqrt{\sum_{i=1}^{n}(x^{(i)}-\bar{x})^2}\sqrt{\sum_{i=1}^{n}(z^{(i)}-\bar{z})^2}}$$

and it ranges from -1 for a perfect negative correlation to +1 for a perfect positive correlation, with 0 indicating no correlation at all.
$x$ and $z$ are two features, and $\bar{x}$ and $\bar{z}$ are their means.


```{python}
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Compute the correlation matrix
corr = X_train.corr()

# Generate a mask for the upper triangle
mask = np.triu(np.ones_like(corr, dtype=bool))

# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(11, 9))
# Generate a diverging colormap
cmap = sns.diverging_palette(230, 20, as_cmap=True)

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,
    square=True, linewidths=.5, cbar_kws={"shrink": .5},
    annot=True, fmt=".1f")
plt.show()
```

This shows us, for example, that density has a high correlation with residual sugar (0.8) and total sulfur dioxide.
Volatile acidity is a feature that isn't strongly correlated with any of the other features.
Alcohol is strongly negative correlated with density.
And so on.


To actually compute the Owen values, aka the clustered SHAP values, we have to cluster the features based on these correlations.
One way to do it is to rely on hierarchical clustering that is implemented in `shap`.
Hierarchical clustering doesn't give us distinct clusters, but rather a tree with the hierarchy of correlated features.
Meaning features are merged to clusters, starting from the ones with the highest correlation.

```{python}
import matplotlib.pyplot as plt
from scipy.cluster import hierarchy

clustering = shap.utils.hclust(X_train, metric = "correlation")

plt.figure(figsize=(10, 7))  
plt.title("Dendrograms")  
dend = hierarchy.dendrogram(clustering, labels=X_train.columns)

# Rotate labels for better readability
plt.xticks(rotation=90)

# Increase label size for better visibility
plt.tick_params(axis='x', which='major', labelsize=10)
plt.ylabel('Correlation Distance')

plt.show()
```

As we saw in the correlation plot, residual sugar and density are combined first, then merged with the cluster of free sulfur dioxide and total sulfur dioxide.
And so on.

This clustering hierarchy is input to the Partition masker and explainer to produce SHAP values where, for the estimation, the hierarchy of features is respected:

```{python}
masker = shap.maskers.Partition(X_train, clustering = clustering)
explainer2 = shap.PartitionExplainer(model.predict, masker)
shap_values2 = explainer2(X_test)
```

Now we've got our new SHAP values.
And the most interesting question is:
Are the results different from when we were ignoring the feature correlation?
Let's start on a summary statistic, the SHAP importance, to get the big picture of changes: 

```{python}
fig = plt.figure(figsize=(12,6))
ax0 = fig.add_subplot(121)
shap.plots.bar(shap_values, show=False)
ax1 = fig.add_subplot(122)
shap.plots.bar(shap_values2, clustering_cutoff=0, show=False)
plt.tight_layout()
plt.show()
```

While the importances aren't exactly the same, they don't differ substantially.
But maybe, on an individual level the explanations are quite different?
Let's compare the SHAP explanation of the first data instance:

```{python}
fig = plt.figure(figsize=(12,6))
ax0 = fig.add_subplot(121)
shap.plots.waterfall(shap_values[0], show=False)
ax1 = fig.add_subplot(122)
shap.plots.waterfall(shap_values2[0], show=False)
plt.tight_layout()
plt.show()
```

Again, there are slight differences in the SHAP values, but nothing that would change our takeaways.
Where does that leave us and our interpretation?
It's actually great that the explanations don't differ.
Because it means that when ignoring the dependencies and the unavoidable extrapolation happened, the resulting SHAP values weren't biased.
At least compared to when we don't extrapolate.

In the [shap docs](https://shap.readthedocs.io/en/latest/example_notebooks/api_examples/explainers/Exact.html) (accessed July 2023) it's also mentioned that:

> Note that there is a strong similarity between the explanation from the Independent masker above and the Partition masker here. In general the distinctions between these methods for tabular data are not large, though the Partition masker allows for much faster runtime and potentially more realistic manipulations of the model inputs (since groups of clustered features are masked/unmasked together).

## Model-understanding for data subsets

Since the global interpretation relies on aggregating SHAP values, we can do the same for any arbitrary group of data.
So instead of plotting the summary for all data points, we can subset our dataset and plot the summary for the subset's SHAP values instead.

```{python}
subset = X_test["alcohol"].values > 12
shap.summary_plot(shap_values[subset], X_test[subset], feature_names=wine.columns)
```

This summary plot is the summary plot from before, but only with the wines that have an alcohol level above 12.
We observe that:

- The alcohol cut-off is immediately visible in the plot, since we now only observe high alcohol levels.
- The importance ranking of the features changed, but so did the interpretation: Conditional on alcohol>12, the most important features were alcohol, free sulfur dioxide, and residual sugar.
- Volatile acidity become much less important.

And this is of course possible for all the plots. 

```{python}
shap.dependence_plot(
  'residual sugar', shap_values.values[subset], X_test[subset],
  feature_names=wine.columns
)
```
::: {.callout-tip}

By analyzing subsets of the data with "global" tools, we can smoothly transition between local and global explanations.
Use

:::


Be creative: Any feature can be used to create subsets. If you have additional data, you may also analyze subsets based on variables that weren't used in the model.
For example you might want to study how the explanations differ for protected attributes such as race or gender, which are variables that you wouldn't use as features.

