# Explaining Language Models {#text}

::: {.callout-tip appearance="simple"}

By the end of this chapter, you will be able to:

- Apply SHAP for text classification and text generation models.
- Select a tokenization level for interpretation.

:::

Let's explore text-based models.
All models in this chapter have one thing in common: their inputs are text.
However, we'll encounter two distinct types of outputs:

- Scores, such as in classification and sentiment analysis.
- Texts, such as in text generation, translation, and summarization.

While they might seem different at first glance, they're quite similar upon closer inspection.
We'll start with the simpler case where a model outputs a single score for a text input, like in classification, for instance, determining the category of a news article.

In this chapter, we'll mainly work with transformers, which are state-of-the-art for text-based machine learning.
However, keep in mind, SHAP values are model-agnostic.
So it doesn't matter if the underlying model is a transformer neural network or a support vector machine that works with engineered features, like TF-IDF (Term Frequency-Inverse Document Frequency).

## How SHAP for text works

As with other applications of SHAP, the aim is to attribute the prediction to the inputs.
SHAP requires a scalar output, which is straightforward when dealing with a text classifier.
For sequence-to-sequence models, the output isn't inherently scalar, but we can make it so by examining the score for a specific token instead of the word.

The features in both text classification and text-to-text models are text-based.
However, it's not as simple as it sounds, because it's not words that are fed into the neural network but numbers.
In the case of state-of-the-art neural networks, these numbers are represented as embedding tokens.
Tokens are typically smaller than words, and there are numerous methods to tokenize text.

## Defining players in text

As seen in the [Image Chapter](#image), we can use different levels of granularity for SHAP inputs than those used for the model.

This provides us with multiple options for computing SHAP values:

- By character
- By token
- By word
- By sentence
- By paragraph
- And everything in between

The choice depends on the specific application, and we'll explore various examples throughout this chapter.
Consider the task of sentiment analysis.
The sentence "I returned the item as it didn't work." has a predicted score indicating a negative sentiment with a score of -0.8.

The aim of SHAP is to attribute this score to the input words.
If you choose to attribute the prediction at the word level, you will obtain one SHAP value for each word: ["I", "returned", "the", "item", "as", "it", "didn't", "work"].

Each word acts as a team player, and the -0.8 score is fairly distributed among them.

## Removing players in text-based scenarios

An interesting question arises: how do you simulate the absence of players/features in text?

In theory, you have multiple options:

- Remove the word.
- Replace it with a fixed token (e.g., "â€¦").
- Replace it with a draw from background data.

SHAP implements options 1 and 2.
Options 1 and 2 are more reasonable than option 3, as option 3 could introduce significantly different texts.

For example, to assess the impact of the word "returned" on the negative sentiment prediction, we would include it in different teams.
Assuming that missing words are replaced with "...", and multiple adjacent words are replaced with a single "...", we get:

- "I ... the ... as it didn't work" -> "I returned the ... as it didn't work"
- "... the item ...  work" -> "... returned the item ... work"
- "..." -> "... returned ..." 

With this theoretical knowledge in hand, let's proceed to a text classification example.

## Text classification

Let's start with a straightforward example of a classification task.
We've already discussed classification, but now our input is text.
A classic example is sentiment analysis, which, while it has its own name, is essentially just classification with predetermined labels: positive and negative.

We'll use Hugging Face transformers, which simplifies the implementation:

```{python}
from transformers import pipeline
model = pipeline('sentiment-analysis', return_all_scores=True)
s = ['This product was a scam']
print(model(s)[0])
```

As anticipated, this statement is classified as negative. 
The keyword "scam" would naturally lead us to this conclusion, but let's delve into whether this was the reason for the model's classification and if SHAP can offer any insight.

Let's create an explainer model, which SHAP will intuitively know how to handle:

```{python}
import shap
explainer = shap.Explainer(model)
shap_values = explainer(s)
shap.plots.bar(shap_values[0, :, 'NEGATIVE'])
```

That was straightforward, wasn't it?

The term "sc" appears to contribute the most to the negative sentiment.

However, the splitting of "scam" into "sc" and "am" isn't ideal for interpretation. 
This issue emerges from our masking of the input, in which the choice of tokenizer influences how the text is masked.

## Experimenting with the masker

Let's modify the tokenizer as previously discussed. 
In the following code, I present a custom tokenizer that partitions the text into words. 
This is achieved using maskers, the SHAP abstraction that simulates the absence of features. 
Next, we supply both the model and the masker to SHAP.

Let's review the results:

```{python}
masker = shap.maskers.Text(tokenizer=r"\W+")
explainer = shap.Explainer(model, masker=masker)
shap_values = explainer(s)
shap.plots.bar(shap_values[0, :, 'NEGATIVE'])
```

Now it's evident that "scam" is the significant term. 
The tokenizer is highly adaptable. 
To illustrate this, let's consider another example using SHAP values calculated on sentences. 
In this scenario, the tokenizer is simple and breaks the input at periods ".". 
We'll experiment with a lengthier input text to observe the contribution of each sentence.

```{python}
s2 = "This product was a scam." + \
     "It was more about marketing than technology." + \
     "But that's why I loved it." + \
     "Learned a bunch about marketing that way."
print(model([s2]))
masker = shap.maskers.Text(tokenizer=r"\.", mask_token=" ")
explainer = shap.Explainer(model, masker=masker)
shap_values = explainer([s2])
shap.plots.bar(shap_values[:, :, 'NEGATIVE'])
```

I've used a whitespace " " as the masking token, which seems suitable for dropping a sentence. 
The default replacement token for text is "...", but generally, if a tokenizer is provided, the .mask_token attribute is utilized, assuming the tokenizer has this attribute.

To illustrate "extreme" masking, let's substitute a removed sentence with a specific one instead of leaving it blank. 
The `collapse_mask_token=True` argument ensures that if two tokens in a row are replaced by the mask_token, the token is only added once. 
In the ensuing example, sentences are replaced with "I love it", but only once consecutively.

Consider the sentence: "This product was a scam. 
It was more about marketing than technology. 
But that's why I loved it. 
Learned a bunch about marketing that way."

Let's analyze the marginal contribution of "Learned a bunch about marketing" when added to an empty set, by comparing these two sentences:

"I love it. Learned a bunch about marketing that way." versus "I love it."

If `collapse_mask_token=False`, we would compare "I love it. I love it. I love it. Learned a bunch about marketing that way." with "I love it. I love it. I love it. I love it."

Therefore, it often makes sense to set `collapse_mask_token` to True.

In theory, you could also create a custom masker.

```{python}
masker = shap.maskers.Text(tokenizer=r"\.", mask_token="I love it", collapse_mask_token=True)
explainer = shap.Explainer(model, masker=masker)
shap_values = explainer([s2])
shap.plots.bar(shap_values[:, :, 'POSITIVE'])
```

```{python}
masker = shap.maskers.Text(tokenizer=r"\.", mask_token='I hate it', collapse_mask_token=True)
explainer = shap.Explainer(model, masker=masker)
shap_values = explainer([s2])
shap.plots.bar(shap_values[:, :, 'POSITIVE'])
```

Here, the replacement acts as a reference.
In one scenario, any sentence removed from the coalition is replaced with "I love it," and in the other scenario, it's replaced with "I hate it."

What changes is the base value, it shifts from strongly positive to negative, as can be inferred from the difference in the range of SHAP values on the x-axis.
Every sentence is now interpreted in contrast to the replacement.
This was also true earlier, but previously we replaced it with an empty string, which is more neutral than the sentences provided.

:::{.callout-warning}

Avoid using extreme masking tokens as they might not make sense.
However, more specific tokens can be beneficial.
This highlights the importance of masking, which serves as background data.
Consider the replacement carefully and test alternatives if necessary.

:::

There's a difference when replacing tokens with " " or "...":

```{python}
masker = shap.maskers.Text(
  tokenizer=r"\.", mask_token='...', collapse_mask_token=True
)
explainer = shap.Explainer(model, masker=masker)
shap_values = explainer([s2])
shap.plots.bar(shap_values[:, :, 'POSITIVE'])
```

```{python}
masker = shap.maskers.Text(
  tokenizer=r"\.", mask_token=' ', collapse_mask_token=True
)
explainer = shap.Explainer(model, masker=masker)
shap_values = explainer([s2])
shap.plots.bar(shap_values[:, :, 'POSITIVE'])
```

Despite the overall attribution not changing significantly, except for the sign change in the "marketing" sentence which was close to zero, the base value changes considerably.

Experiment with it, generate some text, and make a qualitative judgment about whether it makes sense.
To understand more about maskers, refer to the [maskers chapter in the Appendix](#maskers).

## Using logits instead of probabilities

The output falls in the probability space between 0 and 1, necessitating a logit transformation.
Additive explanations perform better on linear scales, such as logits, which occur just before the 0 to 1 squeezing.

SHAP provides a wrapper for transformers that allows specifying whether to use logits or probabilities:

```{python}
model2 = shap.models.TransformersPipeline(
  model, rescale_to_logits=True
)
```

Like the original transformer, you can make predictions with this model:

```{python}
model2(s)
```

Now let's see how this impacts the explanations with SHAP:

```{python}
explainer2 = shap.Explainer(model2)
shap_values2 = explainer2(s)
shap.plots.bar(shap_values2[0,:, 'NEGATIVE'])
```

This result is quite similar, but now "am" is also a positive factor, which makes more sense.

## How SHAP interacts with text-to-text models

Text-to-text models are unique in that they generate multiple outputs.
Each token produced by the neural network is treated as an individual prediction.
This can be viewed as a classification task where the goal is to determine the next token based on some text input.
Consider early large language models that aimed to produce the next words given an input text.
For example:

Input text: "Is this the Krusty Krab?"
Output text: "No! This is Patrick!"

In the context of text-to-text models, each output token is considered an individual prediction, much like in multi-class classification.
We can compute SHAP values for each token.

If the tokenized input has length $n$ and the tokenized output length is $m$, we derive $n \cdot m$ SHAP values.
The level of input tokenization is user-controllable.
In the example above, if the user opts for word-level tokenization for the input, the first token of the output, i.e., "No", receives $n$ SHAP values.
The next token "!" gets $n$ Shapley values, and so on.

However, there is a complication.
Transformers not only consider the input but also the output generated so far when producing the next token.
For instance, if "No! This is" has already been generated by the model, it forms part of the input for deciding the next token, "Patrick".
In the case of SHAP, this portion is simply held constant.
Thankfully, the 'shap' package has already implemented this for us.

## Explaining a text-to-text model

Let's explore a text generation task, a fairly general task.
Again, this is implemented in the Hugging Face transformers package.
It's a large language model similar to those that power GPT-3 and GPT-4.
For a locally runnable example that doesn't require an account, we'll use a less powerful model.
This doesn't change the general interface of text input and output.
The key difference is that we get not only the words but also their scores, crucial for calculating SHAP values.
In this case, we'll use GPT-2, automatically selected by the transformers library for the "text-generation" task at the time of writing.
The following code is partially based on [this shap notebook](https://shap.readthedocs.io/en/latest/example_notebooks/text_examples/text_generation/Open%20Ended%20GPT2%20Text%20Generation%20Explanations.html).

```{python}
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained('gpt2')
model = AutoModelForCausalLM.from_pretrained('gpt2')
```

Next, we decide on the text to complete.

```{python}
#| output: asis
import torch

# Set seed for consistent results
torch.manual_seed(0)

input_text = 'He insulted Italian cuisine by'

# Encode input text
input_ids = tokenizer.encode(input_text, return_tensors='pt')

# Sample instead of returning most likely token
model.config.do_sample=True
# Set maximum length
model.config.max_new_tokens = 30

# Generate text with stop_token set to "."
output = model.generate(input_ids)

# Decode output text
output_text = tokenizer.decode(output[0], skip_special_tokens=True)
print('The result: "' + output_text + '"')
```

Now we can obtain the SHAP explanations for this sentence:

```{python}
torch.manual_seed(0)
# Setting model to decoder to prevent input repetition.
model.config.is_decoder = True
explainer = shap.Explainer(model, tokenizer)
shap_values = explainer([input_text])
shap.plots.waterfall(shap_values[0, :, 5 + 1])
```

This is the explanation for the second word in the predicted sequence, "throwing".
The word "insulted" positively contributed to this word, while all others had a negative contribution.

## Other text-to-text tasks
Here are other examples of text-to-text tasks for which notebooks are available:

- [Question Answering](https://shap.readthedocs.io/en/latest/example_notebooks/text_examples/question_answering/Explaining%20a%20Question%20Answering%20Transformers%20Model.html)
- [Summarization](https://shap.readthedocs.io/en/latest/example_notebooks/text_examples/summarization/Abstractive%20Summarization%20Explanation%20Demo.html)
- [Machine translation](https://shap.readthedocs.io/en/latest/example_notebooks/text_examples/translation/Machine%20Translation%20Explanations.html)

As general text-to-text models can now handle these tasks, the given example should suffice.
