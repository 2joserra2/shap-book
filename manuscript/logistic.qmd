# Classification with Logistic Regression {#classification}

::: {.callout-tip appearance="simple"}

By the end of this chapter, you will be able to:

- Interpret SHAP for classification models
- Decide whether to interpret the probabilities or the log odds
- Handle categorical features
- Use alternatives to the waterfall plot: the bar plot and the force plot
- Describe the cluster plot and the heatmap plot 

:::

This chapter shows how to use and interpret SHAP with logistic regression.
Differences from linear regression include:

- Two outcomes instead of one.
- Outcome is either a probability [^no-probability] or log odds
- A non-linear function at least when output is a probability

Logistic regression can be used for binary classification tasks.
It provides two outputs: one probability output for the first class and one for the second.
However, since one class probability already defines the other's probability, you can work with just one of the probabilities.
Nonetheless, having two classes is a special case of having $k$ classes.
From the SHAP perspective, classification is similar to regression, except the scale is the score output, not regression.

## The adult dataset

For the classification task, we will use the Adult dataset from the UCI repository.
The dataset contains demographic and socioeconomic data of individuals from the 1994 U.S. Census Bureau database, aiming to predict whether an individual's income is greater than or equal to \$50,000 per year.
The dataset includes features such as age, education level, work class, occupation, marital status, and more.
With approximately 32,000 observations, it contains both categorical and numerical features.
Fortunately, the `shap` package includes the adult dataset, simplifying its use in our example.


## Training the model

```{python}
import shap
from sklearn.model_selection import train_test_split

X, y = shap.datasets.adult()

X_train, X_test, y_train, y_test = train_test_split(
  X, y, test_size=0.2, random_state=1
)
```

In the next step, we train the model and compute the SHAP values.
Compared to the linear regression example, you will notice something new here:

```{python}
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
import numpy as np

# Define the categorical and numerical features
cats = ['Workclass', 'Marital Status', 'Occupation',
        'Relationship', 'Race', 'Sex', 'Country']
nums = ['Age', 'Education-Num', 'Capital Gain',
        'Capital Loss', 'Hours per week']

# Define the column transformer
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(), cats),
        ('num', StandardScaler(), nums)
    ])

# Define the pipeline
model = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', LogisticRegression(max_iter=10000))
])

# Fit the pipeline to the training data
model.fit(X_train, y_train)

X_sub = shap.sample(X_train, 100)

ex = shap.Explainer(model.predict_proba, X_sub)
shap_values = ex(X_test.iloc[0:100])
```

The novelties: 

- We only use a subset of the training data as background dataset. This can help with computation speed at the cost of less accurate SHAP value estimates.
- We apply SHAP not to the model, but to the entire pipeline. This allows us to compute SHAP values for the original features instead of their processed versions.

The adult dataset contains both categorical and numerical features.
Both are transformed before inputting them into the logistic regression model:

- Numerical features are standardized: $x_{j,std}^{(i)} = (x_j^{(i)} - \bar{x}_j) / sd(x_j)$,
- Categorical features are one-hot encoded: A feature with 1 column and 3 categories becomes 3 columns. e.g. category "3" may be encoded as (0,0,1).

After these two steps, our dataset has approximately 470 columns.
The numerical features, such as age, are no longer on an easily interpretable scale, so we would need to calculate what age 0.8 represents, for example.

However, the logistic regression model now works with these inputs.
This means that the coefficients are based on this transformed dataset.
If we were to use SHAP values directly on the logistic regression model, we would get 470 SHAP values.

Fortunately, there's a more interpretable approach:
We can combine the preprocessing and logistic regression into a pipeline and treat it as our model.
This procedure is similar to nesting mathematical functions:

- Our model is $y = f(\tilde{x})$, where $\tilde{x}$ is the preprocessed data.
- We have our preprocessing steps, which we'll call $g$, and we can express the preprocessed data as $\tilde{x} = g(x)$.
- The less desirable option would be to use SHAP values on $f$.
- The better option is to define a new function $\tilde{f}(x) = f(g(x))$, where the input is $x$ and not $\tilde{x}$. $\tilde{f}$ represent the pipeline.
- We apply and interpret SHAP values for $\tilde{f}$ instead of $f$.

This allows us to interpret the features in their original form.

::: {.callout-tip}

When preprocessing your data, consider which preprocessing steps you want to include as part of your pipeline when computing SHAP values.
In cases such as feature standardization it makes sense to include them in the pipeline, while transformations that increase interpretability should be outside.

:::

Another notable aspect is that the model's output is 2-dimensional instead of 1-dimensional.
This is reflected in the resulting `shap_values` variable, which gain another dimension.
Therefore we have to also indicate the class for which we want to access the SHAP values.

```{python}
class_index = 0
data_index = 1

shap.plots.waterfall(shap_values[data_index,:,class_index])
```

For this individual, the predicted likelihood of earning more than \$50k was 99.9% which is more than the expected 79%.
This plot reveals that the most influential feature was Marital Status (4 = married), which contributed 0.06.
The interpretation is mostly the same as for regression, only that the outcome is on the probability level and that we have to decide for which class we want to interpret the SHAP values.

Let's examine the SHAP values for the alternative class.

```{python}
class_index = 1
shap.plots.waterfall(shap_values[data_index,:,class_index])
```

This plot is identical to the previous one, except all SHAP values are multiplied by -1.
This makes sense since the probabilities for both classes must sum to 1.
Thus, a factor that increases the classification by 0.11 for class >50k decreases the classification by 0.11 for class <=50k.
We only need to choose one of the two classes.
This changes when we have three or more classes, see the [Image Chapter](#image) for a multi-class example.


## Alternatives to the waterfall plot

The waterfall plot visualizes the SHAP values of a data instances.
But there are more options to visualize the exact same type of information: the bar plot and the force plot.
If I had to put the 3 plots on a spectrum, I would say that the bar plot is the most conventional which people might already know, the comes the waterfall plot and then the force plot which is more difficult to read.

Let's start with the bar plot.

```{python}
shap.plots.bar(shap_values[data_index,:,class_index])
```

I will skip the interpretation here, since it's exactly the same as the waterfall plot.
The only difference in the two plots is how the information is arranged, with the bar plot lacking the information of $\mathbb{E}(f(X))$ and $f(x^{(i)}$.

Also the force plot is just a different arrangement of the SHAP values:

```{python}
#| eval: False
shap.initjs()
shap.plots.force(shap_values[data_index,:,class_index])
```

![Force Plot](images/force-plot.png)

The force plot is interactive and based on Javascrip.
You can hover over the plot to reveal more insights.
Well, not in this static format you are currently reading, but if you create one yourself and embed it, for example, in a Jupyter notebook or website.
What you see above is a screenshot of a force plot.
The plot is called force plot because the SHAP values are plotted as forces, here arrows, that can either push to increase the prediction or decrease it.
If you compare it to the waterfall plot, it's like a horizontal arrangement of the arrows.

Personally, I prefer the waterfall plot since it's easier to read than the force plot and it contains more information than the bar plot.

## Interpreting log odds

For logistic regression, it's common to interpret the model in terms of log odds rather than probability.

To do this, the explainer has a `link` argument, which defaults to the identity link $l(x) = x$.
A useful choice for classification is the logit link: $l(x) = log\left(\frac{x}{1 - x}\right)$.
We can use this logit link to transform the output of the logistic regression model and compute SHAP values on this new scale: 

```{python}
ex_logit = shap.Explainer(
  model.predict_proba, X_sub, link=shap.links.logit
)
sv_logit = ex_logit(X_test.iloc[0:100])
class_index = 0

shap.plots.waterfall(sv_logit[data_index,:,class_index])
```

When the outcome of a logistic regression model is defined in terms of log odds, then the features affect this linearly.
In other words, the logistic regression model is a linear model on the level of the log odds. 

What it means for the interpretation:
A marital status of 0 (married) contributes +1.47 to the log odds of >50k versus <=50k compared to the average prediction.
But one of SHAP values' strengths is its applicability at the probability level and log o odds can be difficult to interpret.
When should you use log odds and when probabilities?

::: {.callout-note}

If you're concerned with the probability outcome, use the identity link (which is the default behavior).
The logit space is more appropriate if you're interested in "evidence" in an information-theoretic sense, even if the effect in probability space isn't substantial.

:::

Let's dive a bit deeper into where the distinction between log odds an probabilities matter:
A shift from 80% to 90% is large in probability space, while a change from 98% to 99.9% is relatively small.

In probability space, the differences are 0.10 versus 0.019.
In logit space, we have:

- $log(0.9/0.1) - log(0.8/0.2) \approx 0.8$ and
- $log(0.999/0.001) - log(0.98/0.02) \approx 3$

In logit space, the step is significantly larger.
This occurs because the logit compresses near 0 and 1, causing changes in the middle of probability space to appear larger.

So, which one should you choose?
If you mostly care about probabilities, and a jump from 50% to 51% is as important to you as from 99% to 100%, opt for the default and use the identity link.
However, if changes in extreme probabilities near 0 and 1 are more crucial for the application, go with logits.
You can also see the difference in step sizes visualized in the following @fig-logits.

![Probabilities versus Logits](images/logits.jpg){#fig-logits}

## Understanding the data globally

To finish up the model interpretation, let's have a look at the global importances and effects with the summary plot.
Here we interpret the model in the probability space again.

```{python}
shap.plots.beeswarm(shap_values[:,:,class_index])
```

We observe that Marital Status and Education are the two most important features (on the log odds scale).
For some individuals, Capital Gain has large effects, indicating that high values of capital gain lead to small SHAP values.


## Clustering SHAP values 

You can cluster your data using SHAP values.
The goal of clustering is to find groups of similar instances.
Typically, clustering is based on features, which are often on different scales.
For example, height might be measured in meters, color intensity from 0 to 100, and some sensor output between -1 and 1.
The challenge is to compute distances between instances with such diverse, non-comparable features.

SHAP clustering works by clustering the SHAP values of each instance, meaning that you cluster instances by explanation similarity.
All SHAP values have the same unit -- the unit of the prediction space.
You can use any clustering method.
The following example uses hierarchical agglomerative clustering to order the instances.

The plot consists of many force plots, each of which explains the prediction of an instance.
We rotate the force plots vertically and place them side by side according to their clustering similarity.
It's again a Javascript plot.
Here for the first 21 data points:

```{python}
#| eval: False
shap.plots.force(sv_logit[0:20:,:,0])
```


![Clustering Plot](images/clustering-plot-1.png)

::: {.callout-note}

## Cluster plot

- The cluster plot is a collection of vertical force plots
- Data instances are spread across the x-axis, the SHAP values across the y-axis
- Color indicates the direction of SHAP values
- The large the area for a feature, the larger the SHAP values across the data (and the more important this feature was).
- By default, the data instances are ordered by their similarity in SHAP values

:::

You can also hover and get more information, change what you see on the x-axis and try many other orderings.
The cluster plot is exploratory tool.

You can also change the ordering, for example by the prediction:

![Clustering Plot](images/clustering-plot-2.png)


## The heatmap plot

The heatmap plot is another global interpretation plot.
The heatmap plot shows all SHAP values, but without aggregating them like in the dependence or importance plot.

```{python}
shap.plots.heatmap(sv_logit[:,:,class_index])
```

::: {.callout-note}

## Heatmap plot

- One row on the y-axis per feature and instances are spread across the x-axis
- Color indicates the SHAP value
- Instances are ordered based on clustered SHAP values
- The curve on top shows the predicted value for the data

:::

Some of the features are automatically summarized, a plotting behavior which you can control by setting the `max_display` argument.

[^no-probability]: While the output is a number between 0 and 1, classifiers are often not so well calibrated, so be careful with interpreting the output as a probability in the real world.
