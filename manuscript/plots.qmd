# SHAP Plots {#plots}

This chapter provides an overview of the plots available for aggregated Shapley values.
Aggregated refers to computing Shapley values for all features and data points in a dataset.
Thus, you will only find plots for tabular data, as there is no meaningful aggregation across images or texts.

This chapter covers:

- Local plots
  - Waterfall plot
  - Force plot
- Global plots
  - Importance plot
  - Summary plot
  - Dependence plot
  - Interaction plot
  - Clustering plot

## Waterfall Plot

![Waterfall plot](images/waterfall.jpg)

The `shap.plots.waterfall` reveals various insights:

- The y-axis displays the individual features, including the values for the first instance in our dataset
- The x-axis is on the scale of the output
- Each bar represents the Shapley value for that specific feature value
- Positive Shapley values point to the right
- The x-axis also indicates the overall expected prediction $E[f(X)]$ and the actual prediction of the instance $f(x)$
- The bars start at the bottom from the expected prediction and add up to the actual prediction

## Force Plot

![Force Plot](images/force-plot-single.jpg)

The `shap.plots.force` contains the same information as a waterfall plot, just that the arrows are arranged into the same horizontal level.
Requires javascript, meaning you can embed it, e.g., into a Jupyter notebook, but not in a PDF file.

## Bar Plot

The `shap.plots.bar` is yet another visualization of the Shapley values for individual predictions.
Rearranges the Shapley values into a bar plot, but I would rather use the waterfall plot.

## Feature Importance Plot

The concept behind SHAP feature importance is straightforward:
Features with large absolute Shapley values are important.
To determine global importance, we average the **absolute** Shapley values per feature across the data:

$$I_j=\frac{1}{n}\sum_{i=1}^n{}|\phi_j^{(i)}|$$

Next, we sort the features by decreasing importance and plot them.
The following figure displays the SHAP feature importance for the random forest previously trained to predict cervical cancer.

![Importance plot](images/shap-importance.jpg)


SHAP feature importance offers an alternative to [permutation feature importance](#feature-importance).
A significant difference exists between these importance measures:
Permutation feature importance is based on the decrease in model performance, while SHAP is based on the magnitude of feature attributions.

Although the feature importance plot is useful, it provides no information beyond the importances.
For a more informative plot, we will examine the summary plot next.

## Summary Plot

The summary plot combines feature importance and feature effects.
Each point on the plot represents a Shapley value for a feature and an instance.
The y-axis position is determined by the feature, and the x-axis position by the Shapley value.
Color indicates the feature value, ranging from low to high.
Overlapping points are jittered in the y-axis direction to show the distribution of Shapley values per feature.
Features are ordered by their importance.

![Summary Plot](images/summary-plot.jpg)

The summary plot provides initial insights into the relationship between a feature's value and its impact on the prediction.
However, to understand the exact nature of this relationship, we must examine SHAP dependence plots.

## Dependence Plot

SHAP feature dependence is perhaps the simplest global interpretation plot:
1) Select a feature.
2) For each data instance, plot a point with the feature value on the x-axis and the corresponding Shapley value on the y-axis.
3) Done.

Mathematically, the plot contains the following points: $\{(x_j^{(i)},\phi_j^{(i)})\}_{i=1}^n$

The following figure displays the SHAP feature dependence for years on hormonal contraceptives:

![Dependence Plot](images/dependence.jpg)

SHAP dependence plots offer an alternative to [partial dependence plots](#pdp) and [accumulated local effects](#ale).
While PDP and ALE plots display average effects, SHAP dependence plots also reveal variance on the y-axis.
This is particularly noticeable in cases of interactions, where the SHAP dependence plot exhibits greater dispersion on the y-axis.
Enhancing the dependence plot by highlighting feature interactions can improve its effectiveness.

Essentially, a dependence plot is a summary plot for a single feature where, instead of using color to represent the feature value, the values are distributed across the x-axis.

## Interaction Plot

Interaction effects represent the combined feature effects after accounting for individual feature effects.
The Shapley interaction index, derived from game theory, is defined as:

$$\phi_{i,j}=\sum_{S\subseteq\backslash\{i,j\}}\frac{|S|!(M-|S|-2)!}{2(M-1)!}\delta_{ij}(S)$$

when $i\neq{}j$ and:

$$\delta_{ij}(S)=\hat{f}_x(S\cup\{i,j\})-\hat{f}_x(S\cup\{i\})-\hat{f}_x(S\cup\{j\})+\hat{f}_x(S)$$

This formula removes the main effects of the features, resulting in the pure interaction effect after accounting for individual effects.
We then average the values over all possible feature coalitions S, similar to the Shapley value computation.
By calculating the SHAP interaction values for all features, we obtain one matrix per instance with dimensions M x M, where M represents the number of features.

How can we utilize the interaction index?
One example is to automatically color the SHAP feature dependence plot according to the strongest interaction:

![Interaction Plot](images/dependence-interaction.jpg)


## Clustering Plot

You can cluster your data using Shapley values.
The goal of clustering is to find groups of similar instances.
Typically, clustering is based on features, which are often on different scales.
For example, height might be measured in meters, color intensity from 0 to 100, and some sensor output between -1 and 1.
The challenge is to compute distances between instances with such diverse, non-comparable features.

SHAP clustering works by clustering the Shapley values of each instance, meaning that you cluster instances by explanation similarity.
All SHAP values have the same unit -- the unit of the prediction space.
You can use any clustering method.
The following example uses hierarchical agglomerative clustering to order the instances.

The plot consists of many force plots, each of which explains the prediction of an instance.
We rotate the force plots vertically and place them side by side according to their clustering similarity.

![Clustering Plot](images/force-plot-multiple.jpg)

