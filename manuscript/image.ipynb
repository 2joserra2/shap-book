{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Image classification\n",
        "\n",
        "All of the examples before have been tabular data.\n",
        "Now it's time to try a different type of data: image data.\n",
        "\n",
        "So the general setup here is:\n",
        "\n",
        "- Input is image data\n",
        "- Output is a score, which can also be multi-dimensional as in multi-class classification\n",
        "\n",
        "Image classification is a common task and it's commonly solved with deep learning.\n",
        "Give an image to the model, get a class back, usually based on what's visible on the image.\n",
        "\n",
        "We aren't going to train our own image classifier, but instead will load a ResNet model (TODO:CITE) which was trained on Imagenet data.\n",
        "\n",
        "The ImageNet task is a large-scale image classification challenge that involves recognizing and categorizing objects within digital images. The challenge uses a dataset of over 1 million images, each of which belongs to one of 1000 different object categories. The task is to develop a machine learning model that can accurately classify each image into its correct category.\n",
        "\n",
        "The ImageNet challenge has been an important driver of progress in the field of computer vision and deep learning, and has led to the development of new and more accurate machine learning models. The challenge has also spurred research into related computer vision tasks such as object detection and image segmentation.\n",
        "\n",
        "Code based on [shap notebook](https://github.com/slundberg/shap/blob/master/notebooks/image_examples/image_classification/Explain%20ResNet50%20using%20the%20Partition%20explainer.ipynb)\n",
        "\n",
        "We will use a pre-trained Resnet in tensorflow.\n"
      ],
      "id": "28158ac6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "import shap"
      ],
      "id": "88b3a9ff",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = ResNet50(weights='imagenet')\n",
        "X, y = shap.datasets.imagenet50()"
      ],
      "id": "9fdeeb57",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So these are 50 images\n",
        "\n",
        "We also need the class names for them:\n"
      ],
      "id": "4d6ef9cc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import json\n",
        "import os\n",
        "import urllib.request\n",
        "\n",
        "# Path to the JSON file on disk (change this to the desired location)\n",
        "json_file_path = 'imagenet_class_index.json'\n",
        "\n",
        "# Check if the JSON file exists on disk\n",
        "if os.path.exists(json_file_path):\n",
        "    with open(json_file_path) as file:\n",
        "        class_names = [v[1] for v in json.load(file).values()]\n",
        "else:\n",
        "    url = 'https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json'\n",
        "    with urllib.request.urlopen(url) as response:\n",
        "        json_data = response.read().decode()\n",
        "    with open(json_file_path, 'w') as file:\n",
        "        file.write(json_data)\n",
        "    class_names = [v[1] for v in json.loads(json_data).values()]"
      ],
      "id": "b13c197e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# wrap the model\n",
        "def predict(x):\n",
        "    tmp = x.copy()\n",
        "    preprocess_input(tmp)\n",
        "    return model(tmp)"
      ],
      "id": "006add76",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Seems like the network \"thinks\" this image shows a whippet.\n",
        "But what's the explanation for this classification?\n",
        "\n",
        "To answer this, we will use Shapley values and explain the image classification.\n",
        "\n",
        "## SHAP for image classification\n",
        "\n",
        "To create a shap explanation, we need three things:\n",
        "\n",
        "- The prediction function, which wraps the model.\n",
        "- The masker, which is also a function\n",
        "- The class names \n",
        "\n",
        "Then we are finally ready to estimate the Shapley values.\n"
      ],
      "id": "802f0a59"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Number of top classes for which to compute SHAP values\n",
        "topk = 3\n",
        "\n",
        "index = [21,28]\n",
        "\n",
        "# The masker blurs out parts of the image \n",
        "masker = shap.maskers.Image(\n",
        "  \"blur(128,128)\", shape = X[0].shape\n",
        ")\n",
        "\n",
        "explainer = shap.Explainer(\n",
        "  predict, masker, output_names=class_names\n",
        ")\n",
        "\n",
        "shap_values = explainer(\n",
        "  # TODO: Set to 1000\n",
        "  X[index], max_evals=1000,\n",
        "  batch_size = 50,\n",
        "  outputs=shap.Explanation.argsort.flip[:topk],\n",
        "  silent=True\n",
        ")"
      ],
      "id": "b191212b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The shap values are best visualized by laying them over the original image:\n"
      ],
      "id": "afce5104"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "shap.image_plot(shap_values, pixel_values=X[index]/255)"
      ],
      "id": "6fd79504",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Interpretation\n",
        "<!-- Interpretation is for (eval = 1000) -->\n",
        "\n",
        "Let's begin with the burger.\n",
        "It's misclassified, since the top class is \"bottlecap\".\n",
        "Looking at the Shapley values, it's mostly the top of the burger that pushed the classification towards bottlecap.\n",
        "Some of the middle parts of the burger had a negative influence, but still the top part looked, for the Resnet, too much like a bottlecap.\n",
        "But right after that, the second class is cheeseburger, which is the right class.\n",
        "Here we see that mostly parts in the middle contributed towards this classification.\n",
        "\n",
        "The second image shows a pocket watch.\n",
        "There is, however, now pocket watch class in the 1000 classes of ImageNet.\n",
        "So chain might be an okayish classification.\n",
        "And we can see that the chain was identified for the right reasons, namely the chain part of the watch.\n",
        "\n",
        "A disadvantage of computing the Shapley values for multiple images here is that the color scale for the Shapley values which you see on the bottom is for all images.\n",
        "And for the watch there are larger values, so that the values for the cheeseburger are scaled closer to white.\n",
        "\n",
        "\n",
        "## Effect of Different Maskers\n",
        "\n",
        "The different maskers here are different so-called inpainting methods.\n",
        "The task of inpainting is that you have an image with a \"hole\" for which we don't know the image part.\n",
        "Inpainting then replaces this hole.\n",
        "So exactly what we need when we remove pixels for computing SHAP values.\n",
        "\n",
        "List of inpainters that are available in SHAP:\n",
        "\n",
        "- `inpaint_telea` Telea inpainting fills in the missing area with a weighted average of all neighboring pixels. Neighborhood depends on a radius.\n",
        "- `inpaint_ns` NS, which stands for Navier-Stokes and is based on fluid dynamics with partial differential equations.\n",
        "- `blur(16, 16)` Blurring, which depends on kernel size, which you can set as a user. The larger the value the further away pixels play a role for inpainting. Bluring is faster than inpainting.\n",
        "\n",
        "The shap package uses the `cv2` package for inpainting and blurring.\n",
        "\n",
        "Here is how this looks like:\n"
      ],
      "id": "a0606cf6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# define a masker that is used to mask out partitions of the input image. \n",
        "mask_names = [\"inpaint_telea\", \"inpaint_ns\", \"blur(128, 128)\", \"blur(16, 16)\"]\n",
        "masks = [shap.maskers.Image(m, shape = X[0].shape) for m in mask_names]\n",
        "\n",
        "# Create a numpy array of shape (224, 224, 3)\n",
        "arr = np.zeros((224 * 224 * 3), dtype=bool)\n",
        "# Set the upper half of the image to True\n",
        "arr[:75264] = True\n",
        "# Set the lower half of the image to False\n",
        "arr[75264:] = False\n",
        "\n",
        "# Assuming your numpy arrays are called \"arr1\", \"arr2\", \"arr3\", and \"arr4\"\n",
        "fig, axs = plt.subplots(2, 2)\n",
        "\n",
        "axs[0, 0].imshow(masks[0](x=X[21], mask=arr)[0][0]/255)\n",
        "axs[0, 1].imshow(masks[1](x=X[21], mask=arr)[0][0]/255)\n",
        "axs[1, 0].imshow(masks[2](x=X[21], mask=arr)[0][0]/255)\n",
        "axs[1, 1].imshow(masks[3](x=X[21], mask=arr)[0][0]/255)\n",
        "\n",
        "# Add text annotations to label the panels\n",
        "axs[0, 0].set_title(mask_names[0])\n",
        "axs[0, 1].set_title(mask_names[1])\n",
        "axs[1, 0].set_title(mask_names[2])\n",
        "axs[1, 1].set_title(mask_names[3])\n",
        "axs[0, 0].set_xticks([])\n",
        "axs[0, 1].set_xticks([])\n",
        "\n",
        "plt.show()"
      ],
      "id": "bfe01195",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's check them all out:\n"
      ],
      "id": "6fd77404"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# define a masker that is used to mask out partitions of the input image. \n",
        "mask_names = [\"inpaint_telea\", \"inpaint_ns\", \"blur(128, 128)\", \"blur(16, 16)\"]\n",
        "\n",
        "masks = [shap.maskers.Image(m, shape = X[0].shape) for m in mask_names]\n",
        "\n",
        "topk = 3\n",
        "\n",
        "# walk through all the masks\n",
        "for mask in masks:\n",
        "    print(mask)\n",
        "    explainer = shap.Explainer(predict, mask, output_names=class_names, silent=True)\n",
        "    # TODO: set eval to 1000\n",
        "    shap_values = explainer(X[[21]], max_evals=500, batch_size=50, outputs=shap.Explanation.argsort.flip[:topk])\n",
        "    shap.image_plot(shap_values, pixel_values=X[[21]]/255)"
      ],
      "id": "98bea277",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- based on 500 evals -->\n",
        "The choice of masker does make a bit of a difference here for explaining the cheeseburger classification.\n",
        "The wrong classification was that the cheeseburger was classified as a bottlecap.\n",
        "The pixels most responsible were teh ones on the top, that's what all maskers agreed upon, except the blur(16,16) masker, which found the pixels on the sides most relevant.\n",
        "In all cases, the pixels on the bottom of the burger seemed to speak against a bottlecap classification.\n",
        "\n",
        "TODO: Visualize the maskers, by visualizing an image?\n",
        "Should be doable with the masker directly\n",
        "\n",
        "### Effect of increasing the evaluation steps:\n",
        "\n",
        "Another hyperparameter for SHAP in image is the number of evaluations.\n",
        "The number of evaluations steers how fine-grained the explanations are.\n",
        "Since we use the Partition explainer, more evaluations allow more fine-grained partition and therefore more local superpixels.\n",
        "\n",
        "But let's see what happens when we increase the amount of evaluations, starting with a very low number of just 10 evaluations:\n"
      ],
      "id": "5a11eaf9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO: Add 10k\n",
        "evals = [10, 100, 1000, 5000]\n",
        "topk = 1\n",
        "\n",
        "masker = shap.maskers.Image(\n",
        "  \"blur(128,128)\", shape = X[[21]].shape\n",
        ")\n",
        "\n",
        "# walk through all the masks\n",
        "for ev in evals:\n",
        "    print(ev)\n",
        "    explainer = shap.Explainer(predict, mask, output_names=class_names, silent=True)\n",
        "    shap_values = explainer(X[[21]], max_evals=ev, batch_size=50, outputs=shap.Explanation.argsort.flip[:topk])\n",
        "    shap.image_plot(shap_values, pixel_values=X[[21]]/255)"
      ],
      "id": "d3584974",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What we can observe by increasing the number of evaluations:\n",
        "\n",
        "- We get more fine-grained superpixels, which has to do with the Partition explainer. Only 10 evaluations and we only get 4 leaves, which requires a tree of depth 2, and therefore 2 + 2 * 2 = 6 Shapley values (TODO:check Partition expllainer. \n",
        "- The Shapley values get smaller. Well, actually the partitions get smaller and therefore the number of pixels. And makes sense that the fewer pixels are masked, the less influence it has on the prediction.\n",
        "- Computation times scales linearly with the number of evaluations \n",
        "\n",
        "\n",
        "\n",
        "## Problems and TODOs:\n",
        "    \n",
        "- When changing the masker, the topk classes change, but they shouldn't?\n",
        "- \n"
      ],
      "id": "7c20736f"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}