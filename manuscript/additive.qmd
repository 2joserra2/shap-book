# Shapley Values for Additive Models

TODO: some ## headers

In this next step, we allow the relationship between a feature and the target to be non-linear.
However, we do not allow interactions between the features.
Additive models with non-linear base functions are perfectly modeled with so-called generalized additive models, also known as GAMs.

A GAM models y in the following way:

$$\hat{f}(x) = \beta_0 + \beta_1 f_1(x_1) + \ldots + f_p(x_p)$$

The difference from the simple linear model is that we allow the functions $f_j$. 
If these are the identity function for all features, namely $f_j(x_j) = x_j$, then we again arrive at the linear model.
So, linear regression models are special cases of GAMs.

But with GAMs, we can now have arbitrary functions for the features.
A popular choice is spline functions, which allow for rather flexible shapes of functions that are still smooth and have a gradient.

Educationally, to understand Shapley values, they are perfect because they allow us to study what happens with non-linear functions, but without interactions.
It is only with interactions that the interpretation becomes trickier.
So, we go back to the same wine example and fit a GAM instead of a linear regression model.

For this, we rely on the interpret Python library. You can install it with:

```{python}
#| eval: false
!pip install interpret
```

First, we fit a model.
It's an Explainable Boosting Regressor from the interpret package.

::: .{callout-note}

The Explainable Boosting Machine (EBM) is a tree-based GAM.
It's based on cyclic gradient boosting with automated interaction detection, which of course we won't use for this example.
So for our case, each tree can only use one feature, because otherwise, it would model interactions.

:::

This is the model we train:
```{python}
#| output: false
import pandas as pd
from sklearn.model_selection import train_test_split
from interpret.glassbox import ExplainableBoostingRegressor
wine = pd.read_csv("wine.csv")
y = wine["quality"]
X = wine.drop("quality", axis=1)
X_train, X_test, y_train, y_test = train_test_split(
  X, y, test_size=0.2, random_state=42
)

model = ExplainableBoostingRegressor(interactions=0)
model.fit(X_train, y_train)
```

Once again, let's evaluate how well the model predicts the test data:

```{python}
from sklearn.metrics import mean_absolute_error
y_pred = model.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)

# Print the mean absolute error
print(f"MAE: {mae:.2f}")
```

The mean absolute error on the test data is better than for the linear regression model.
That's great news.
It means that by using a GAM and allowing feature effects to be non-linear, we were able to improve the predictions and some effects actually appeared to be non-linear.

Now let's see what the SHAP values have to say about how the model works:

```{python}
#| output: false
import shap
explainer = shap.Explainer(model.predict, X_test)
shap_values = explainer(X_test)
```

The question now is: What do the Shapley values look like?
And do they match what we would expect?

First, let's explain the initial prediction:

```{python}
shap.waterfall_plot(shap_values[0], max_display=10)
```

::: {.callout-note}
A SHAP-specific note here: since the explainer is a Permutation explainer, we can directly use the shap_values as input to the waterfall plot, which we couldn't do with the Linear explainer.
:::
This waterfall plot presents a distinct picture of what's happening, compared to the purely linear model.

- The most important features were alcohol and volatile acidity, while in the linear model, it was residual sugar, followed by alcohol.
- Generally, the prediction for the GAM is lower in this case (around 5.97), compared to 6.28 for the linear case.
- This example beautifully demonstrates how, although the global average prediction and the local prediction are quite similar, there are numerous Shapley values canceling each other out.

We know that it's an additive model.
This means that to understand the effect a feature has, we can simply take one data point, modify that feature, and observe how this alters the prediction.
Since it's an additive model, this curve will be the same for all data points, only with different intercepts, as the rest of the additive effects (which we keep fixed) vary.

```{python}
# plot SHAP values against feature values
import matplotlib.pyplot as plt
import numpy as np
feature_name='alcohol'
feature_idx = np.where(X_test.columns==feature_name)
sv = shap_values.values[:, feature_idx]
plt.scatter(X_test[feature_name], sv, alpha=0.1)
plt.xlabel(feature_name)
plt.ylabel('SHAP value')
plt.show()
```

In the case of alcohol, we see a positive effect.
The higher the alcohol content, the higher the predicted quality.
However, at very high and very low levels, the effect plateaus.

The question now is whether and how the Shapley values for the alcohol feature will reflect this.

TODO: Figure out how to compute the differences in intercept between PDP and SHAP here.
```{python}
first_row = X.iloc[np.repeat(0, 100)]
x_range = np.linspace(X[feature_name].min(), X[feature_name].max(), num=100)
first_row[feature_name] = x_range
preds = model.predict(first_row)
```

```{python}
Xpred = model.predict(X_test).mean()
# plot SHAP values against feature values
sv_values = shap_values.values[:, feature_idx]
plt.scatter(X_test[feature_name], sv_values, alpha=0.2)
plt.xlabel(feature_name)
plt.ylabel('SHAP value')

x0pred = model.predict(X_test.iloc[[0]])

#plt.plot(feature_range, preds - x0pred, color='red')
plt.plot(x_range, preds, color='red')

plt.show()
```

Again, as you can see, the SHAP values follow the same trajectory as when we would simply change one of the features (here alcohol).
This gives us another boost in trust for understanding Shapley values.
There's also a paper [@bordt2022shapley] showing that when the model is a GAM, then the non-linear components can be recovered by SHAP.

But things get tricky when we have interactions.

And they get even trickier when we have interactions plus correlations.
However, the situation is still manageable (and other interpretation methods have to deal with the same difficulties).

Alright, now we allowed the model to have non-linear feature effects, but it still had to be additive.

In the next step, we will allow the model to have interactions and see how this affects the interpretation of the Shapley values.
