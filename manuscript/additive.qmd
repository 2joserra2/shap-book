## Shapley values for additive models

In this next step, we allow the relation between a feature and the target to be non-linear.
We don't, however, allow interactions between the features.
Additive models with non-linear base functions are perfectly modeled with so-called generalized additive models, also called GAMs.


A GAM models y in the following way:

$$\hat{f}(x) = \beta_0 + \beta_1 f_1(x_1) + \ldots + f_p(x_p)$$

The difference to the simple linear model is that we allow the functions $f_j$. If these are the identity function for all features, namely $f_j(x_j) = x_j$, then we again come out with the linear model.
So linear regression models are special cases of GAMs.

But with GAMs, we can now have arbitrary functions for the features.
A popular choice are spline functions which allow for rather flexible shapes of functions that are still smooth and have a gradient.

Educationally, to understand Shapley values, they are perfect, because they allow us to study what happens with non-linear functions, but without interactions.
Because only with interactions will the interpretation become trickier.
So we go back to the same wine example and fit a GAM instead of a linear regression model.

For this we rely on the interpret Python library. YOu can install it with:

```{python}
#| eval: false
!pip install interpret
```

First we fit a model.
It's an explainable boosting regressor from the interpet package

::: .{callout-note}

The Explainable Boosting Machine (EBM) is a tree-based GAM.
It's based on cylcic gradient boosting with automated interaction detection, which of course we won't use for this example.
So for our case each tree can only use one feature, because otherwise it would model interactions.

:::

This is the model we train:

```{python}
#| output: false
import pandas as pd
from sklearn.model_selection import train_test_split
from interpret.glassbox import ExplainableBoostingRegressor
wine = pd.read_csv("wine.csv")
y = wine["quality"]
X = wine.drop("quality", axis=1)
X_train, X_test, y_train, y_test = train_test_split(
  X, y, test_size=0.2, random_state=42
)

model = ExplainableBoostingRegressor(interactions=0)
model.fit(X_train, y_train)
```

Again, let's check out how good the model predicts the test data:

```{python}
from sklearn.metrics import mean_absolute_error
y_pred = model.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)

# Print the mean absolute error
print(f"MAE: {mae:.2f}")
```

The mean absolute error on test data is better than for the linear regression model.
That's great.
It means that by using a GAM and allowing feature effects to be non-linear, we could improve the predictions and some effects actually seemed to be non-linear.

So let's see what the SHAP values have to say about how the model works:

```{python}
#| output: false
import shap
explainer = shap.Explainer(model.predict, X_test)
shap_values = explainer(X_test)
```

The question now: How do the Shapley values look like?
And does it match what we would expect?

Let's first explain the first prediction again:


```{python}
shap.waterfall_plot(shap_values[0], max_display=10)
```

::: {.callout-note}

A shap-specific here: since the explainer is a Permutation explainer, we can directly use the shap_values as input to the waterfall plot, which we couldn't do with the Linear explainer.

:::

This waterfall plot gives us a rather different image of what's going on, compared to the purely linear model.

- The most important feature were alcohol and volatile acidity, and in the linear model case it  was residual sugar, followed by alcohol.
- In general, the prediction for the GAM is lower for this case (around 5.97) which was 6.28 for the linear case.
- This case beautifully shows how, while the global average prediction and the local prediction are quit similar, there is a lot of Shapley values cancelling each other out going on.
 

We know that it's an additive model.
That means to understand the effect a feature has, we can just take one of the data point and change that feature and observe how this changes the prediction.
Since it's an additive model, this will be the same curve for all data points, only with different intercepts, since the rest of the additive effects (which we keep fixed) are different.

```{python}
# plot SHAP values against feature values
import matplotlib.pyplot as plt
import numpy as np
feature_name='alcohol'
feature_idx = np.where(X_test.columns==feature_name)
sv = shap_values.values[:, feature_idx]
plt.scatter(X_test[feature_name], sv, alpha=0.1)
plt.xlabel(feature_name)
plt.ylabel('SHAP value')
plt.show()
```

In this case for alcohol we see that there is a positive effect.
The more alcohol, the higher the predicted quality.
However, at very high and very low levels, the effect is flat.


The question now is whether and how the Shapley values for the alcohol feature will reflect this.

TODO: Figure out how to compute the differences in intercept between PDP and SHAP here


```{python}
first_row = X.iloc[np.repeat(0, 100)]
x_range = np.linspace(X[feature_name].min(), X[feature_name].max(), num=100)
first_row[feature_name] = x_range
preds = model.predict(first_row)
```


```{python}
Xpred = model.predict(X_test).mean()
# plot SHAP values against feature values
sv_values = shap_values.values[:, feature_idx]
plt.scatter(X_test[feature_name], sv_values, alpha=0.2)
plt.xlabel(feature_name)
plt.ylabel('SHAP value')


x0pred = model.predict(X_test.iloc[[0]])

#plt.plot(feature_range, preds - x0pred, color='red')
plt.plot(x_range, preds, color='red')

plt.show()
```

Again, as you can see, the SHAP values follow the same trajectory as when we would simply change one of the features (here alcohol).
This gives us another boost in the trust of understanding Shapley values.

But things get tricky when we have interactions.

And they get really tricky when we have interactions + correlations.
But manageable (and other interpretation methods have to deal with the same difficulties)



Alright, now we allowed the model to have non-linear feature effects, but it still had to be additive.

In the next step, we will allow the model to have interactions and see how this affects the interpretation of the Shapley values.

