# SHAP Values for Additive Models {#additive}

In this next step, we allow the relationship between a feature and the target to be non-linear.
However, we do not allow interactions between the features.

## Introducing the generalized additive model (GAM)

Additive models with non-linear base functions are perfectly modeled with so-called generalized additive models, also known as GAMs.

A GAM models the target in the following way:

$$f(x) = \beta_0 + \beta_1 f_1(x_1) + \ldots + f_p(x_p)$$

The difference from the simple linear model is that we allow the functions $f_j$ to be non-linear. 
If these are the identity function for all features, namely $f_j(x_j) = x_j$, then we again arrive at the linear model.
So, linear regression models are special cases of GAMs.

But with GAMs, we can now have arbitrary functions for the features.
A popular choice is spline functions, which allow for rather flexible shapes of functions that are still smooth and have a gradient.

Educationally, to understand SHAP values, they are perfect because they allow us to study what happens with non-linear functions, but without interactions.
It is only with interactions that the interpretation becomes trickier.

## Fitting the GAM

So, we go back to the same wine example and fit a GAM instead of a linear regression model.

For this, we rely on the `interpret` Python library. You can install it with:

```{python}
#| eval: false
!pip install interpret
```

First, we fit a model.
It's an Explainable Boosting Regressor from the interpret package.

::: .{callout-note}

The Explainable Boosting Machine (EBM) is a tree-based GAM.
It's based on cyclic gradient boosting with automated interaction detection, which of course we won't use for this example.
So for our case, each tree in the ensemble can only use one feature, because otherwise, it would model interactions.

:::

This is the model we train:
```{python}
#| output: false
import pandas as pd
from sklearn.model_selection import train_test_split
from interpret.glassbox import ExplainableBoostingRegressor

wine = pd.read_csv("wine.csv")
y = wine["quality"]
X = wine.drop("quality", axis=1)
X_train, X_test, y_train, y_test = train_test_split(
  X, y, test_size=0.2, random_state=42
)

model = ExplainableBoostingRegressor(interactions=0)
model.fit(X_train, y_train)
```

Once again, let's evaluate how well the model predicts the test data:

```{python}
from sklearn.metrics import mean_absolute_error
y_pred = model.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)

# Print the mean absolute error
print(f"MAE: {mae:.2f}")
```

The mean absolute error on the test data is better than for the linear regression model.
That's great news.
It means that by using a GAM and allowing feature effects to be non-linear, we were able to improve the predictions and some effects actually appeared to be non-linear.

## Interpreting the GAM with SHAP

Now let's see what the SHAP values have to say about how the model works:

```{python}
#| output: false
import shap
explainer = shap.Explainer(model.predict, X_train)
shap_values = explainer(X_test)
```

The question now is: What do the SHAP values look like?
And do they match what we would expect?

First, let's explain the initial prediction of a data point:

```{python}
shap.waterfall_plot(shap_values[0], max_display=10)
```

::: {.callout-note}
A SHAP-specific note here: since the explainer is a Permutation explainer, we can directly use the `shap_values` as input to the waterfall plot, which we couldn't do with the Linear explainer.
:::
This waterfall plot presents a different picture of what's happening, compared to the purely linear model.

- For this wine, the most important features were alcohol and free sulfur dioxide, while in the linear model, it was residual sugar and free sulfur dioxide.
- Generally, the prediction for the GAM is lower in this case (around 6.0).
- This example beautifully demonstrates how, although the global average prediction and the local prediction are quite similar, there are numerous SHAP values canceling each other out.

## SHAP recover non-linear functions

We know that it's an additive model.
This means that to understand the effect a feature has, we can simply take one data point, modify that feature, and observe how this alters the prediction.
Since it's an additive model, this curve will be the same for all data points, only with different intercepts, as the rest of the additive effects (which we keep fixed) vary.

```{python}
# plot SHAP values against feature values
import matplotlib.pyplot as plt
import numpy as np
feature_name = 'alcohol'
feature_idx = np.where(X_test.columns == feature_name)
sv = shap_values.values[:, feature_idx]
plt.scatter(X_test[feature_name], sv, alpha=0.1)
plt.xlabel(feature_name)
plt.ylabel('SHAP value')
plt.show()
```

In the case of alcohol, we see a positive effect.
The higher the alcohol content, the higher the predicted quality.
However, at very high and very low levels, the effect plateaus.

Let's compare this now with the raw predictions.
Well, we have to accommodate for the fact that data points with different alcohol levels also differ in other features.
But we can make use of a property of the GAM: Since the alcohol is modeled without interactions with other features, changing the alcohol has the same effect no matter the other features.
That means we can draw a line for one data point and just change the alcohol level.
Like this we get the effect of alcohol which looks the same for all data points (only with different intercepts).

```{python}
first_row = X.iloc[np.repeat(0, 100)]
x_range = np.linspace(
  X[feature_name].min(), X[feature_name].max(), num=100
)
first_row.loc[:, feature_name] = x_range
preds = model.predict(first_row)

Xpred = model.predict(X_test).mean()
sv_values = shap_values.values[:, feature_idx]
plt.scatter(
  X_test[feature_name], sv_values - np.mean(sv_values), alpha=0.2
)
plt.xlabel(feature_name)
plt.ylabel('SHAP value')
plt.plot(x_range, preds - np.mean(preds), color='red')
plt.show()
```

Again, as you can see, the SHAP values follow the same trajectory as when we would simply change one of the features (here alcohol).
This gives us another boost in trust for understanding SHAP values.
There's also a paper [@bordt2022shapley] showing that when the model is a GAM, then the non-linear components can be recovered by SHAP.

Similarly to the linear case, in the additive case, SHAP values track the feature effect well and are aligned to what we would expect.


