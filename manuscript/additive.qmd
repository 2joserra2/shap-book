# SHAP Values for Additive Models {#additive}

In this next step, we allow the relationship between a feature and the target to be non-linear.
However, we do not allow interactions between the features.

## Introducing the generalized additive model (GAM)

Additive models with non-linear base functions are perfectly modeled with so-called generalized additive models, also known as GAMs.

A GAM models the target in the following way:

$$f(x) = \beta_0 + \beta_1 f_1(x_1) + \ldots + f_p(x_p)$$

The difference from the simple linear model is that we allow the functions $f_j$ to be non-linear. 
If these are the identity function for all features, namely $f_j(x_j) = x_j$, then we again arrive at the linear model.
So, linear regression models are special cases of GAMs.

But with GAMs, we can now have arbitrary functions for the features.
A popular choice is spline functions, which allow for rather flexible shapes of functions that are still smooth and have a gradient.
But it's also possible to use tree-based basis function, which have a fast implementation, as we do in this example.
Additive models are a good way to further expand our understanding of SHAP values, since it allows us to study what happens with non-linear functions, but without interactions.
While we could add interaction terms to a GAM, we won't for this chapter.
With interactions, the interpretation becomes trickier.

## Fitting the GAM

So, we go back to the same wine example and fit a GAM instead of a linear regression model.

For this, we rely on the `interpret` Python library. You can install it with:

```{python}
#| eval: false
!pip install interpret
```

First, we fit a model.
It's an Explainable Boosting Regressor from the interpret package.

::: .{callout-note}

The Explainable Boosting Machine (EBM) is a tree-based GAM.
It has (optional) automated interaction detection, which we won't use for this example.
So for our case, each tree in the ensemble can only use one feature, because otherwise, it would model interactions.

:::

This is the model we train:
```{python}
#| output: false
import pandas as pd
from sklearn.model_selection import train_test_split
from interpret.glassbox import ExplainableBoostingRegressor

wine = pd.read_csv("wine.csv")
y = wine["quality"]
X = wine.drop("quality", axis=1)
X_train, X_test, y_train, y_test = train_test_split(
  X, y, test_size=0.2, random_state=42
)

model = ExplainableBoostingRegressor(interactions=0)
model.fit(X_train, y_train)
```

Once again, let's evaluate how well the model predicts the test data:

```{python}
from sklearn.metrics import mean_absolute_error
y_pred = model.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)

# Print the mean absolute error
print(f"MAE: {mae:.2f}")
```

The mean absolute error on the test data is better than for the linear regression model.
That's great news.
It means that by using a GAM and allowing feature effects to be non-linear, we were able to improve the predictions and some effects actually appeared to be non-linear.

## Interpreting the GAM with SHAP

Now let's see what the SHAP values have to say about how the model works:

```{python}
#| output: false
import shap
explainer = shap.Explainer(model.predict, X_train)
shap_values = explainer(X_test)
```

The question now is: What do the SHAP values look like?
And do they match what we would expect?

First, let's explain the initial prediction of a data point:

```{python}
shap.waterfall_plot(shap_values[0], max_display=10)
```

::: {.callout-note}
A SHAP-specific note here: the explainer here is a Permutation explainer. But why isn't the Additive estimation method used here? Because in `shap` the additive estimation is only implemented for `interpret.glassbox.ExplainableBoostingClassifier`, but we are using the Regressor.

And there is a difference to the results from the Linear explainer: we can directly use the `shap_values` as input to the waterfall plot.

:::

This waterfall plot presents a different picture of what's happening, compared to the purely linear model.

- For this wine, the most important features were alcohol and free sulfur dioxide, while in the linear model, it was residual sugar and free sulfur dioxide.
- Generally, the prediction for the GAM is lower in this case (around 6.0).
- This example beautifully demonstrates how, although the global average prediction and the local prediction are quite similar, there are numerous SHAP values canceling each other out.

## SHAP recovers non-linear functions

We know that it's an additive model.
This means that to understand the effect a feature has, we can simply take one data point, modify that feature, and observe how this alters the prediction.
Since it's an additive model, this curve will be the same for all data points, only with different intercepts, as the rest of the additive effects (which we keep fixed) vary.

```{python}
# plot SHAP values against feature values
import matplotlib.pyplot as plt
import numpy as np
feature_name = 'alcohol'
feature_idx = np.where(X_test.columns == feature_name)
sv = shap_values.values[:, feature_idx]
plt.scatter(X_test[feature_name], sv, alpha=0.1)
plt.xlabel(feature_name)
plt.ylabel('SHAP value')
plt.show()
```

In the case of alcohol, we see a positive effect.
The higher the alcohol content, the higher the predicted quality.
However, at very high and very low levels, the effect plateaus.

Let's compare the SHAP values with the learned function for alcohol from the GAM.
Since the tree-based GAM is a purely additive model, we can visualize the full effect of alcohol on the prediction by extracting this functional part from the GAM.

```{python}
feature_name = 'alcohol'
feature_idx = np.where(X_test.columns == feature_name)
sv = shap_values.values[:, feature_idx]
plt.scatter(X_test[feature_name], sv, alpha=0.1)
plt.xlabel(feature_name)
plt.ylabel('SHAP value')

# First get the index of the alcohol feature
idx = model.explain_global().data()['names'].index('alcohol')
# extract the relevant data from the tree-based GAM 
explain_data = model.explain_global().data(idx)
# the alcohol feature values 
x_data = explain_data["names"]
# the part of the prediction function for  alcohol
y_data = explain_data["scores"]
y_data = np.r_[y_data, y_data[np.newaxis, -1]]
plt.plot(x_data, y_data, color='red')
plt.show()
```

Again, as you can see, the SHAP values follow the same trajectory as when we would simply change one of the features (here alcohol).
This gives us another boost in trust for understanding SHAP values.
There's also a paper [@bordt2022shapley] showing that when the model is a GAM, then the non-linear components can be recovered by SHAP.

Similarly to the linear case, in the additive case, SHAP values track the feature effect well and are aligned to what we would expect.

## Analyzing the feature importance

There is another type of plot that `shap` offers: the importance plot.
The concept behind SHAP feature importance is straightforward:
Features with large absolute SHAP values are important.
To determine global importance, we average the **absolute** SHAP values per feature across the data:

$$I_j=\frac{1}{n}\sum_{i=1}^n{}|\phi^{(i)}_j^{(i)}|$$

Next, we sort the features by decreasing importance and plot them.
The following figure displays the SHAP feature importance for the random forest previously trained to predict cervical cancer.
This, by the way, is the same way that the summary plot sorts the features. 

```{python}
shap.plots.bar(shap_values)
```


SHAP feature importance offers an alternative to [permutation feature importance](https://christophm.github.io/interpretable-ml-book/feature-importance.html).
A significant difference exists between these importance measures:

::: {.callout-note}

Permutation feature importance (PFI) is based on the decrease in model performance, while SHAP is based on the magnitude of feature attributions.
This difference is especially stark if the model is overfitting, because a feature that in reality doesn't have a relation to the target will have an expected PFI of zero, but may have a non-zero SHAP importance.

:::

Although the feature importance plot is useful, it provides no information beyond the importances.
For a more informative plot, we will examine the summary plot next.




