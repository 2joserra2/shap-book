# SHAP Values for Additive Models {#additive}

::: {.callout-tip appearance="simple"}

By the end of this chapter, you will be able to:

- Use SHAP for additive regression models.
- Interpret non-linear effects.
- Explain SHAP feature importance.

:::

In this section, we introduce the concept of non-linear relationships between a feature and the target, while excluding interactions between the features.

## Introducing the generalized additive model (GAM)

Generalized additive models (GAMs) are perfect for modeling purely additive effects using non-linear base functions.

A GAM models the target as follows:

$$f(x) = \beta_0 + \beta_1 f_1(x_1) + \ldots + f_p(x_p)$$

Unlike the simple linear model, we allow the functions $f_j$ to be non-linear. 
If for all features, $f_j(x_j) = x_j$, we arrive at the linear model.
Thus, linear regression models are special cases of GAMs.

With GAMs, we can use arbitrary functions for the features.
Popular choices include spline functions, which allow for flexible, smooth functions with a gradient.
Tree-based basis functions, which have a fast implementation, are also an option.
Additive models expand our understanding of SHAP values, as they allow us to examine non-linear functions without interactions.
Although we could add interaction terms to a GAM, we will not do so in this chapter, as the interpretation becomes more complex.

## Fitting the GAM

We return to the wine example and fit a GAM instead of a linear regression model.
We'll use the `interpret` Python library for this. You can install it with:

```{python}
#| eval: false
pip install interpret
```

Next, we fit a model.
We're using the Explainable Boosting Regressor from the interpret package.
The Explainable Boosting Machine (EBM) is a tree-based GAM.
It offers optional automated interaction detection, which we won't use in this example.
In our case, each tree in the ensemble can only use one feature to avoid modeling interactions.

Here we train the model:

```{python}
#| output: false
import pandas as pd
from sklearn.model_selection import train_test_split
from interpret.glassbox import ExplainableBoostingRegressor

wine = pd.read_csv('wine.csv')
y = wine['quality']
X = wine.drop('quality', axis=1)
X_train, X_test, y_train, y_test = train_test_split(
  X, y, test_size=0.2, random_state=42
)

model = ExplainableBoostingRegressor(interactions=0)
model.fit(X_train, y_train)
```

Let's evaluate the model's predictions on the test data:

```{python}
from sklearn.metrics import mean_absolute_error

y_pred = model.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)
print(f"MAE: {mae:.2f}")
```

The mean absolute error on the test data is less than that of the linear regression model.
This is promising as it indicates that by using a GAM and allowing non-linear feature effects, we improved predictions.
The increase in performance indicates that some relations between wine quality and features are non-linear.

## Interpreting the GAM with SHAP

Now let's see what the SHAP values have to say about how the model works:

```{python}
#| output: false
import shap
explainer = shap.Explainer(model.predict, X_train)
shap_values = explainer(X_test)
```

The explainer used here is a Permutation explainer.
The Additive explainer method isn't used because the additive estimation in `shap` is only implemented for `interpret.glassbox.ExplainableBoostingClassifier`, and we are using the Regressor.

The question now is: How do we interpret the SHAP values?
And do they align with our expectations?
First, let's elucidate the initial prediction of a data point:

```{python}
shap.plots.waterfall(shap_values[0], max_display=10)
```

This waterfall plot provides a different perspective than the purely linear model.

- For this particular wine, the most important features were alcohol and free sulfur dioxide, whereas, in the linear model, they were residual sugar and free sulfur dioxide.
- The quality predicted by the GAM is approximately 6.0, lower than the 6.4 predicted by the linear model. 
- This example clearly illustrates how the global average prediction and the local prediction can be similar, but numerous SHAP values cancel each other out.

## SHAP recovers non-linear functions

A GAM is an additive model, which implies that we can inspect each feature in isolation to understand its effect without considering interaction effects.
Let's examine the SHAP dependence plot for alcohol:

```{python}
shap.plots.scatter(shap_values[:,"alcohol"])
```


In the case of alcohol, there is a positive relationship between alcohol levels and the SHAP values.
The SHAP contribution increases with the alcohol content, but it plateaus at extremely high and low levels.

Let's compare these SHAP values with the alcohol effect learned by the GAM.
We can plot the SHAP values and overlay the alcohol curve extracted directly from the GAM.

```{python}
import matplotlib.pyplot as plt
import numpy as np

shap.plots.scatter(shap_values[:,"alcohol"], show=False)

# First get the index of the alcohol feature
idx = model.explain_global().data()['names'].index('alcohol')
# extract the relevant data from the tree-based GAM 
explain_data = model.explain_global().data(idx)
# the alcohol feature values 
x_data = explain_data["names"]
# the part of the prediction function for  alcohol
y_data = explain_data["scores"]
y_data = np.r_[y_data, y_data[np.newaxis, -1]]
plt.plot(x_data, y_data, color='red')
plt.show()
```

As evident, the SHAP values follow the same trajectory as we would see when simply altering one of the features (here, alcohol).
This reinforces our confidence in understanding SHAP values.
There's a paper [@bordt2022shapley] that demonstrates that when the model is a GAM, the non-linear components can be recovered by SHAP.
Like the linear case, in the additive case, SHAP values accurately track the feature effect and align with what we would expect.

## Analyzing feature importance

`shap` offers another type of plot: the importance plot.
The principle behind SHAP feature importance is simple:
Features with large absolute SHAP values are important.
To determine global importance, we average the **absolute** SHAP values per feature across the data:

$$I_j=\frac{1}{n}\sum_{i=1}^n |\phi^{(i)}_j|$$

We then sort the features by decreasing importance and plot them.
This method of sorting features is also used in the summary plot. 

```{python}
shap.plots.bar(shap_values)
```

SHAP feature importance provides an alternative to [permutation feature importance](https://christophm.github.io/interpretable-ml-book/feature-importance.html).
There's a significant difference between these importance measures:

::: {.callout-note}
Permutation Feature Importance (PFI) is derived from the decline in model performance, whereas SHAP relies on the magnitude of feature attributions.
This difference becomes particularly pronounced when the model is overfitting. A feature that doesn't actually correlate with the target will have an expected PFI of zero but may exhibit a non-zero SHAP importance.

:::

