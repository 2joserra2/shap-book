# Going from Shapley values to SHAP


::: {.callout-tip appearance="simple"}

By the end of this chapter, you will be able to:

- Describe a prediction as a collaborative game
- Explain how SHAP values are defined
- Interpret Shapley value axioms in terms of machine learning predictions

:::


We have been on a deep dive into Shapley values from coalitional game theory.
But how do Shapley values connect to machine learning explanations?
The connection doesn't seem immediately apparent at least not for me and warrants an explanation.

## A machine learning example

Let's consider the following scenario:
You have trained a machine learning model $f$ to predict apartment prices.
For a specific apartment $x^{(i)}$, the model predicts $f(x^{(i)})=300,000$ and you need to explain this prediction.
The apartment has an area of 50 m^2^ (538 square feet), is located on the 2nd floor, has a park nearby, and cats are banned:

![The predicted price for a 50 $m^2$ 2nd floor apartment with a nearby park and cat ban is €300,000. Our goal is to explain how each of these feature values contributed to the prediction.](images/shapley-instance.jpg)

The average prediction for all apartments in the test data is €310,000, putting the predicted  price of the specific apartment slightly below average.
How much has each feature value contributed to the prediction compared to the average prediction?

In the apartment example, the feature values `park-nearby`, `cat-banned`, `area-50`, and `floor-2nd` worked together to achieve the prediction of €300,000.
In the data, the row with the features and the prediction would look like this:

<!-- TODO: Check markdown tables again if correct values -->

| Park| Cat | Area | Floor | Predicted Price   |
|-------------|------------|------|-------|---------|
| Nearby | Banned | 50   | 2nd   | €300,000|

Our goal is to explain the difference between the actual prediction (€300,000) and the average prediction (€310,000), which amounts to a difference of -€10,000.

To give you an idea of what an answer might look like:
`park-nearby` contributed €30,000; `area-50` contributed €10,000; `floor-2nd` contributed €0; and `cat-banned` contributed -€50,000.
The contributions add up to -€10,000, which is the final prediction minus the average predicted apartment price.
We know from the Shapley theory chapter that Shapley values can deliver such a fair attribution of a payout -- we only have to translate concepts from game theory to concepts from machine learning predictions.

## We can see a prediction as coalitional game

A prediction can be seen as a coalitional game by assuming that each feature value of the instance is a "player" in a game with a payout, which involves the predicted value.
We'll call this version of Shapley values adapted to machine learning predictions "SHAP".
Let's translate the terms from game theory to machine learning predictions one by one with the following table:


| Game Theoretic Concept | Machine Learning | Math terms |
|--------|-------------------|------------|------|
| Player | Feature index | $j$|
| Coalition | Set of feature values | $S \subseteq \{1, \ldots, p\}$ |
| Not in coalition | Features not in coalition $S$ | $C: C = \{1, \ldots, p\} \setminus S$ |
| Coalition size | Number of feature values in coalition | $|S|$| 
| Total number of players | Number of features | $p$| 
| Total payout | Prediction for $x^{(i)}$ minus average prediction | $f(x^{(i)}) - \mathbb{E}(f(X))$ |
| Value function| Prediction for feature values in coalition S minus expected | $v_{f,x^{(i)}}(S)$|
| SHAP value | Contribution of feature $j$ towards payout |  $\phi^{(i)}_j$ |

I know, you might have questions about these terms, but they will be addressed soon.
The value function is a quite central piece to SHAP and we will get to that matter.
And it's entangled with the question of how we simulate absent features.

::: {.callout-note}

The value function depends on a certain model $f$ and a certain data point to be explained $x^{(i)}$ and maps a coalition $S$ to it's value.
The correct way to write it therefore would be as $v_{f, x^{(i)}}(S)$, but for shortness I will use $v(S)$.

:::

## How to get predictions when features are absent?

The SHAP value function for a given model $f$ and data instance $x^{(i)}$ is defined as:

$$v_{f,x^{(i)}}(S) = \int f(x^{(i)}_S, X_C)dX_{C} - \mathbb{E}(f(X))$$

This value function holds the answer to the question of how to simulate absent features.
The second part $\mathbb{E}(f(X))$ is quickly explained:
It ensures that the value of an empty coalition $v(\emptyset)$ receives a value of 0.

Verify yourself:

\begin{align}
v(\emptyset) &=& \int f(x_{1},\ldots,x_{p})d\mathbb{P}_{X}-E_X(f(X)) \\
             &=& E_X(f(X)) - E_X(f(X)) \\
             &=& 0 \\
\end{align}


The first part of the equation, $\int f(x^{(i)}_S, X_C)dX_{C}$ is where the magic happens.
The model prediction function $f$ is at the very center of the value function.
The prediction function take the feature vector $x^{(i)} \in \mathbb{R}^p$ as input.
But we only know the features in set $S$, so we have to do something for the features not in $S$, which we index with $C$.

The trick here is to treat the features that we don't know as random variables and integrate over the distribution of that random variable. 
This concept of integrating over the distribution of a random variable is called marginalization.

::: {.callout-tip}

## Marginalization

While integration of a function typically involves calculating the area under the curve, integration with respect to a distribution implies weighting certain portions under the curve more heavily, based on their likelihood within the integral.

:::

There is a 1-to-1 correspondence between the feature values that you put into the prediction function for your instance of interest $x^{(i)}$ and random variables.

| Feature  value   | Random variable            |
|------------------|----------------------------|
| $x^{(i)}_j$      | $X_j$                        |
| $x^{(i)}_S$      | $X_S$                        |
| $x^{(i)}_C$      | $X_C$                        |
| $x^{(i)}  $      | $X$                          |

Quick summary on calculating the value of a coalition of features:
For features in coalition $S$, we insert the corresponding values from $x^{(i)}$, namely $x^{(i)}$ into the prediction function $f$.
If feature with index $j$ is not in coalition $S$ (but in $C$), we integrate the prediction function $f$ with respect to the distribution of $X_C$.

Let's go back to this example:

| Park| Cat | Area | Floor | Predicted Price   |
|-------------|------------|------|-------|---------|
| Nearby | Banned | 50   | 2nd   | €300,000|

Very informally, the value function for the coalition of park, floor would be:

$$v(\{\text{park}, \text{floor}\}) = \int f(x_{park},X_{cat}, X_{area}, x_{floor})d\mathbb{P}_{X_{cat,area}}-E_X(f(X)),$$

with $x_{park}=\text{nearby}$, $x_{floor}=2$, and $E_X(f(X))=300 000$.

So the roles for this coalition and how we put them into the value function:

- Park and floor, the "present" features: just put them into $f$ with the values the data instance has for these features
- Cat and area: treat them as random variables and integrate over them. 


## Now the marginal contribution

We are slowly working our way up to the Shapley value.
We just had a look at the value function and now we look at the marginal contribution:
How much does a feature $j$ contribute to a coalition of features $S$?

The marginal contribution of $j$ to $S$ is:

\begin{align*}

v(S \cup \{j\}) - v(S) &= \int f(x^{(i)}S, X_C) d\mathbb{P}{X_{C}} - E_X(f(X)) \\
&\quad - \left(\int f(x^{(i)}{S \cup {j}}, X{C \backslash {j}}) d\mathbb{P}{X{C \backslash {j}}} - E_X(f(X))\right) \\
&= \int f(X | X_{S \cup {j}} = x^{(i)}{S \cup {j}}) d\mathbb{P}{X_{C \backslash {j}}} \\

&\quad - \int f(X | X_{S} = x^{(i)}{S}) d\mathbb{P}{X_{C}}

\end{align*}


Again, for the apartment example, the contribution of the cat to a coalition of {park, floor} would be:

$$v(\{\text{cat}, \text{park}, \text{floor}\}) - v(\{\text{park}, \text{floor})$$ 

The result is a number that describes how much the value of the coalition increases when we "know" the cat feature for a coalition where we already know the park and the floor feature.
Because that's what the value function expresses: Present features are known, absent feature values are unknown, so the marginal contribution just tells you how much, given that you already know some of the features, how much the prediction is pushed (either up or down) when we know, in this case, the cat feature as well.


## Putting it all together

Putting all the terms together the Shapley value equation, we get:

<!--
3 line version
\begin{align*}
\phi^{(i)}_j &= \sum_{S\subseteq{1,\ldots,p} \backslash {j}} \frac{|S|!\left(p-|S|-1\right)!}{p!} \\
&\quad \times \left(\int f(x^{(i)}_{S \cup{j}}, X_{C \backslash j})d\mathbb{P}{X_{C \backslash {j}}}\right.\\
&\quad\quad - \left. \int f(x^{(i)}_S, X_C)d\mathbb{P}{X_{C}} \right)
\end{align*}
-->

\begin{align*}
\phi^{(i)}_j &= \sum_{S\subseteq{1,\ldots,p} \backslash {j}} \frac{|S|!\left(p-|S|-1\right)!}{p!} \\
&\quad \times \left(\int f(x^{(i)}_{S \cup{j}}, X_{C \backslash j})d\mathbb{P}{X_{C \backslash {j}}} - \int f(x^{(i)}_S, X_C)d\mathbb{P}{X_{C}} \right)
\end{align*}


The SHAP value $\phi^{(i)}_j$ of a feature value is the average marginal contribution of a feature value $x^{(i)}$ to all possible coalitions of all features.

And that's it.
It's the same formula as in the [Shapley Theory Chapter](#theory), but with the value function adapted to explaining a machine learning prediction.
The formula is again an average of marginal contributions and each contribution is again weighted based on size of coalition.
And since SHAP values are defined just like Shapley values -- just with a very specific value function -- they also fulfil the axioms.

## The axioms help us understand SHAP values

The axioms formed the basis for defining Shapley values.
Since SHAP values *are* Shapley values, but with a specific value function and game definition, we know that they fulfill the axioms.
@strumbelj2010efficient, @strumbelj2014explaining and @lundberg2017unified showed this.
The axioms are also interesting in the other direction: By knowing that a SHAP adheres to Efficiency, Symmetry, Dummy and Additivity, we can infer how to interpret SHAP values or at least get a first idea.
Let's go through the axioms one by one and identify what the mean for the theoretic SHAP values.

### Efficiency: SHAP values adds up to the (centered) prediction

The SHAP values must add up to the difference between the prediction for $x^{(i)}$ and the expected prediction:

$$\sum_{j=1}^p\phi^{(i)}_j = f(x) - E_X(f(X))$$

Implications: 
The efficiency axiom is quite common in explainable AI, and it's also followed by methods like LIME.
The efficiency axiom provides an anchoring effect, ensuring that the attributions are on the same scale as the output. 
An example of interpretation without this anchoring effect of efficiency would be examining only the gradients of the predicted score concerning the inputs. 
These gradients would be on a different scale and would not add up to the prediction.

### Symmetry: The order of features doesn't matter

The contributions of two feature values j and k should be the same if they contribute equally to all possible coalitions.

If

$$v_{f,x}(S \cup \{j\}) = val_{f,x}(S \cup \{k\})$$

for all

$$S \subseteq \{1, \ldots, p\} \backslash \{j, k\}$$

then

$$\phi^{(i)}_j = \phi^{(i)}_{k}$$

Implications: 
The symmetry axiom implies that, for example, the order of features should not matter.
If they contribute equally, they should receive the same SHAP value.
Other methods, such as the breakdown method [@staniak2018explanations] or counterfactual explanations, violate this axiom, and two features could have the same impact on the prediction without receiving the same attribution.
This axiom is a requirement for us to accurately interpret the ordering of the SHAP values.

### Dummy: Features that don't change the prediction get a SHAP value of 0 

A feature j that does not change the predicted value – regardless of which coalition of feature values it's added to – should have a SHAP value of 0.

If

$$v_{f,x}(S \cup \{j\}) = v_{f,x}(S)$$

for all

$$S \subseteq \{1, \ldots, p\}$$

then

$$\phi^{(i)}_j = 0$$

Implications: 
The dummy axiom ensures that features not used by the model receive an attribution of zero. 
This is a straightforward implication.
For example, if a LASSO model was trained, we can be certain that a feature with a $\beta_j = 0$ will have a SHAP value of zero for this feature for all possible data points.

### Additivity: Additive predictions are also additive in their SHAP values

For a game with combined payouts $v_1+v_2$, the respective SHAP values are as follows:

$$\phi^{(i)}_{j}(v_1)+\phi^{(i)}_j(v_2)$$

Implications:
Suppose you trained a random forest, which means that the prediction is an average of many decision trees. 
The Additivity property guarantees that you can calculate a feature's SHAP value for each tree individually, average the SHAP, and obtain the SHAP value for the random forest. 
Also for an additive ensemble of models, the SHAP value is the sum of individual SHAP values.

::: {.callout-note}

There exists an alternative formulation of the Shapley axioms in which the Dummy and Additivity axioms are replaced with a Linearity axiom; however, both formulations ultimately lead to the Shapley values.

:::

This chapter provided us with the theoretic SHAP values.
But we have a big problem: 
In reality, we don't have a closed form expression for $f$ and we don't know the distributions of $X_C$.
That means we can't calculate the SHAP values, but, fortunately, we can estimate them.

