# From Shapley Values to SHAP

::: {.callout-tip appearance="simple"}

By the end of this chapter, you will be able to:

- Describe a prediction as a coalitional game.
- Explain how SHAP values are defined.
- Interpret Shapley value axioms in terms of machine learning predictions.

:::

We have been learning about Shapley values from coalitional game theory.
But how do these values connect to machine learning explanations?
The connection might not seem apparent -- it certainly didn't to me when I first learned about SHAP.

## A machine learning example

Consider the following scenario:
You have trained a machine learning model $f$ to predict apartment prices.
For a specific apartment $x^{(i)}$, the model predicts $f(x^{(i)})=300,000$.
Your task is to explain this prediction.
The apartment has an area of 50 m^2^ (538 square feet), is located on the 2nd floor, has a nearby park, and cats are banned.
These features are what the model used to make the prediction.

![The predicted price for a 50 $m^2$ 2nd floor apartment with a nearby park and cat ban is €300,000. Our goal is to explain how each of these feature values contributed to the prediction.](images/shapley-instance.jpg)

The average prediction for all apartments in the data is €310,000, which places the predicted price of this specific apartment slightly below average. How much did each feature value contribute to the prediction compared to the average prediction?

In the apartment example, the feature values `park-nearby`, `cat-banned`, `area-50`, and `floor-2nd` collectively led to a prediction of €300,000.
The row with these features and prediction in the data would look like this, where the header of the column is the feature's name and the row below indicates the value for our apartment:

| Park| Cat | Area | Floor | Predicted Price   |
|-------------|------------|------|-------|---------|
| Nearby | Banned | 50   | 2nd   | €300,000|

Our goal is to explain the difference between the actual prediction (€300,000) and the average prediction (€310,000), which is a difference of -€10,000.

Here's an example of what an answer might look like: `park-nearby` contributed €30,000; `area-50` contributed €10,000; `floor-2nd` contributed €0; and `cat-banned` contributed -€50,000. The contributions add up to -€10,000, which is the final prediction minus the average predicted apartment price. From the Shapley theory chapter, we know that Shapley values can provide a fair attribution of a payout. We just need to translate concepts from game theory to machine learning prediction concepts.

## Viewing a prediction as a coalitional game

A prediction can be viewed as a coalitional game by considering each feature value of an instance as a "player" in a game. The "payout" involves the predicted value. We refer to this version of Shapley values adapted for machine learning predictions as "SHAP". Let's translate the terms from game theory to machine learning predictions one by one with the following table:

| Game Theoretic Concept | Machine Learning | Math term |
|--------|-------------------|------------|
| Player | Feature index | $j$|
| Coalition | Set of features | $S \subseteq \{1, \ldots, p\}$ |
| Not in coalition | Features not in coalition $S$ | $C: C = \{1, \ldots, p\} \setminus S$ |
| Coalition size | Number of features in coalition $S$ | $|S|$| 
| Total number of players | Number of features | $p$| 
| Total payout | Prediction for $x^{(i)}$ minus average prediction | $f(x^{(i)}) - \mathbb{E}(f(X))$ |
| Value function| Prediction for feature values in coalition S minus expected | $v_{f,x^{(i)}}(S)$|
| SHAP value | Contribution of feature $j$ towards payout |  $\phi^{(i)}_j$ |
You may have questions about these terms, but we will address them shortly.
The value function is central to SHAP, and we will discuss it in detail.
This function is closely related to the simulation of absent features.

::: {.callout-note}

The value function relies on a specific model $f$ and a particular data point to be explained $x^{(i)}$, and maps a coalition $S$ to its value.
Although the correct notation is $v_{f, x^{(i)}}(S)$, I will occasionally use $v(S)$ for brevity.

:::

## The SHAP value function

The SHAP value function, for a given model $f$ and data instance $x^{(i)}$, is defined as:

$$v_{f,x^{(i)}}(S) = \int f(x^{(i)}_S \cup X_C)d\mathbb{P}_{X_{C}} - \mathbb{E}(f(X))$$

This function provides an answer to the simulation of absent features.
The second part $\mathbb{E}(f(X))$ is straightforward:
It ensures the value of an empty coalition $v(\emptyset)$ equals 0.

Confirm this for yourself:

\begin{align}
v(\emptyset) &=& \int f(X_{1},\ldots,X_{p})d\mathbb{P}_{X}-E_X(f(X)) \\
             &=& E_X(f(X)) - E_X(f(X)) \\
             &=& 0 \\
\end{align}

The first part of the equation, $\int f(x^{(i)}_S, X_C)dX_{C}$, is where the magic occurs.
The model prediction function $f$, which is central to the value function, takes the feature vector $x^{(i)} \in \mathbb{R}^p$ as input and generates the prediction $\in \mathbb{R}$.
However, we only know the features in set $S$, so we need to account for the features not in $S$, which we index with $C$.

SHAP's approach is to treat the unknown features as random variables and integrate over their distribution.
This concept of integrating over the distribution of a random variable is called marginalization.

::: {.callout-tip}

## Marginalization

Integration of a function typically involves calculating the area under the curve.
However, when integrating with respect to a distribution, certain portions under the curve are weighted more heavily based on their likelihood within the integral.

:::

This means that known features are treated as values that we can directly input into the model $f$, while absent features are treated as random variables.
In mathematical terms, I distinguish a random variable from an observed value by capitalizing it:

| Feature value   | Random variable            |
|-----------------|----------------------------|
| $x^{(i)}_j$     | $X_j$                      |
| $x^{(i)}_S$     | $X_S$                      |
| $x^{(i)}_C$     | $X_C$                      |
| $x^{(i)}$       | $X$                        |

Let's revisit the apartment example:

| Park         | Cat     | Area | Floor | Predicted Price |
|--------------|---------|------|-------|-----------------|
| Nearby       | Banned  | 50   | 2nd   | €300,000        |

Informally, the value function for the coalition of park, floor would be:

$$v(\{\text{park}, \text{floor}\}) = \int f(x_{park},X_{cat}, X_{area}, x_{floor})d\mathbb{P}_{X_{cat,area}}-E_X(f(X)),$$

where $x_{park}=\text{nearby}$, $x_{floor}=2$, and $E_X(f(X))=300.000$.

- The features 'park' and 'floor' are "present", so we input their corresponding values into $f$.
- The features 'cat' and 'area' are "absent", and thus are treated as random variables and integrated over. 

## Marginal contribution

We are gradually working our way up to the SHAP value.
We've examined the value function, and the next step is to determine the marginal contribution.
This is the contribution of feature $j$ to a coalition of features $S$.

The marginal contribution of $j$ to $S$ is:

\begin{align*}

v(S \cup \{j\}) - v(S) &= \int f(x^{(i)}_S \cup X_C) d\mathbb{P}_{X_{C}} - \mathbb{E}(f(X)) \\
&\quad - \left(\int f(x^{(i)}_{S \cup {j}} \cup X_{C \backslash {j}}) d\mathbb{P}_{X_{C \backslash {j}}} - \mathbb{E}(f(X))\right) \\
&= \int f(x^{(i)}_{S \cup {j}} \cup X_{C \backslash j}) d\mathbb{P}_{X_{C \backslash {j}}} \\

&\quad - \int f(x^{(i)}_{S} \cup X_C) d\mathbb{P}_{X_{C}}

\end{align*}


For instance, the contribution of 'cat' to a coalition of {park, floor} would be:

$$v(\{\text{cat}, \text{park}, \text{floor}\}) - v(\{\text{park}, \text{floor})$$ 

The resulting marginal contribution describes the change in the value of the coalition {park, floor} when the 'cat' feature is included.
Another way to interpret the marginal contribution is that present features are known, absent feature values are unknown, so the marginal contribution illustrates how much the value changes from knowing $j$ in addition to already knowing $S$.

## Putting it all together

Combining all the terms into the Shapley value equation, we get the SHAP equation:

\begin{align*}
\phi^{(i)}_j &= \sum_{S\subseteq{1,\ldots,p} \backslash {j}} \frac{|S|!\left(p-|S|-1\right)!}{p!} \\
&\quad \times \left(\int f(x^{(i)}_{S \cup{j}} \cup X_{C \backslash j})d\mathbb{P}{X_{C \backslash {j}}} - \int f(x^{(i)}_S \cup X_C)d\mathbb{P}{X_{C}} \right)
\end{align*}


The SHAP value $\phi^{(i)}_j$ of a feature value is the average marginal contribution of a feature value $x^{(i)}_j$ to all possible coalitions of all features.
And that concludes it.
This formula is similar to the one in the [Shapley Theory Chapter](#theory), but the value function is adapted to explain a machine learning prediction.
The formula, once again, is an average of marginal contributions, each contribution being weighted based on the size of the coalition.

## Interpreting SHAP values through axioms

The axioms form the foundation for defining Shapley values.
As SHAP values are Shapley values with a specific value function and game definition, they adhere to these axioms.
This has been demonstrated by @strumbelj2010efficient, @strumbelj2014explaining, and @lundberg2017unified.
Given that SHAP follows the principles of Efficiency, Symmetry, Dummy, and Additivity, we can deduce how to interpret SHAP values or at least obtain a preliminary understanding.
Let's explore each axiom individually and determine their implications for the interpretation of SHAP values.

### Efficiency: SHAP values correspond to the (centered) prediction

SHAP values must total to the difference between the prediction for $x^{(i)}$ and the expected prediction:

$$\sum_{j=1}^p\phi^{(i)}_j = f(x^{(i)}) - \mathbb{E}(f(X))$$

Implications:
The efficiency axiom is prevalent in explainable AI and is adhered to by methods like LIME.
This axiom guarantees that attributions are on the scale of the output, allowing us to interpret the results as contributions to the prediction.
Gradients, another method for explaining model predictions, do not sum up to the prediction, hence in my opinion, are more challenging to interpret.

### Symmetry: Feature order is irrelevant {-}

If two feature values j and k contribute equally to all possible coalitions, their contributions should be equal.

Given

$$v_{f,x^{(i)}}(S \cup \{j\}) = v_{f,x^{(i)}}(S \cup \{k\})$$

for all

$$S \subseteq \{1, \ldots, p\} \backslash \{j, k\}$$

then

$$\phi^{(i)}_j = \phi^{(i)}_{k}$$

Implications:
The symmetry axiom implies that the order of features doesn't matter.
If two features contribute equally, they will receive the same SHAP value.
Other methods, such as the breakdown method [@staniak2018explanations] or counterfactual explanations, violate the symmetry axiom because two features can impact the prediction equally without receiving the same attribution.
Symmetry is essential for accurately interpreting the order of SHAP values, for instance, when ranking features using SHAP importance (sum of absolute SHAP values per feature).

### Dummy: Features not influencing the prediction receive a SHAP value of 0  {-}

A feature j that does not alter the predicted value, regardless of the coalition of feature values it is added to, should have a SHAP value of 0.

Given

$$v_{f,x^{(i)}}(S \cup \{j\}) = v_{f,x^{(i)}}(S)$$

for all

$$S \subseteq \{1, \ldots, p\}$$

then

$$\phi^{(i)}_j = 0$$

Implications: 
The dummy axiom ensures that unused features by the model receive a zero attribution.
This is an obvious implication.
For instance, if a sparse linear regression model was trained, we can be sure that a feature with a $\beta_j = 0$ will have a SHAP value of zero for all data points.

### Additivity: Additive predictions correspond to additive SHAP values

For a game with combined payouts $v_1+v_2$, the respective SHAP values are:

$$\phi^{(i)}_{j}(v_1)+\phi^{(i)}_j(v_2)$$

Implications:
Consider a scenario where you've trained a random forest, meaning the prediction is an average of numerous decision trees. 
The Additivity property ensures that you can compute a feature's SHAP value for each tree separately and average them to obtain the SHAP value for the random forest.
For an additive ensemble of models, the SHAP value equals the sum of the individual SHAP values.

::: {.callout-note}

An alternative formulation of the Shapley axioms exists where the Dummy and Additivity axioms are replaced with a Linearity axiom; however, both formulations eventually yield the Shapley values.

:::

This chapter has provided theoretical SHAP values.
However, we face a significant problem: 
In practice, we lack a closed form expression for $f$ and we are unaware of the distributions of $X_C$.
This means we are unable to calculate the SHAP values, but, fortunately, we can estimate them.
