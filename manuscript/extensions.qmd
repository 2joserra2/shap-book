# Extensions of SHAP {#extensions}

This chapter outlines some extensions of SHAP values, with a focus on adaptations for specific models or data structures, while preserving their role in explaining predictions.

## L-Shapley and C-Shapley for data with a graph structure

Although the SHAP values in the `shap` library are intended for i.i.d data, many structured data types consist of interconnected elements that form a graph.
To accommodate this, @chen2018shapley introduced L-Shapley (local) and C-Shapley (connected) for data with a graph structure.
These constrained versions of Shapley values are especially helpful in explaining image and text classifiers.

For text, L-Shapley only takes into account neighboring words, rather than all word interactions.
For instance, when calculating the SHAP value of the 6th word in a text, only neighboring words are considered for word absence, based on the assumption that distant words have minimal interaction.
C-Shapley, conversely, focuses on n-grams with $n\leq4$, preventing coalitions between distant words.

## Group SHAP for grouped features

Group SHAP [@lin2022model] is a simple method that groups features together and calculates a single SHAP value for each group instead of each feature value.
This approach is less computationally taxing and provides a potential solution to the correlation issue.
The `shap` library's Partition explainer facilitates this by allowing custom groupings.

## n-Shapley values

The n-Shapley values [@bordt2022shapley] link SHAP values with GAMs (generalized additive models), with n signifying the interaction depth considered during the Shapley value calculation, depending on the GAM's training.
A standard GAM includes no interaction (n=1), but interactions can be incorporated.
The paper illustrates the relationship between n-Shapley values and functional decomposition, providing the [nshap package in Python](https://github.com/tml-tuebingen/nshap) for implementation.
If the model being explained is a GAM, SHAP recovers all non-linear components, as outlined in the [Additive Chapter](#additive).

## Shapley Interaction Index

While SHAP values allocate predictions to individual players fairly, what happens with interactions between players?
Intrinsically, these interactions are split among the participating players.
However, measuring the interaction effect directly can be beneficial.
Shapley Taylor [@sundararajan2020shapley] and Faith-Shap [@tsai2023faith] are two examples of such interaction indices.

## Causality and SHAP Values

Without additional assumptions, SHAP values don't represent causal attributions.
Although they attribute features to the model prediction, this doesn't necessarily correspond to observable effects in reality.
Additional assumptions are required to ensure causal effects.

Several extensions for causal interpretation have been proposed, including Causal Shapley Values [@heskes2020causal], Shapley Flow [@wang2021shapley], and Asymmetric Shapley Values [@frye2020asymmetric].
These methods typically use directed acyclic graphs to identify (or assume) causal structures or apply the do-calculus.
Occasionally, they modify or extend the original Shapley axioms, which can change the resulting Shapley game.

## Counterfactual SHAP

As mentioned in the [Limitations Chapter](#limitations), SHAP values may not be the best choice for counterfactual explanations.
A counterfactual explanation is a contrastive explanation that clarifies why the current prediction was made instead of a counterfactual outcome.
This is crucial in recourse situations when someone affected by a decision wants to challenge a prediction, such as a creditworthiness classifier.
Counterfactual SHAP (or CF-SHAP) [@albini2022counterfactual] introduces this approach to SHAP values through careful selection of background data.


## Explanation Shifts[^carlos]

Explanation shifts [@mougan2022explanation] refer to differences in how predictions are explained by machine learning models on training data compared to new data.
By analyzing variations in SHAP values, we get insights into how the relationship between models and features evolves.
Explanation shifts offer advantages over traditional methods that focus only on distribution shifts (like monitoring performance), as explanations provide a deeper understanding of changes in the model's behavior.
Changes that might otherwise be overlooked.
Explanation shifts is implemented within the [skshift](https://skshift.readthedocs.io/en/latest/) Python package which also includes tutorials.

## Fairness Measures via Explanations Distributions: Equal Treatment

@mougan2023demographic proposed a new fairness metric based on liberalism-oriented philosophical principles: “Equal Treatment”.
This measure considers the influence of feature values on predictions, defines distributions of SHAP explanations, and compares explanation distributions between populations with different protected characteristics.
While related work focused on model predictions to measure demographic parity, the measure of equal treatment is more fine-grained, accounting for the usage of attributes by the model via explanation distributions.
Consequently, equal treatment implies equal outcomes, but the converse is not necessarily true.
Equal treatment is implemented within the [explanationspace](https://explanationspace.readthedocs.io/en/latest/Python) package, which includes tutorials.


## And many more

This summary is not comprehensive.
At the time of writing, approximately 12k papers cited the original SHAP paper [@lundberg2017unified], with around 10k of these also mentioning "shap" or "shapley" in their title or abstract.
Although many of these are review or application papers, a notable number extend SHAP values, making it unfeasible to cover all of them here.


[^carlos]: The preceding paragraph and the one about fairness were authored by Carlos Mougan, the researcher responsible for these extended uses of SHAP (and slightly edited by me).
