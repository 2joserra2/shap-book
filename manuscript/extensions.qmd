# Extensions of SHAP {#extensions}

This chapter highlights some extensions for SHAP values, focusing on adaptations for specific models or data structures while still maintaining their purpose in explaining predictions.

## L-Shapley and C-Shapley for data with a graph structure

Although SHAP values in the `shap` library are designed for i.i.d data, many structured data types feature interconnected elements forming a graph. To address this, @chen2018shapley introduced L-Shapley (local) and C-Shapley (connected) for data with a graph structure. These constrained forms of Shapley values are particularly useful in explaining image and text classifiers.

For text, L-Shapley considers only neighboring words instead of interactions between all words. For example, when calculating the Shapley value of the 6th word in a text, only neighboring words are considered for word absence, assuming that words further away have minimal interaction. C-Shapley, on the other hand, focuses on n-grams with $n\leq4$, avoiding coalitions between distant words.

## Group SHAP for grouped features

Group SHAP [@lin2022model] is a straightforward approach that groups features together and computes a single SHAP value per group rather than for each feature value. This method is less computationally expensive and offers a potential solution to the correlation problem. The `shap` library's Partition explainer allows this by providing custom groupings.

## n-Shapley values

The n-Shapley values [@bordt2022shapley] connect SHAP values with GAMs (generalized additive models), where n represents the interaction depth considered during Shapley value computation, depending on the GAM's training.
A "vanilla" GAM features no interaction (n=1), but interactions can be added.
The paper demonstrates the relationship between n-Shapley values and functional decomposition, providing the [nshap package in Python](https://github.com/tml-tuebingen/nshap) for implementation.
If the model being explained is a GAM, SHAP recovers all non-linear components, as discussed in the [Additive Chapter](#additive).

## Shapley Interaction Index

SHAP values fairly allocate predictions to individual players, but what about interactions between players?
By design, these interactions are divided among participating players.
However, it can be useful to quantify the interaction effect directly.
Shapley Taylor [@sundararajan2020shapley] and Faith-Shap [@tsai2023faith] are two such examples of such interaction indices.

## Causality and SHAP Values

Without further assumptions, SHAP values don't represent causal attributions.
While they describe how the model output is affected, this doesn't necessarily translate to observable effects in reality.
Ensuring causal effects requires additional assumptions.

Several extensions have been proposed for causal interpretation, such as Causal Shapley Values [@heskes2020causal], Shapley Flow [@wang2021shapley], and Asymmetric Shapley Values [@frye2020asymmetric].
These approaches typically utilize directed acyclic graphs to identify (or assume) causal structures or apply the do-calculus.
Occasionally, they modify or expand the original Shapley axioms, which can alter the resulting Shapley game.

## Counterfactual SHAP

As discussed in the [Limitations Chapter](#limitations), SHAP values are not necessarily the best option for counterfactual explanations.
A counterfactual explanation is a contrastive explanation, explaining why the current prediction was made instead of a counterfactual outcome.
This is important, for example, in recourse situations when someone affected by a decision wants to challenge a prediction, such as a creditworthiness classifier.
Counterfactual SHAP (or CF-SHAP) [@albini2022counterfactual] brings this approach to SHAP values through careful background data selection.


## Explanation Shifts

Explanation shifts [@mougan2022explanation] refer to differences in how predictions are explained by machine learning models on training data compared to new data.
By analyzing variations in SHAP values, we get insights into how the relation models and features evolved.
Explanation shifts offer advantages over traditional methods that focus only on distribution shifts (like monitoring performance), as explanations provide a deeper understanding of changes in the model behavior.
Changes that might otherwise be overlooked.
Explanation shifts is implemented within the [skshift](https://skshift.readthedocs.io/en/latest/) Python package which also includes tutorials.

## Fairness Measures via Explanations Distributions: Equal Treatment

[@mougan2023demographic] proposed a new fairness metric based on liberalism-oriented philosophical principles: “Equal Treatment”.
This measure considers the influence of feature values on predictions, defines distributions of explanations, and compares explanation distributions between populations with different protected characteristics.
While related work focused on model predictions to measure demographic parity measure of equal treatment is more fine-grained, accounting for the usage of attributes by the model via explanation distributions.
Consequently, equal treatment implies equal outcomes, but the converse is not necessarily true.
Equal treatment is implemented within the [explanationspace](https://explanationspace.readthedocs.io/en/latest/Python) package, including tutorials.


## And many more

This overview is not exhaustive.
At the time of writing, there were approximately 12k papers citing the original SHAP paper [@lundberg2017unified].
Among those, around 10k also had "shap" or "shapley" in their title or abstract.
While many of these are review or application papers, a significant portion are extensions of SHAP values, making it impossible to cover them all here.
