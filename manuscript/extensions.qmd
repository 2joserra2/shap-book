# Extensions of Shapley values {#extensions}

This chapter lists some extensions for Shapley values.
And with extensions I mean we are still in the area of explaining predictions, but adapting Shapley values to more specific model or data structures.
Instead, here are a few highlights.

## L-Shapley and C-Shapley for data with a graph structure

Shapley values as envisioned in shap are for more or less i.i.d data.
But there are many structured data types where the features form a graph.
So @chen2018shapley came up with L-Shapley (L for local) and C-Shapley (C for connected) for the task of data with a graph structure.
Both of these are constrained form of Shapley values, which can be useful especially for explaining image and text classifiers.
In the case of text, instead of considering interactions between all the words in a text, e.g., L-Shapley only consider neighboring words.
So if you want the Shapley value of, say, the 6th word in a text, only the neighboring words are also considered for word absence.
The assumption is that words further away won't interact much with the word in question.
And for C-Shapley n-grams with $n\leq4$.
So you wouldn't form a coalition of the first and the last word.


## Group SHAP for grouped features

Group SHAP [@lin2022model] is the simple proposal of grouping features together and computing one Shapley vlaue per group instead of for each feature value.
It's less costly than computing the Shapley value for each feature value and a possible solution for the correlation problem.
In `shap` you can already do this with the Partition explainer and by providing your own grouping.

## n-Shapley values

The n-Shapley values [@bordt2022shapley] link Shapley values to GAMs (generalized additive models)
The n stands for the interaction depth that the Shapley value computation should consider, which again depends on how the GAM was trained.
The "vanilla" GAM would be without interaction (n=1), but it's possible to add interactions.
The paper also shows the relation between the n-Shapley values and functional decomposition.
They also provide [code: nshap package in Python](https://github.com/tml-tuebingen/nshap)
If the model to be explained is a GAM, then SHAP recover all the non-linear components, as we have seen in the [additive chapter](#additive).


## Shapley interaction index

Shapley values fairly attribute the prediction to individual players.
But what about interactions between players?
They are, by design, split up among the players that contribute to the interaction.
Sometimes it's useful to not do that but to state how large the interaction effect is.

We have Shapley Taylor [@sundararajan2020shapley], Faith-Shap [@tsai2023faith] as two examples.


## Causality and Shapley values

Shapley values, without further assumptions, don't describe causal attributions.
They only describe how the model output is affected, but that does not mean that in reality we would observe such an effect.
The reason is that we can't guarantee without making assumptions that they describe a causal effect.

Multiple extensions have been proposed to allow a causal interpretation, for example,  Causal Shapley Values [@heskes2020causal], Shapley Flow [@wang2021shapley], and  Asymmetric Shapley values [@frye2020asymmetric].

The approaches usually leverage directed acyclic graphs as tools to identify causal structures (or rather, make assumptions about it) or they employ the do-calculus.
Sometimes they extend or change the original Shapley axioms, which in the latter case alters the resulting Shapley game.

## Counterfactual SHAP

As discussed in the [limitations chapter](#limitations), Shapley values aren't necessarily the best option when you want a counterfactual explanations.
A counterfactual explanation is a contrastive explanation why the current prediction was made instead of some counterfactual outcome.
This is important, for example, for recourse -- when someone affected by a decision wants to challenge the prediction of, e.g., a credit worthiness classifier.
Counterfactual SHAP (or CF-SHAP) [@albini2022counterfactual] brings this approach to Shapley values. 
This is controlled via the carefully setting the background data.


## And many more

The overview is not exhaustive.
At the time of writing, there were ~12k papers  citing the original SHAP paper[^lundberg2017unified].
Of those papers, ~10k also had shap or shapley in their title or abstract.
While many of those are also review or application papers, a good portion of them are extension of Shapley values, so it's too much to cover.
Too much to cover here.

